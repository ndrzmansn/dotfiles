{"mode":"editor","version":1,"windowDimensions":{"x":1920,"y":302,"width":1920,"height":778},"syntax":{"deserializer":"Syntax","grammarOverridesByPath":{}},"project":{"path":"/Users/zwei/Workspace/mxtool2","buffers":[{"text":"#!/usr/bin/env python2.7\n#\n# ----------------------------------------------------------------------------------------------------\n#\n# Copyright (c) 2007, 2014, Oracle and/or its affiliates. All rights reserved.\n# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n#\n# This code is free software; you can redistribute it and/or modify it\n# under the terms of the GNU General Public License version 2 only, as\n# published by the Free Software Foundation.\n#\n# This code is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n# version 2 for more details (a copy is included in the LICENSE file that\n# accompanied this code).\n#\n# You should have received a copy of the GNU General Public License version\n# 2 along with this work; if not, write to the Free Software Foundation,\n# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n# or visit www.oracle.com if you need additional information or have any\n# questions.\n#\n# ----------------------------------------------------------------------------------------------------\n#\n\nr\"\"\"\nmx is a command line tool for managing the development of Java code organized as suites of projects.\n\nFull documentation can be found in the Wiki at the site from which mxtool was downloaded.\n\"\"\"\n\nimport sys, os, errno, time, datetime, subprocess, shlex, types, StringIO, zipfile, signal, xml.sax.saxutils, tempfile, fnmatch\nimport multiprocessing\nimport textwrap\nimport socket\nimport xml.parsers.expat\nimport tarfile\nimport hashlib\nimport shutil, re, xml.dom.minidom\nimport pipes\nimport difflib\nfrom collections import Callable\nfrom threading import Thread\nfrom argparse import ArgumentParser, REMAINDER\nfrom os.path import join, basename, dirname, exists, getmtime, isabs, expandvars, isdir, isfile\n\n_projects = dict()\n_libs = dict()\n_jreLibs = dict()\n_dists = dict()\n_suites = dict()\n_annotationProcessors = None\n_primary_suite_path = None\n_primary_suite = None\n_src_suitemodel = None\n_dst_suitemodel = None\n_opts = None\n_java_homes = None\n_check_global_structures = True  # can be set False to allow suites with duplicate definitions to load without aborting\n_warn = False\n_hg = None\n\n\n\"\"\"\nA distribution is a jar or zip file containing the output from one or more Java projects.\n\"\"\"\nclass Distribution:\n    def __init__(self, suite, name, path, sourcesPath, deps, mainClass, excludedDependencies, distDependencies):\n        self.suite = suite\n        self.name = name\n        self.path = path.replace('/', os.sep)\n        self.path = _make_absolute(self.path, suite.dir)\n        self.sourcesPath = _make_absolute(sourcesPath.replace('/', os.sep), suite.dir) if sourcesPath else None\n        self.deps = deps\n        self.update_listeners = set()\n        self.mainClass = mainClass\n        self.excludedDependencies = excludedDependencies\n        self.distDependencies = distDependencies\n\n    def sorted_deps(self, includeLibs=False):\n        try:\n            excl = [dependency(d) for d in self.excludedDependencies]\n        except SystemExit as e:\n            abort('invalid excluded dependency for {} distribution: {}'.format(self.name, e))\n        return [d for d in sorted_deps(self.deps, includeLibs=includeLibs) if d not in excl]\n\n    def __str__(self):\n        return self.name\n\n    def add_update_listener(self, listener):\n        self.update_listeners.add(listener)\n\n    def make_archive(self):\n        # are sources combined into main archive?\n        unified = self.path == self.sourcesPath\n\n        with Archiver(self.path) as arc, Archiver(None if unified else self.sourcesPath) as srcArcRaw:\n            srcArc = arc if unified else srcArcRaw\n            services = {}\n            def overwriteCheck(zf, arcname, source):\n                if not hasattr(zf, '_provenance'):\n                    zf._provenance = {}\n                existingSource = zf._provenance.get(arcname, None)\n                isOverwrite = False\n                if existingSource and existingSource != source:\n                    if arcname[-1] != os.path.sep:\n                        logv('warning: ' + self.path + ': avoid overwrite of ' + arcname + '\\n  new: ' + source + '\\n  old: ' + existingSource)\n                    isOverwrite = True\n                zf._provenance[arcname] = source\n                return isOverwrite\n\n            if self.mainClass:\n                manifest = \"Manifest-Version: 1.0\\nMain-Class: %s\\n\\n\" % (self.mainClass)\n                if not overwriteCheck(arc.zf, \"META-INF/MANIFEST.MF\", \"project files\"):\n                    arc.zf.writestr(\"META-INF/MANIFEST.MF\", manifest)\n\n            for dep in self.sorted_deps(includeLibs=True):\n                if dep.isLibrary():\n                    l = dep\n                    # merge library jar into distribution jar\n                    logv('[' + self.path + ': adding library ' + l.name + ']')\n                    lpath = l.get_path(resolve=True)\n                    libSourcePath = l.get_source_path(resolve=True)\n                    if lpath:\n                        with zipfile.ZipFile(lpath, 'r') as lp:\n                            for arcname in lp.namelist():\n                                if arcname.startswith('META-INF/services/') and not arcname == 'META-INF/services/':\n                                    service = arcname[len('META-INF/services/'):]\n                                    assert '/' not in service\n                                    services.setdefault(service, []).extend(lp.read(arcname).splitlines())\n                                else:\n                                    if not overwriteCheck(arc.zf, arcname, lpath + '!' + arcname):\n                                        arc.zf.writestr(arcname, lp.read(arcname))\n                    if srcArc.zf and libSourcePath:\n                        with zipfile.ZipFile(libSourcePath, 'r') as lp:\n                            for arcname in lp.namelist():\n                                if not overwriteCheck(srcArc.zf, arcname, lpath + '!' + arcname):\n                                    srcArc.zf.writestr(arcname, lp.read(arcname))\n                elif dep.isProject():\n                    p = dep\n\n                    isCoveredByDependecy = False\n                    for d in self.distDependencies:\n                        if p in _dists[d].sorted_deps():\n                            logv(\"Excluding {0} from {1} because it's provided by the dependency {2}\".format(p.name, self.path, d))\n                            isCoveredByDependecy = True\n                            break\n\n                    if isCoveredByDependecy:\n                        continue\n\n                    # skip a  Java project if its Java compliance level is \"higher\" than the configured JDK\n                    jdk = java(p.javaCompliance)\n                    assert jdk\n\n                    logv('[' + self.path + ': adding project ' + p.name + ']')\n                    outputDir = p.output_dir()\n                    for root, _, files in os.walk(outputDir):\n                        relpath = root[len(outputDir) + 1:]\n                        if relpath == join('META-INF', 'services'):\n                            for service in files:\n                                with open(join(root, service), 'r') as fp:\n                                    services.setdefault(service, []).extend([provider.strip() for provider in fp.readlines()])\n                        elif relpath == join('META-INF', 'providers'):\n                            for provider in files:\n                                with open(join(root, provider), 'r') as fp:\n                                    for service in fp:\n                                        services.setdefault(service.strip(), []).append(provider)\n                        else:\n                            for f in files:\n                                arcname = join(relpath, f).replace(os.sep, '/')\n                                if not overwriteCheck(arc.zf, arcname, join(root, f)):\n                                    arc.zf.write(join(root, f), arcname)\n                    if srcArc.zf:\n                        sourceDirs = p.source_dirs()\n                        if p.source_gen_dir():\n                            sourceDirs.append(p.source_gen_dir())\n                        for srcDir in sourceDirs:\n                            for root, _, files in os.walk(srcDir):\n                                relpath = root[len(srcDir) + 1:]\n                                for f in files:\n                                    if f.endswith('.java'):\n                                        arcname = join(relpath, f).replace(os.sep, '/')\n                                        if not overwriteCheck(srcArc.zf, arcname, join(root, f)):\n                                            srcArc.zf.write(join(root, f), arcname)\n\n            for service, providers in services.iteritems():\n                arcname = 'META-INF/services/' + service\n                arc.zf.writestr(arcname, '\\n'.join(providers))\n\n        self.notify_updated()\n\n\n    def notify_updated(self):\n        for l in self.update_listeners:\n            l(self)\n\n\"\"\"\nA dependency is a library or project specified in a suite.\n\"\"\"\nclass Dependency:\n    def __init__(self, suite, name):\n        self.name = name\n        self.suite = suite\n\n    def __str__(self):\n        return self.name\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n    def __ne__(self, other):\n        return self.name != other.name\n\n    def __hash__(self):\n        return hash(self.name)\n\n    def isLibrary(self):\n        return isinstance(self, Library)\n\n    def isJreLibrary(self):\n        return isinstance(self, JreLibrary)\n\n    def isProject(self):\n        return isinstance(self, Project)\n\nclass Project(Dependency):\n    def __init__(self, suite, name, srcDirs, deps, javaCompliance, workingSets, d):\n        Dependency.__init__(self, suite, name)\n        self.srcDirs = srcDirs\n        self.deps = deps\n        self.checkstyleProj = name\n        self.javaCompliance = JavaCompliance(javaCompliance) if javaCompliance is not None else None\n        self.native = False\n        self.workingSets = workingSets\n        self.dir = d\n\n        # Verify that a JDK exists for this project if its compliance level is\n        # less than the compliance level of the default JDK\n        jdk = java(self.javaCompliance)\n        if jdk is None and self.javaCompliance < java().javaCompliance:\n            abort('Cannot find ' + str(self.javaCompliance) + ' JDK required by ' + name + '. ' +\n                  'Specify it with --extra-java-homes option or EXTRA_JAVA_HOMES environment variable.')\n\n        # Create directories for projects that don't yet exist\n        if not exists(d):\n            os.mkdir(d)\n        for s in self.source_dirs():\n            if not exists(s):\n                os.mkdir(s)\n\n    def all_deps(self, deps, includeLibs, includeSelf=True, includeJreLibs=False, includeAnnotationProcessors=False):\n        \"\"\"\n        Add the transitive set of dependencies for this project, including\n        libraries if 'includeLibs' is true, to the 'deps' list.\n        \"\"\"\n        childDeps = list(self.deps)\n        if includeAnnotationProcessors and len(self.annotation_processors()) > 0:\n            childDeps = self.annotation_processors() + childDeps\n        if self in deps:\n            return deps\n        for name in childDeps:\n            assert name != self.name\n            dep = dependency(name)\n            if not dep in deps and (dep.isProject or (dep.isLibrary() and includeLibs) or (dep.isJreLibrary() and includeJreLibs)):\n                dep.all_deps(deps, includeLibs=includeLibs, includeJreLibs=includeJreLibs, includeAnnotationProcessors=includeAnnotationProcessors)\n        if not self in deps and includeSelf:\n            deps.append(self)\n        return deps\n\n    def _compute_max_dep_distances(self, name, distances, dist):\n        currentDist = distances.get(name)\n        if currentDist is None or currentDist < dist:\n            distances[name] = dist\n            p = project(name, False)\n            if p is not None:\n                for dep in p.deps:\n                    self._compute_max_dep_distances(dep, distances, dist + 1)\n\n    def canonical_deps(self):\n        \"\"\"\n        Get the dependencies of this project that are not recursive (i.e. cannot be reached\n        via other dependencies).\n        \"\"\"\n        distances = dict()\n        result = set()\n        self._compute_max_dep_distances(self.name, distances, 0)\n        for n, d in distances.iteritems():\n            assert d > 0 or n == self.name\n            if d == 1:\n                result.add(n)\n\n        if len(result) == len(self.deps) and frozenset(self.deps) == result:\n            return self.deps\n        return result\n\n    def max_depth(self):\n        \"\"\"\n        Get the maximum canonical distance between this project and its most distant dependency.\n        \"\"\"\n        distances = dict()\n        self._compute_max_dep_distances(self.name, distances, 0)\n        return max(distances.values())\n\n    def source_dirs(self):\n        \"\"\"\n        Get the directories in which the sources of this project are found.\n        \"\"\"\n        return [join(self.dir, s) for s in self.srcDirs]\n\n    def source_gen_dir(self):\n        \"\"\"\n        Get the directory in which source files generated by the annotation processor are found/placed.\n        \"\"\"\n        if self.native:\n            return None\n        return join(self.dir, 'src_gen')\n\n    def output_dir(self):\n        \"\"\"\n        Get the directory in which the class files of this project are found/placed.\n        \"\"\"\n        if self.native:\n            return None\n        return join(self.dir, 'bin')\n\n    def jasmin_output_dir(self):\n        \"\"\"\n        Get the directory in which the Jasmin assembled class files of this project are found/placed.\n        \"\"\"\n        if self.native:\n            return None\n        return join(self.dir, 'jasmin_classes')\n\n    def append_to_classpath(self, cp, resolve):\n        if not self.native:\n            cp.append(self.output_dir())\n\n    def find_classes_with_matching_source_line(self, pkgRoot, function, includeInnerClasses=False):\n        \"\"\"\n        Scan the sources of this project for Java source files containing a line for which\n        'function' returns true. A map from class name to source file path for each existing class\n        corresponding to a matched source file is returned.\n        \"\"\"\n        result = dict()\n        pkgDecl = re.compile(r\"^package\\s+([a-zA-Z_][\\w\\.]*)\\s*;$\")\n        for srcDir in self.source_dirs():\n            outputDir = self.output_dir()\n            for root, _, files in os.walk(srcDir):\n                for name in files:\n                    if name.endswith('.java') and name != 'package-info.java':\n                        matchFound = False\n                        source = join(root, name)\n                        with open(source) as f:\n                            pkg = None\n                            for line in f:\n                                if line.startswith(\"package \"):\n                                    match = pkgDecl.match(line)\n                                    if match:\n                                        pkg = match.group(1)\n                                if function(line.strip()):\n                                    matchFound = True\n                                if pkg and matchFound:\n                                    break\n\n                        if matchFound:\n                            simpleClassName = name[:-len('.java')]\n                            assert pkg is not None\n                            if pkgRoot is None or pkg.startswith(pkgRoot):\n                                pkgOutputDir = join(outputDir, pkg.replace('.', os.path.sep))\n                                if exists(pkgOutputDir):\n                                    for e in os.listdir(pkgOutputDir):\n                                        if includeInnerClasses:\n                                            if e.endswith('.class') and (e.startswith(simpleClassName) or e.startswith(simpleClassName + '$')):\n                                                className = pkg + '.' + e[:-len('.class')]\n                                                result[className] = source\n                                        elif e == simpleClassName + '.class':\n                                            className = pkg + '.' + simpleClassName\n                                            result[className] = source\n        return result\n\n    def _init_packages_and_imports(self):\n        if not hasattr(self, '_defined_java_packages'):\n            packages = set()\n            extendedPackages = set()\n            depPackages = set()\n            for d in self.all_deps([], includeLibs=False, includeSelf=False):\n                depPackages.update(d.defined_java_packages())\n            imports = set()\n            importRe = re.compile(r'import\\s+(?:static\\s+)?([^;]+);')\n            for sourceDir in self.source_dirs():\n                for root, _, files in os.walk(sourceDir):\n                    javaSources = [name for name in files if name.endswith('.java')]\n                    if len(javaSources) != 0:\n                        pkg = root[len(sourceDir) + 1:].replace(os.sep, '.')\n                        if not pkg in depPackages:\n                            packages.add(pkg)\n                        else:\n                            # A project extends a package already defined by one of it dependencies\n                            extendedPackages.add(pkg)\n                            imports.add(pkg)\n\n                        for n in javaSources:\n                            with open(join(root, n)) as fp:\n                                content = fp.read()\n                                imports.update(importRe.findall(content))\n            self._defined_java_packages = frozenset(packages)\n            self._extended_java_packages = frozenset(extendedPackages)\n\n            importedPackages = set()\n            for imp in imports:\n                name = imp\n                while not name in depPackages and len(name) > 0:\n                    lastDot = name.rfind('.')\n                    if lastDot == -1:\n                        name = None\n                        break\n                    name = name[0:lastDot]\n                if name is not None:\n                    importedPackages.add(name)\n            self._imported_java_packages = frozenset(importedPackages)\n\n    def defined_java_packages(self):\n        \"\"\"Get the immutable set of Java packages defined by the Java sources of this project\"\"\"\n        self._init_packages_and_imports()\n        return self._defined_java_packages\n\n    def extended_java_packages(self):\n        \"\"\"Get the immutable set of Java packages extended by the Java sources of this project\"\"\"\n        self._init_packages_and_imports()\n        return self._extended_java_packages\n\n    def imported_java_packages(self):\n        \"\"\"Get the immutable set of Java packages defined by other Java projects that are\n           imported by the Java sources of this project.\"\"\"\n        self._init_packages_and_imports()\n        return self._imported_java_packages\n\n    def annotation_processors(self):\n        if not hasattr(self, '_annotationProcessors'):\n            ap = set()\n            if hasattr(self, '_declaredAnnotationProcessors'):\n                ap = set(self._declaredAnnotationProcessors)\n\n            # find dependencies that auto-inject themselves as annotation processors to all dependents\n            allDeps = self.all_deps([], includeLibs=False, includeSelf=False, includeAnnotationProcessors=False)\n            for p in allDeps:\n                if hasattr(p, 'annotationProcessorForDependents') and p.annotationProcessorForDependents.lower() == 'true':\n                    ap.add(p.name)\n            self._annotationProcessors = list(ap)\n        return self._annotationProcessors\n\n    def update_current_annotation_processors_file(self):\n        aps = self.annotation_processors()\n        outOfDate = False\n        currentApsFile = join(self.suite.mxDir, 'currentAnnotationProcessors', self.name)\n        currentApsFileExists = exists(currentApsFile)\n        if currentApsFileExists:\n            with open(currentApsFile) as fp:\n                currentAps = [l.strip() for l in fp.readlines()]\n                if currentAps != aps:\n                    outOfDate = True\n        if outOfDate or not currentApsFileExists:\n            if not exists(dirname(currentApsFile)):\n                os.mkdir(dirname(currentApsFile))\n            with open(currentApsFile, 'w') as fp:\n                for ap in aps:\n                    print >> fp, ap\n        return outOfDate\n\n    def make_archive(self, path=None):\n        outputDir = self.output_dir()\n        if not path:\n            path = join(self.dir, self.name + '.jar')\n        with Archiver(path) as arc:\n            for root, _, files in os.walk(outputDir):\n                for f in files:\n                    relpath = root[len(outputDir) + 1:]\n                    arcname = join(relpath, f).replace(os.sep, '/')\n                    arc.zf.write(join(root, f), arcname)\n        return path\n\n\ndef _make_absolute(path, prefix):\n    \"\"\"\n    Makes 'path' absolute if it isn't already by prefixing 'prefix'\n    \"\"\"\n    if not isabs(path):\n        return join(prefix, path)\n    return path\n\ndef _download_file_with_sha1(name, path, urls, sha1, sha1path, resolve, mustExist, sources=False):\n    def _download_lib():\n        print 'Downloading ' + (\"Sources \" if sources else \"\") + name + ' from ' + str(urls)\n        download(path, urls)\n\n    def _sha1Cached():\n        with open(sha1path, 'r') as f:\n            return f.read()[0:40]\n\n    def _writeSha1Cached():\n        with open(sha1path, 'w') as f:\n            f.write(_sha1OfFile())\n\n    def _sha1OfFile():\n        with open(path, 'rb') as f:\n            d = hashlib.sha1()\n            while True:\n                buf = f.read(4096)\n                if not buf:\n                    break\n                d.update(buf)\n            return d.hexdigest()\n\n\n    if resolve and mustExist and not exists(path):\n        assert not len(urls) == 0, 'cannot find required library ' + name + ' ' + path\n        _download_lib()\n\n    if sha1 and not exists(sha1path):\n        _writeSha1Cached()\n\n    if sha1 and sha1 != _sha1Cached():\n        _download_lib()\n        if sha1 != _sha1OfFile():\n            abort(\"SHA1 does not match for \" + name + \". Broken download? SHA1 not updated in projects file?\")\n        _writeSha1Cached()\n\n    return path\n\nclass BaseLibrary(Dependency):\n    def __init__(self, suite, name, optional):\n        Dependency.__init__(self, suite, name)\n        self.optional = optional\n\n    def __ne__(self, other):\n        result = self.__eq__(other)\n        if result is NotImplemented:\n            return result\n        return not result\n\n\"\"\"\nA library that will be provided by the JDK but may be absent.\nAny project or normal library that depends on a missing library\nwill be removed from the global project and library dictionaries\n(i.e., _projects and _libs).\n\nThis mechanism exists primarily to be able to support code\nthat may use functionality in one JDK (e.g., Oracle JDK)\nthat is not present in another JDK (e.g., OpenJDK). A\nmotivating example is the Java Flight Recorder library\nfound in the Oracle JDK.\n\"\"\"\nclass JreLibrary(BaseLibrary):\n    def __init__(self, suite, name, jar, optional):\n        BaseLibrary.__init__(self, suite, name, optional)\n        self.jar = jar\n\n    def __eq__(self, other):\n        if isinstance(other, JreLibrary):\n            return self.jar == other.jar\n        else:\n            return NotImplemented\n\n    def is_present_in_jdk(self, jdk):\n        for e in jdk.bootclasspath().split(os.pathsep):\n            if basename(e) == self.jar:\n                return True\n        for d in jdk.extdirs().split(os.pathsep):\n            if len(d) and self.jar in os.listdir(d):\n                return True\n        for d in jdk.endorseddirs().split(os.pathsep):\n            if len(d) and self.jar in os.listdir(d):\n                return True\n        return False\n\n    def all_deps(self, deps, includeLibs, includeSelf=True, includeJreLibs=False, includeAnnotationProcessors=False):\n        \"\"\"\n        Add the transitive set of dependencies for this JRE library to the 'deps' list.\n        \"\"\"\n        if includeJreLibs and includeSelf and not self in deps:\n            deps.append(self)\n        return deps\n\nclass Library(BaseLibrary):\n    def __init__(self, suite, name, path, optional, urls, sha1, sourcePath, sourceUrls, sourceSha1, deps):\n        BaseLibrary.__init__(self, suite, name, optional)\n        self.path = path.replace('/', os.sep)\n        self.urls = urls\n        self.sha1 = sha1\n        self.sourcePath = sourcePath\n        self.sourceUrls = sourceUrls\n        self.sourceSha1 = sourceSha1\n        self.deps = deps\n        abspath = _make_absolute(self.path, self.suite.dir)\n        if not optional and not exists(abspath):\n            if not len(urls):\n                abort('Non-optional library {} must either exist at {} or specify one or more URLs from which it can be retrieved'.format(name, abspath))\n        for url in urls:\n            if url.endswith('/') != self.path.endswith(os.sep):\n                abort('Path for dependency directory must have a URL ending with \"/\": path=' + self.path + ' url=' + url)\n\n    def __eq__(self, other):\n        if isinstance(other, Library):\n            if len(self.urls) == 0:\n                return self.path == other.path\n            else:\n                return self.urls == other.urls\n        else:\n            return NotImplemented\n\n    def get_path(self, resolve):\n        path = _make_absolute(self.path, self.suite.dir)\n        sha1path = path + '.sha1'\n\n        includedInJDK = getattr(self, 'includedInJDK', None)\n        if includedInJDK and java().javaCompliance >= JavaCompliance(includedInJDK):\n            return None\n\n        return _download_file_with_sha1(self.name, path, self.urls, self.sha1, sha1path, resolve, not self.optional)\n\n    def get_source_path(self, resolve):\n        if self.sourcePath is None:\n            return None\n        path = _make_absolute(self.sourcePath, self.suite.dir)\n        sha1path = path + '.sha1'\n\n        return _download_file_with_sha1(self.name, path, self.sourceUrls, self.sourceSha1, sha1path, resolve, len(self.sourceUrls) != 0, sources=True)\n\n    def append_to_classpath(self, cp, resolve):\n        path = self.get_path(resolve)\n        if path and (exists(path) or not resolve):\n            cp.append(path)\n\n    def all_deps(self, deps, includeLibs, includeSelf=True, includeJreLibs=False, includeAnnotationProcessors=False):\n        \"\"\"\n        Add the transitive set of dependencies for this library to the 'deps' list.\n        \"\"\"\n        if not includeLibs:\n            return deps\n        childDeps = list(self.deps)\n        if self in deps:\n            return deps\n        for name in childDeps:\n            assert name != self.name\n            dep = library(name)\n            if not dep in deps:\n                dep.all_deps(deps, includeLibs=includeLibs, includeJreLibs=includeJreLibs, includeAnnotationProcessors=includeAnnotationProcessors)\n        if not self in deps and includeSelf:\n            deps.append(self)\n        return deps\n\nclass HgConfig:\n    \"\"\"\n    Encapsulates access to Mercurial (hg)\n    \"\"\"\n    def __init__(self):\n        self.missing = 'no hg executable found'\n        self.has_hg = None\n\n    def check(self, abortOnFail=True):\n        if self.has_hg is None:\n            try:\n                subprocess.check_output(['hg'])\n                self.has_hg = True\n            except OSError:\n                self.has_hg = False\n                warn(self.missing)\n\n        if not self.has_hg:\n            if abortOnFail:\n                abort(self.missing)\n            else:\n                warn(self.missing)\n\n    def tip(self, sDir, abortOnError=True):\n        try:\n            return subprocess.check_output(['hg', 'tip', '-R', sDir, '--template', '{node}'])\n        except OSError:\n            warn(self.missing)\n        except subprocess.CalledProcessError:\n            if abortOnError:\n                abort('failed to get tip revision id')\n            else:\n                return None\n\n    def isDirty(self, sDir, abortOnError=True):\n        try:\n            return len(subprocess.check_output(['hg', 'status', '-R', sDir])) > 0\n        except OSError:\n            warn(self.missing)\n        except subprocess.CalledProcessError:\n            if abortOnError:\n                abort('failed to get status')\n            else:\n                return None\n\n    def can_push(self, s, strict=True):\n        try:\n            output = subprocess.check_output(['hg', '-R', s.dir, 'status'])\n            if strict:\n                return output == ''\n            else:\n                if len(output) > 0:\n                    for line in output.split('\\n'):\n                        if len(line) > 0 and not line.startswith('?'):\n                            return False\n                return True\n        except OSError:\n            warn(self.missing)\n        except subprocess.CalledProcessError:\n            return False\n\n    def default_push(self, sdir):\n        with open(join(sdir, '.hg', 'hgrc')) as f:\n            for line in f:\n                line = line.rstrip()\n                if line.startswith('default = '):\n                    return line[len('default = '):]\n        return None\n\nclass SuiteModel:\n    \"\"\"\n    Defines how to locate a URL/path for a suite, including imported suites.\n    Conceptually a SuiteModel is defined by a kind (src,dst), a primary suite URL/path,\n    and a map from suite name to URL/path for imported suites.\n    Subclasses define a specfic implementation.\n    \"\"\"\n    def __init__(self, kind):\n        self.kind = kind\n        self.primaryDir = None\n        self.suitenamemap = {}\n\n    def find_suite_dir(self, suitename):\n        \"\"\"locates the URL/path for suitename or None if not found\"\"\"\n        abort('find_suite_dir not implemented')\n\n    def set_primary_dir(self, d):\n        \"\"\"informs that d is the primary suite directory\"\"\"\n        self._primaryDir = d\n\n    def importee_dir(self, importer_dir, suite_import, check_alternate=True):\n        \"\"\"\n        returns the directory path for an import of suite_import.name, given importer_dir.\n        For a \"src\" suite model, if check_alternate == True and if suite_import specifies an alternate URL,\n        check whether path exists and if not, return the alternate.\n        \"\"\"\n        abort('importee_dir not implemented')\n\n    def nestedsuites_dirname(self):\n        \"\"\"Returns the dirname that contains any nested suites if the model supports that\"\"\"\n        return None\n\n    def _mxDirName(self, name):\n        # temporary workaround until mx.graal exists\n        if name == 'graal':\n            return 'mx'\n        else:\n            return 'mx.' + name\n\n    def _search_dir(self, searchDir, mxDirName):\n        if not exists(searchDir):\n            return None\n        for dd in os.listdir(searchDir):\n            sd = _is_suite_dir(join(searchDir, dd), mxDirName)\n            if sd is not None:\n                return sd\n\n    def _check_exists(self, suite_import, path, check_alternate=True):\n        if check_alternate and self.kind == \"src\" and suite_import.alternate is not None and not exists(path):\n            return suite_import.alternate\n        return path\n\n    def _create_suitenamemap(self, optionspec, suitemap):\n        \"\"\"Three ways to specify a suite name mapping, in order of precedence:\n        1. Explicitly in optionspec.\n        2. In suitemap.\n        3. in MXSUITEMAP environment variable.\n        \"\"\"\n        if optionspec != '':\n            spec = optionspec\n        elif suitemap is not None:\n            spec = suitemap\n        elif get_env('MXSUITEMAP') is not None:\n            spec = get_env('MXSUITEMAP')\n        else:\n            return\n        pairs = spec.split(',')\n        for pair in pairs:\n            mappair = pair.split('=')\n            self.suitenamemap[mappair[0]] = mappair[1]\n\n    @staticmethod\n    def set_suitemodel(kind, option, suitemap):\n        if option.startswith('sibling'):\n            return SiblingSuiteModel(kind, os.getcwd(), option, suitemap)\n        elif option.startswith('nested'):\n            return NestedImportsSuiteModel(kind, os.getcwd(), option, suitemap)\n        elif option.startswith('path'):\n            return PathSuiteModel(kind, option[len('path:'):])\n        else:\n            abort('unknown suitemodel type: ' + option)\n\n    @staticmethod\n    def parse_options():\n        # suite-specific args may match the known args so there is no way at this early stage\n        # to use ArgParser to handle the suite model global arguments, so we just do it manually.\n        def _get_argvalue(arg, args, i):\n            if i < len(args):\n                return args[i]\n            else:\n                abort('value expected with ' + arg)\n\n        args = sys.argv[1:]\n        if len(args) == 0:\n            _argParser.print_help()\n            sys.exit(0)\n\n        if len(args) == 1:\n            if args[0] == '--version':\n                print 'mx version ' + str(version)\n                sys.exit(0)\n            if args[0] == '--help' or args[0] == '-h':\n                _argParser.print_help()\n                sys.exit(0)\n\n        src_suitemodel_arg = dst_suitemodel_arg = 'sibling'\n        suitemap_arg = None\n\n        i = 0\n        while i < len(args):\n            arg = args[i]\n            if arg == '--src-suitemodel':\n                src_suitemodel_arg = _get_argvalue(arg, args, i + 1)\n            elif arg == '--dst-suitemodel':\n                dst_suitemodel_arg = _get_argvalue(arg, args, i + 1)\n            elif arg == '--suitemap':\n                suitemap_arg = _get_argvalue(arg, args, i + 1)\n            elif arg == '-w':\n                # to get warnings on suite loading issues before command line is parsed\n                global _warn\n                _warn = True\n            elif arg == '--primary-suite-path':\n                global _primary_suite_path\n                _primary_suite_path = os.path.abspath(_get_argvalue(arg, args, i + 1))\n            i = i + 1\n\n        global _src_suitemodel\n        _src_suitemodel = SuiteModel.set_suitemodel(\"src\", src_suitemodel_arg, suitemap_arg)\n        global _dst_suitemodel\n        _dst_suitemodel = SuiteModel.set_suitemodel(\"dst\", dst_suitemodel_arg, suitemap_arg)\n\n\nclass SiblingSuiteModel(SuiteModel):\n    \"\"\"All suites are siblings in the same parent directory, recorded as _suiteRootDir\"\"\"\n    def __init__(self, kind, suiteRootDir, option, suitemap):\n        SuiteModel.__init__(self, kind)\n        self._suiteRootDir = suiteRootDir\n        self._create_suitenamemap(option[len('sibling:'):], suitemap)\n\n    def find_suite_dir(self, name):\n        return self._search_dir(self._suiteRootDir, self._mxDirName(name))\n\n    def set_primary_dir(self, d):\n        SuiteModel.set_primary_dir(self, d)\n        self._suiteRootDir = dirname(d)\n\n    def importee_dir(self, importer_dir, suite_import, check_alternate=True):\n        suitename = suite_import.name\n        if self.suitenamemap.has_key(suitename):\n            suitename = self.suitenamemap[suitename]\n        path = join(dirname(importer_dir), suitename)\n        return self._check_exists(suite_import, path, check_alternate)\n\nclass NestedImportsSuiteModel(SuiteModel):\n    \"\"\"Imported suites are all siblings in an 'imported_suites' directory of the primary suite\"\"\"\n    def _imported_suites_dirname(self):\n        return \"imported_suites\"\n\n    def __init__(self, kind, primaryDir, option, suitemap):\n        SuiteModel.__init__(self, kind)\n        self._primaryDir = primaryDir\n        self._create_suitenamemap(option[len('nested:'):], suitemap)\n\n    def find_suite_dir(self, name):\n        return self._search_dir(join(self._primaryDir, self._imported_suites_dirname()), self._mxDirName(name))\n\n    def importee_dir(self, importer_dir, suite_import, check_alternate=True):\n        suitename = suite_import.name\n        if self.suitenamemap.has_key(suitename):\n            suitename = self.suitenamemap[suitename]\n        if basename(importer_dir) == basename(self._primaryDir):\n            # primary is importer\n            this_imported_suites_dirname = join(importer_dir, self._imported_suites_dirname())\n            if not exists(this_imported_suites_dirname):\n                os.mkdir(this_imported_suites_dirname)\n            path = join(this_imported_suites_dirname, suitename)\n        else:\n            path = join(dirname(importer_dir), suitename)\n        return self._check_exists(suite_import, path, check_alternate)\n\n    def nestedsuites_dirname(self):\n        return self._imported_suites_dirname()\n\nclass PathSuiteModel(SuiteModel):\n    \"\"\"The most general model. Uses a map from suitename to URL/path provided by the user\"\"\"\n    def __init__(self, kind, path):\n        SuiteModel.__init__(self, kind)\n        paths = path.split(',')\n        self.suit_to_url = {}\n        for path in paths:\n            pair = path.split('=')\n            if len(pair) > 1:\n                suitename = pair[0]\n                suiteurl = pair[1]\n            else:\n                suitename = basename(pair[0])\n                suiteurl = pair[0]\n            self.suit_to_url[suitename] = suiteurl\n\n    def find_suite_dir(self, suitename):\n        if self.suit_to_url.has_key(suitename):\n            return self.suit_to_url[suitename]\n        else:\n            return None\n\n    def importee_dir(self, importer_dir, suite_import):\n        # since this is completely explicit, we pay no attention to any suite_import.alternate\n        suitename = suite_import.name\n        if suitename in self.suit_to_url:\n            return self.suit_to_url[suitename]\n        else:\n            abort('suite ' + suitename + ' not found')\n\nclass SuiteImport:\n    def __init__(self, name, version, alternate):\n        self.name = name\n        self.version = version\n        self.alternate = alternate\n\n    @staticmethod\n    def parse_specification(specification):\n        parts = specification.split(',')\n        name = parts[0]\n        alternate = None\n        if len(parts) > 1:\n            version = parts[1]\n            if len(version) == 0:\n                version = None\n            if len(parts) > 2:\n                alternate = parts[2]\n        else:\n            version = None\n        return SuiteImport(name, version, alternate)\n\n    @staticmethod\n    def tostring(name, version=None, alternate=None):\n        result = name\n        if version:\n            result = result + ',' + version\n        if alternate is not None:\n            result = result + ',' + alternate\n        return result\n\n    def __str__(self):\n        return SuiteImport.tostring(self.name, self.version, self.alternate)\n\nclass Suite:\n    def __init__(self, mxDir, primary, load=True):\n        self.dir = dirname(mxDir)\n        self.mxDir = mxDir\n        self.projects = []\n        self.libs = []\n        self.jreLibs = []\n        self.dists = []\n        self.imports = []\n        self.commands = None\n        self.primary = primary\n        self.requiredMxVersion = None\n        self.name = _suitename(mxDir)  # validated in _load_projects\n        self.post_init = False\n        if load:\n            # load suites bottom up to make sure command overriding works properly\n            self._load_imports()\n            self._load_env()\n            self._load_commands()\n        _suites[self.name] = self\n\n    def __str__(self):\n        return self.name\n\n    def version(self, abortOnError=True):\n        # we do not cache the version\n        return _hg.tip(self.dir, abortOnError)\n\n    def _load_projects(self):\n        libsMap = dict()\n        jreLibsMap = dict()\n        projsMap = dict()\n        distsMap = dict()\n        projectsFile = join(self.mxDir, 'projects')\n        if not exists(projectsFile):\n            return\n\n        with open(projectsFile) as f:\n            prefix = ''\n            lineNum = 0\n\n            def error(message):\n                abort(projectsFile + ':' + str(lineNum) + ': ' + message)\n\n            for line in f:\n                lineNum = lineNum + 1\n                line = line.strip()\n                if line.endswith('\\\\'):\n                    prefix = prefix + line[:-1]\n                    continue\n                if len(prefix) != 0:\n                    line = prefix + line\n                    prefix = ''\n                if len(line) != 0 and line[0] != '#':\n                    if '=' not in line:\n                        error('non-comment line does not contain an \"=\" character')\n                    key, value = line.split('=', 1)\n\n                    parts = key.split('@')\n\n                    if len(parts) == 1:\n                        if parts[0] == 'suite':\n                            if self.name != value:\n                                error('suite name in project file does not match ' + _suitename(self.mxDir))\n                        elif parts[0] == 'mxversion':\n                            try:\n                                self.requiredMxVersion = VersionSpec(value)\n                            except AssertionError as ae:\n                                error('Exception while parsing \"mxversion\" in project file: ' + str(ae))\n                        else:\n                            error('Single part property must be \"suite\" or \"mxversion\": ' + key)\n                        continue\n                    if len(parts) != 3:\n                        error('Property name does not have 3 parts separated by \"@\": ' + key)\n                    kind, name, attr = parts\n                    if kind == 'project':\n                        m = projsMap\n                    elif kind == 'library':\n                        m = libsMap\n                    elif kind == 'jrelibrary':\n                        m = jreLibsMap\n                    elif kind == 'distribution':\n                        m = distsMap\n                    else:\n                        error('Property name does not start with \"project@\", \"library@\" or \"distribution@\": ' + key)\n\n                    attrs = m.get(name)\n                    if attrs is None:\n                        attrs = dict()\n                        m[name] = attrs\n                    value = expandvars_in_property(value)\n                    attrs[attr] = value\n\n        def pop_list(attrs, name):\n            v = attrs.pop(name, None)\n            if v is None or len(v.strip()) == 0:\n                return []\n            return [n.strip() for n in v.split(',')]\n\n        for name, attrs in projsMap.iteritems():\n            srcDirs = pop_list(attrs, 'sourceDirs')\n            deps = pop_list(attrs, 'dependencies')\n            ap = pop_list(attrs, 'annotationProcessors')\n            # deps += ap\n            javaCompliance = attrs.pop('javaCompliance', None)\n            subDir = attrs.pop('subDir', None)\n            if subDir is None:\n                d = join(self.dir, name)\n            else:\n                d = join(self.dir, subDir, name)\n            workingSets = attrs.pop('workingSets', None)\n            p = Project(self, name, srcDirs, deps, javaCompliance, workingSets, d)\n            p.checkstyleProj = attrs.pop('checkstyle', name)\n            p.native = attrs.pop('native', '') == 'true'\n            if not p.native and p.javaCompliance is None:\n                error('javaCompliance property required for non-native project ' + name)\n            if len(ap) > 0:\n                p._declaredAnnotationProcessors = ap\n            p.__dict__.update(attrs)\n            self.projects.append(p)\n\n        for name, attrs in jreLibsMap.iteritems():\n            jar = attrs.pop('jar')\n            # JRE libraries are optional by default\n            optional = attrs.pop('optional', 'true') != 'false'\n            l = JreLibrary(self, name, jar, optional)\n            self.jreLibs.append(l)\n\n        for name, attrs in libsMap.iteritems():\n            path = attrs.pop('path')\n            urls = pop_list(attrs, 'urls')\n            sha1 = attrs.pop('sha1', None)\n            sourcePath = attrs.pop('sourcePath', None)\n            sourceUrls = pop_list(attrs, 'sourceUrls')\n            sourceSha1 = attrs.pop('sourceSha1', None)\n            deps = pop_list(attrs, 'dependencies')\n            # Add support optional libraries once we have a good use case\n            optional = False\n            l = Library(self, name, path, optional, urls, sha1, sourcePath, sourceUrls, sourceSha1, deps)\n            l.__dict__.update(attrs)\n            self.libs.append(l)\n\n        for name, attrs in distsMap.iteritems():\n            path = attrs.pop('path')\n            sourcesPath = attrs.pop('sourcesPath', None)\n            deps = pop_list(attrs, 'dependencies')\n            mainClass = attrs.pop('mainClass', None)\n            exclDeps = pop_list(attrs, 'exclude')\n            distDeps = pop_list(attrs, 'distDependencies')\n            d = Distribution(self, name, path, sourcesPath, deps, mainClass, exclDeps, distDeps)\n            d.__dict__.update(attrs)\n            self.dists.append(d)\n\n        if self.name is None:\n            abort('Missing \"suite=<name>\" in ' + projectsFile)\n\n    def _commands_name(self):\n        return 'mx_' + self.name.replace('-', '_')\n\n    def _find_commands(self, name):\n        commandsPath = join(self.mxDir, name + '.py')\n        if exists(commandsPath):\n            return name\n        else:\n            return None\n\n    def _load_commands(self):\n        commandsName = self._find_commands(self._commands_name())\n        if commandsName is None:\n            # backwards compatibility\n            commandsName = self._find_commands('commands')\n        if commandsName is not None:\n            if commandsName in sys.modules:\n                abort(commandsName + '.py in suite ' + self.name + ' duplicates ' + sys.modules[commandsName].__file__)\n            # temporarily extend the Python path\n            sys.path.insert(0, self.mxDir)\n            mod = __import__(commandsName)\n\n            self.commands = sys.modules.pop(commandsName)\n            sys.modules[commandsName] = self.commands\n\n            # revert the Python path\n            del sys.path[0]\n\n            if not hasattr(mod, 'mx_init'):\n                abort(commandsName + '.py in suite ' + self.name + ' must define an mx_init(suite) function')\n            if hasattr(mod, 'mx_post_parse_cmd_line'):\n                self.mx_post_parse_cmd_line = mod.mx_post_parse_cmd_line\n\n            mod.mx_init(self)\n            self.commands = mod\n\n    def _imports_file(self):\n        return join(self.mxDir, 'imports')\n\n    def import_timestamp(self):\n        return TimeStampFile(self._imports_file())\n\n    def visit_imports(self, visitor, **extra_args):\n        \"\"\"\n        Visitor support for the imports file.\n        For each line of the imports file that specifies an import, the visitor function is\n        called with this suite, a SuiteImport instance created from the line and any extra args\n        passed to this call. In addition, if extra_args contains a key 'update_versions' that is True,\n        a StringIO value is added to extra_args with key 'updated_imports', and the visitor is responsible\n        for writing a (possibly) updated import line to the file, and the file is (possibly) updated after\n        all imports are processed.\n        N.B. There is no built-in support for avoiding visiting the same suite multiple times,\n        as this function only visits the imports of a single suite. If a (recursive) visitor function\n        wishes to visit a suite exactly once, it must manage that through extra_args.\n        \"\"\"\n        importsFile = self._imports_file()\n        if exists(importsFile):\n            update_versions = extra_args.has_key('update_versions') and extra_args['update_versions']\n            out = StringIO.StringIO() if update_versions else None\n            extra_args['updated_imports'] = out\n            with open(importsFile) as f:\n                for line in f:\n                    sline = line.strip()\n                    if len(sline) == 0 or sline.startswith('#'):\n                        if out is not None:\n                            out.write(sline + '\\n')\n                        continue\n                    suite_import = SuiteImport.parse_specification(line.strip())\n                    visitor(self, suite_import, **extra_args)\n\n            if out is not None:\n                update_file(importsFile, out.getvalue())\n\n    @staticmethod\n    def _find_and_loadsuite(importing_suite, suite_import, **extra_args):\n        \"\"\"visitor for the initial suite load\"\"\"\n        for s in _suites.itervalues():\n            if s.name == suite_import.name:\n                return s\n        importMxDir = _src_suitemodel.find_suite_dir(suite_import.name)\n        if importMxDir is None:\n            fail = False\n            if suite_import.alternate is not None:\n                _hg.check()\n                cmd = ['hg', 'clone']\n                if suite_import.version is not None:\n                    cmd.append('-r')\n                    cmd.append(suite_import.version)\n                cmd.append(suite_import.alternate)\n                cmd.append(_src_suitemodel.importee_dir(importing_suite.dir, suite_import, check_alternate=False))\n                try:\n                    subprocess.check_output(cmd)\n                    importMxDir = _src_suitemodel.find_suite_dir(suite_import.name)\n                    if importMxDir is None:\n                        # wasn't a suite after all\n                        fail = True\n                except subprocess.CalledProcessError:\n                    fail = True\n            else:\n                fail = True\n\n            if fail:\n                if extra_args[\"dynamicImport\"]:\n                    return None\n                else:\n                    abort('import ' + suite_import.name + ' not found')\n        importing_suite.imports.append(suite_import)\n        return Suite(importMxDir, False)\n        # we do not check at this stage whether the tip version of imported_suite\n        # matches that of the import, since during development, this can and will change\n\n    def import_suite(self, name, version=None, alternate=None):\n        \"\"\"Dynamic import of a suite. Returns None if the suite cannot be found\"\"\"\n        suite_import = SuiteImport(name, version, alternate)\n        imported_suite = Suite._find_and_loadsuite(self, suite_import, dynamicImport=True)\n        if imported_suite and not imported_suite.post_init:\n            imported_suite._post_init()\n        return imported_suite\n\n    def _load_imports(self):\n        self.visit_imports(self._find_and_loadsuite)\n\n    def _load_env(self):\n        e = join(self.mxDir, 'env')\n        if exists(e):\n            with open(e) as f:\n                lineNum = 0\n                for line in f:\n                    lineNum = lineNum + 1\n                    line = line.strip()\n                    if len(line) != 0 and line[0] != '#':\n                        if not '=' in line:\n                            abort(e + ':' + str(lineNum) + ': line does not match pattern \"key=value\"')\n                        key, value = line.split('=', 1)\n                        os.environ[key.strip()] = expandvars_in_property(value.strip())\n\n    def _post_init(self):\n        self._load_projects()\n        if self.requiredMxVersion is None:\n            warn(\"This suite does not express any required mx version. Consider adding 'mxversion=<version>' to your projects file.\")\n        elif self.requiredMxVersion > version:\n            abort(\"This suite requires mx version \" + str(self.requiredMxVersion) + \" while your current mx version is \" + str(version) + \". Please update mx.\")\n        # set the global data structures, checking for conflicts unless _check_global_structures is False\n        for p in self.projects:\n            existing = _projects.get(p.name)\n            if existing is not None and _check_global_structures:\n                abort('cannot override project  ' + p.name + ' in ' + p.dir + \" with project of the same name in  \" + existing.dir)\n            if not p.name in _opts.ignored_projects:\n                _projects[p.name] = p\n        for l in self.libs:\n            existing = _libs.get(l.name)\n            # Check that suites that define same library are consistent\n            if existing is not None and existing != l and _check_global_structures:\n                abort('inconsistent library redefinition of ' + l.name + ' in ' + existing.suite.dir + ' and ' + l.suite.dir)\n            _libs[l.name] = l\n        for l in self.jreLibs:\n            existing = _jreLibs.get(l.name)\n            # Check that suites that define same library are consistent\n            if existing is not None and existing != l:\n                abort('inconsistent JRE library redefinition of ' + l.name + ' in ' + existing.suite.dir + ' and ' + l.suite.dir)\n            _jreLibs[l.name] = l\n        for d in self.dists:\n            existing = _dists.get(d.name)\n            if existing is not None and _check_global_structures:\n                # allow redefinition, so use path from existing\n                # abort('cannot redefine distribution  ' + d.name)\n                warn('distribution ' + d.name + ' redefined')\n                d.path = existing.path\n            _dists[d.name] = d\n\n        if hasattr(self, 'mx_post_parse_cmd_line'):\n            self.mx_post_parse_cmd_line(_opts)\n        self.post_init = True\n\n    @staticmethod\n    def _post_init_visitor(importing_suite, suite_import, **extra_args):\n        imported_suite = suite(suite_import.name)\n        if not imported_suite.post_init:\n            imported_suite.visit_imports(imported_suite._post_init_visitor)\n            imported_suite._post_init()\n\n    def _depth_first_post_init(self):\n        '''depth first _post_init driven by imports graph'''\n        self.visit_imports(self._post_init_visitor)\n        self._post_init()\n\n\nclass XMLElement(xml.dom.minidom.Element):\n    def writexml(self, writer, indent=\"\", addindent=\"\", newl=\"\"):\n        writer.write(indent + \"<\" + self.tagName)\n\n        attrs = self._get_attributes()\n        a_names = attrs.keys()\n        a_names.sort()\n\n        for a_name in a_names:\n            writer.write(\" %s=\\\"\" % a_name)\n            xml.dom.minidom._write_data(writer, attrs[a_name].value)\n            writer.write(\"\\\"\")\n        if self.childNodes:\n            if not self.ownerDocument.padTextNodeWithoutSiblings and len(self.childNodes) == 1 and isinstance(self.childNodes[0], xml.dom.minidom.Text):\n                # if the only child of an Element node is a Text node, then the\n                # text is printed without any indentation or new line padding\n                writer.write(\">\")\n                self.childNodes[0].writexml(writer)\n                writer.write(\"</%s>%s\" % (self.tagName, newl))\n            else:\n                writer.write(\">%s\" % (newl))\n                for node in self.childNodes:\n                    node.writexml(writer, indent + addindent, addindent, newl)\n                writer.write(\"%s</%s>%s\" % (indent, self.tagName, newl))\n        else:\n            writer.write(\"/>%s\" % (newl))\n\nclass XMLDoc(xml.dom.minidom.Document):\n\n    def __init__(self):\n        xml.dom.minidom.Document.__init__(self)\n        self.current = self\n        self.padTextNodeWithoutSiblings = False\n\n    def createElement(self, tagName):\n        # overwritten to create XMLElement\n        e = XMLElement(tagName)\n        e.ownerDocument = self\n        return e\n\n    def comment(self, txt):\n        self.current.appendChild(self.createComment(txt))\n\n    def open(self, tag, attributes=None, data=None):\n        if attributes is None:\n            attributes = {}\n        element = self.createElement(tag)\n        for key, value in attributes.items():\n            element.setAttribute(key, value)\n        self.current.appendChild(element)\n        self.current = element\n        if data is not None:\n            element.appendChild(self.createTextNode(data))\n        return self\n\n    def close(self, tag):\n        assert self.current != self\n        assert tag == self.current.tagName, str(tag) + ' != ' + self.current.tagName\n        self.current = self.current.parentNode\n        return self\n\n    def element(self, tag, attributes=None, data=None):\n        if attributes is None:\n            attributes = {}\n        return self.open(tag, attributes, data).close(tag)\n\n    def xml(self, indent='', newl='', escape=False, standalone=None):\n        assert self.current == self\n        result = self.toprettyxml(indent, newl, encoding=\"UTF-8\")\n        if escape:\n            entities = {'\"':  \"&quot;\", \"'\":  \"&apos;\", '\\n': '&#10;'}\n            result = xml.sax.saxutils.escape(result, entities)\n        if standalone is not None:\n            result = result.replace('encoding=\"UTF-8\"?>', 'encoding=\"UTF-8\" standalone=\"' + str(standalone) + '\"?>')\n        return result\n\nclass GateTask:\n    def __init__(self, title):\n        self.start = time.time()\n        self.title = title\n        self.end = None\n        self.duration = None\n        log(time.strftime('gate: %d %b %Y %H:%M:%S: BEGIN: ') + title)\n    def stop(self):\n        self.end = time.time()\n        self.duration = datetime.timedelta(seconds=self.end - self.start)\n        log(time.strftime('gate: %d %b %Y %H:%M:%S: END:   ') + self.title + ' [' + str(self.duration) + ']')\n        return self\n    def abort(self, codeOrMessage):\n        self.end = time.time()\n        self.duration = datetime.timedelta(seconds=self.end - self.start)\n        log(time.strftime('gate: %d %b %Y %H:%M:%S: ABORT: ') + self.title + ' [' + str(self.duration) + ']')\n        abort(codeOrMessage)\n        return self\n\ndef _basic_gate_body(args, tasks):\n    return\n\ndef _add_omit_clean_args(parser):\n    parser.add_argument('-j', '--omit-java-clean', action='store_false', dest='cleanJava', help='omit cleaning Java native code')\n    parser.add_argument('-n', '--omit-native-clean', action='store_false', dest='cleanNative', help='omit cleaning and building native code')\n    parser.add_argument('-e', '--omit-ide-clean', action='store_false', dest='cleanIDE', help='omit ideclean/ideinit')\n    parser.add_argument('-d', '--omit-dist-clean', action='store_false', dest='cleanDist', help='omit cleaning distributions')\n    parser.add_argument('-o', '--omit-clean', action='store_true', dest='noClean', help='equivalent to -j -n -e')\n\ndef gate(args, gate_body=_basic_gate_body, parser=None):\n    \"\"\"run the tests used to validate a push\n    This provides a generic gate that does all the standard things.\n    Additional tests can be provided by passing a custom 'gate_body'.\n\n    If this command exits with a 0 exit code, then the gate passed.\"\"\"\n\n    suppliedParser = parser is not None\n    parser = parser if suppliedParser else ArgumentParser(prog='mx gate')\n\n    _add_omit_clean_args(parser)\n    parser.add_argument('-p', '--omit-pylint', action='store_false', dest='pylint', help='omit pylint check')\n    if suppliedParser:\n        parser.add_argument('remainder', nargs=REMAINDER, metavar='...')\n    args = parser.parse_args(args)\n\n    if args.noClean:\n        args.cleanIDE = False\n        args.cleanJava = False\n        args.cleanNative = False\n        args.cleanDist = False\n\n    tasks = []\n    total = GateTask('Gate')\n\n    try:\n        if args.pylint:\n            t = GateTask('Pylint')\n            pylint([])\n            tasks.append(t.stop())\n\n        t = GateTask('Clean')\n        cleanArgs = []\n        if not args.cleanNative:\n            cleanArgs.append('--no-native')\n        if not args.cleanJava:\n            cleanArgs.append('--no-java')\n        if not args.cleanDist:\n            cleanArgs.append('--no-dist')\n        command_function('clean')(cleanArgs)\n        tasks.append(t.stop())\n\n        if args.cleanIDE:\n            t = GateTask('IDEConfigCheck')\n            command_function('ideclean')([])\n            command_function('ideinit')([])\n            tasks.append(t.stop())\n\n        eclipse_exe = os.environ.get('ECLIPSE_EXE')\n        if eclipse_exe is not None:\n            t = GateTask('CodeFormatCheck')\n            if eclipseformat(['-e', eclipse_exe]) != 0:\n                t.abort('Formatter modified files - run \"mx eclipseformat\", check in changes and repush')\n            tasks.append(t.stop())\n\n        t = GateTask('Canonicalization Check')\n        log(time.strftime('%d %b %Y %H:%M:%S - Ensuring mx/projects files are canonicalized...'))\n        if canonicalizeprojects([]) != 0:\n            t.abort('Rerun \"mx canonicalizeprojects\" and check-in the modified mx/projects files.')\n        tasks.append(t.stop())\n\n        t = GateTask('BuildJava')\n        # Make sure we use any overridden build command\n        command_function('build')(['-p', '--no-native', '--jdt-warning-as-error'])\n        tasks.append(t.stop())\n\n        gate_body(args, tasks)\n\n    except KeyboardInterrupt:\n        total.abort(1)\n\n    except BaseException as e:\n        import traceback\n        traceback.print_exc()\n        total.abort(str(e))\n\n    total.stop()\n\n    log('Gate task times:')\n    for t in tasks:\n        log('  ' + str(t.duration) + '\\t' + t.title)\n    log('  =======')\n    log('  ' + str(total.duration))\n\ndef _basic_bench_harness(args, vmArgs):\n    return 0\n\ndef bench(args, harness=_basic_bench_harness, parser=None):\n    '''run benchmarks (suite-specfic) after clean build (optional)'''\n    suppliedParser = parser is not None\n    parser = parser if suppliedParser else ArgumentParser(prog='mx bench')\n    parser.add_argument('--J', dest='vm_args', help='target VM arguments (e.g. --J @-dsa)', metavar='@<args>')\n    _add_omit_clean_args(parser)\n    if suppliedParser:\n        parser.add_argument('remainder', nargs=REMAINDER, metavar='...')\n    args = parser.parse_args(args)\n\n    if args.noClean:\n        args.cleanIDE = False\n        args.cleanJava = False\n        args.cleanNative = False\n        args.cleanDist = False\n\n    cleanArgs = []\n    if not args.cleanNative:\n        cleanArgs.append('--no-native')\n    if not args.cleanJava:\n        cleanArgs.append('--no-java')\n    if not args.cleanDist:\n        cleanArgs.append('--no-dist')\n    command_function('clean')(cleanArgs)\n\n    if args.cleanIDE:\n        command_function('ideclean')([])\n        command_function('ideinit')([])\n\n    command_function('build')([])\n\n    return harness(args, args.vm_args)\n\ndef get_os():\n    \"\"\"\n    Get a canonical form of sys.platform.\n    \"\"\"\n    if sys.platform.startswith('darwin'):\n        return 'darwin'\n    elif sys.platform.startswith('linux'):\n        return 'linux'\n    elif sys.platform.startswith('sunos'):\n        return 'solaris'\n    elif sys.platform.startswith('win32') or sys.platform.startswith('cygwin'):\n        return 'windows'\n    else:\n        abort('Unknown operating system ' + sys.platform)\n\ndef suites(opt_limit_to_suite=False):\n    \"\"\"\n    Get the list of all loaded suites.\n    \"\"\"\n    if opt_limit_to_suite and _opts.specific_suites:\n        result = []\n        for s in _suites.values():\n            if s.name in _opts.specific_suites:\n                result.append(s)\n        return result\n    else:\n        return _suites.values()\n\ndef createsuite(args):\n    \"\"\"create new suite in a subdirectory of cwd\"\"\"\n    parser = ArgumentParser(prog='mx createsuite')\n    parser.add_argument('--name', help='suite name', required=True)\n    parser.add_argument('--py', action='store_true', help='create (empty) extensions file')\n    args = parser.parse_args(args)\n\n    suite_name = args.name\n    if exists(suite_name):\n        abort('suite directory already exists')\n    os.mkdir(suite_name)\n    mx_dot_suite_name = 'mx.' + suite_name\n    mxDirPath = join(suite_name, mx_dot_suite_name)\n    os.mkdir(mxDirPath)\n\n    def update_file(template_file, target_file):\n        with open(join(dirname(__file__), 'templates', template_file)) as f:\n            content = f.read()\n        with open(join(mxDirPath, target_file), 'w') as f:\n            f.write(content.replace('MXPROJECT', mx_dot_suite_name))\n\n    with open(join(mxDirPath, 'projects'), 'w') as f:\n        f.write('suite=' + suite_name + '\\n')\n        f.write('mxversion=' + str(version) + '\\n')\n\n    update_file('hg-ignore', '.hgignore')\n\n    _hg.check()\n    run(['hg', 'init'], cwd=suite_name)\n\n    if args.py:\n        with open(join(mxDirPath, 'mx_' + suite_name + '.py'), 'w') as f:\n            f.write('import mx\\n\\n')\n            f.write('def mx_init(suite):\\n')\n            f.write('    commands = {\\n')\n            f.write('    }\\n')\n            f.write('    mx.update_commands(suite, commands)\\n')\n\n        update_file('eclipse-pyproject', '.project')\n        update_file('eclipse-pydevproject', '.pydevproject')\n\ndef suite(name, fatalIfMissing=True):\n    \"\"\"\n    Get the suite for a given name.\n    \"\"\"\n    s = _suites.get(name)\n    if s is None and fatalIfMissing:\n        abort('suite named ' + name + ' not found')\n    return s\n\n\ndef projects_from_names(projectNames):\n    \"\"\"\n    Get the list of projects corresponding to projectNames; all projects if None\n    \"\"\"\n    if projectNames is None:\n        return projects()\n    else:\n        return [project(name) for name in projectNames]\n\ndef projects(opt_limit_to_suite=False):\n    \"\"\"\n    Get the list of all loaded projects limited by --suite option if opt_limit_to_suite == True\n    \"\"\"\n\n    if opt_limit_to_suite:\n        return _projects_opt_limit_to_suites(_projects.values())\n    else:\n        return _projects.values()\n\ndef projects_opt_limit_to_suites():\n    \"\"\"\n    Get the list of all loaded projects optionally limited by --suite option\n    \"\"\"\n    return projects(True)\n\ndef _projects_opt_limit_to_suites(projects):\n    if not _opts.specific_suites:\n        return projects\n    else:\n        result = []\n        for p in projects:\n            s = p.suite\n            if s.name in _opts.specific_suites:\n                result.append(p)\n        return result\n\ndef annotation_processors():\n    \"\"\"\n    Get the list of all loaded projects that define an annotation processor.\n    \"\"\"\n    global _annotationProcessors\n    if _annotationProcessors is None:\n        aps = set()\n        for p in projects():\n            for ap in p.annotation_processors():\n                if project(ap, False):\n                    aps.add(ap)\n        _annotationProcessors = list(aps)\n    return _annotationProcessors\n\ndef distribution(name, fatalIfMissing=True):\n    \"\"\"\n    Get the distribution for a given name. This will abort if the named distribution does\n    not exist and 'fatalIfMissing' is true.\n    \"\"\"\n    d = _dists.get(name)\n    if d is None and fatalIfMissing:\n        abort('distribution named ' + name + ' not found')\n    return d\n\ndef dependency(name, fatalIfMissing=True):\n    \"\"\"\n    Get the project or library for a given name. This will abort if a project  or library does\n    not exist for 'name' and 'fatalIfMissing' is true.\n    \"\"\"\n    d = _projects.get(name)\n    if d is None:\n        d = _libs.get(name)\n        if d is None:\n            d = _jreLibs.get(name)\n    if d is None and fatalIfMissing:\n        if name in _opts.ignored_projects:\n            abort('project named ' + name + ' is ignored')\n        abort('project or library named ' + name + ' not found')\n    return d\n\ndef project(name, fatalIfMissing=True):\n    \"\"\"\n    Get the project for a given name. This will abort if the named project does\n    not exist and 'fatalIfMissing' is true.\n    \"\"\"\n    p = _projects.get(name)\n    if p is None and fatalIfMissing:\n        if name in _opts.ignored_projects:\n            abort('project named ' + name + ' is ignored')\n        abort('project named ' + name + ' not found')\n    return p\n\ndef library(name, fatalIfMissing=True):\n    \"\"\"\n    Gets the library for a given name. This will abort if the named library does\n    not exist and 'fatalIfMissing' is true.\n    \"\"\"\n    l = _libs.get(name)\n    if l is None and fatalIfMissing:\n        if _projects.get(name):\n            abort(name + ' is a project, not a library')\n        abort('library named ' + name + ' not found')\n    return l\n\ndef _as_classpath(deps, resolve):\n    cp = []\n    if _opts.cp_prefix is not None:\n        cp = [_opts.cp_prefix]\n    for d in deps:\n        d.append_to_classpath(cp, resolve)\n    if _opts.cp_suffix is not None:\n        cp += [_opts.cp_suffix]\n    return os.pathsep.join(cp)\n\ndef classpath(names=None, resolve=True, includeSelf=True, includeBootClasspath=False):\n    \"\"\"\n    Get the class path for a list of given dependencies and distributions, resolving each entry in the\n    path (e.g. downloading a missing library) if 'resolve' is true.\n    \"\"\"\n    if names is None:\n        deps = sorted_deps(includeLibs=True)\n        dists = list(_dists.values())\n    else:\n        deps = []\n        dists = []\n        if isinstance(names, types.StringTypes):\n            names = [names]\n        for n in names:\n            dep = dependency(n, fatalIfMissing=False)\n            if dep:\n                dep.all_deps(deps, True, includeSelf)\n            else:\n                dist = distribution(n)\n                if not dist:\n                    abort('project, library or distribution named ' + n + ' not found')\n                dists.append(dist)\n\n    if len(dists):\n        distsDeps = set()\n        for d in dists:\n            distsDeps.update(d.sorted_deps())\n\n        # remove deps covered by a dist that will be on the class path\n        deps = [d for d in deps if d not in distsDeps]\n\n    result = _as_classpath(deps, resolve)\n\n    # prepend distributions\n    if len(dists):\n        distsCp = os.pathsep.join(dist.path for dist in dists)\n        if len(result):\n            result = distsCp + os.pathsep + result\n        else:\n            result = distsCp\n\n    if includeBootClasspath:\n        result = os.pathsep.join([java().bootclasspath(), result])\n    return result\n\ndef classpath_walk(names=None, resolve=True, includeSelf=True, includeBootClasspath=False):\n    \"\"\"\n    Walks the resources available in a given classpath, yielding a tuple for each resource\n    where the first member of the tuple is a directory path or ZipFile object for a\n    classpath entry and the second member is the qualified path of the resource relative\n    to the classpath entry.\n    \"\"\"\n    cp = classpath(names, resolve, includeSelf, includeBootClasspath)\n    for entry in cp.split(os.pathsep):\n        if not exists(entry):\n            continue\n        if isdir(entry):\n            for root, dirs, files in os.walk(entry):\n                for d in dirs:\n                    entryPath = join(root[len(entry) + 1:], d)\n                    yield entry, entryPath\n                for f in files:\n                    entryPath = join(root[len(entry) + 1:], f)\n                    yield entry, entryPath\n        elif entry.endswith('.jar') or entry.endswith('.zip'):\n            with zipfile.ZipFile(entry, 'r') as zf:\n                for zi in zf.infolist():\n                    entryPath = zi.filename\n                    yield zf, entryPath\n\ndef sorted_deps(projectNames=None, includeLibs=False, includeJreLibs=False, includeAnnotationProcessors=False):\n    \"\"\"\n    Gets projects and libraries sorted such that dependencies\n    are before the projects that depend on them. Unless 'includeLibs' is\n    true, libraries are omitted from the result.\n    \"\"\"\n    projects = projects_from_names(projectNames)\n\n    return sorted_project_deps(projects, includeLibs=includeLibs, includeJreLibs=includeJreLibs, includeAnnotationProcessors=includeAnnotationProcessors)\n\ndef sorted_project_deps(projects, includeLibs=False, includeJreLibs=False, includeAnnotationProcessors=False):\n    deps = []\n    for p in projects:\n        p.all_deps(deps, includeLibs=includeLibs, includeJreLibs=includeJreLibs, includeAnnotationProcessors=includeAnnotationProcessors)\n    return deps\n\ndef _handle_missing_java_home():\n    if not sys.stdout.isatty():\n        abort('Could not find bootstrap JDK. Use --java-home option or ensure JAVA_HOME environment variable is set.')\n\n    candidateJdks = []\n    if get_os() == 'darwin':\n        base = '/Library/Java/JavaVirtualMachines'\n        candidateJdks = [join(base, n, 'Contents/Home') for n in os.listdir(base) if exists(join(base, n, 'Contents/Home'))]\n    elif get_os() == 'linux':\n        base = '/usr/lib/jvm'\n        candidateJdks = [join(base, n) for n in os.listdir(base) if exists(join(base, n, 'jre/lib/rt.jar'))]\n    elif get_os() == 'solaris':\n        base = '/usr/jdk/instances'\n        candidateJdks = [join(base, n) for n in os.listdir(base) if exists(join(base, n, 'jre/lib/rt.jar'))]\n    elif get_os() == 'windows':\n        base = r'C:\\Program Files\\Java'\n        candidateJdks = [join(base, n) for n in os.listdir(base) if exists(join(base, n, r'jre\\lib\\rt.jar'))]\n\n    javaHome = None\n    if len(candidateJdks) != 0:\n        javaHome = select_items(candidateJdks + ['<other>'], allowMultiple=False)\n        if javaHome == '<other>':\n            javaHome = None\n\n    while javaHome is None:\n        javaHome = raw_input('Enter path of bootstrap JDK: ')\n        rtJarPath = join(javaHome, 'jre', 'lib', 'rt.jar')\n        if not exists(rtJarPath):\n            log('Does not appear to be a valid JDK as ' + rtJarPath + ' does not exist')\n            javaHome = None\n        else:\n            break\n\n    if _primary_suite is not None:\n        envPath = join(_primary_suite.mxDir, 'env')\n        if ask_yes_no('Persist this setting by adding \"JAVA_HOME=' + javaHome + '\" to ' + envPath, 'y'):\n            with open(envPath, 'a') as fp:\n                print >> fp, 'JAVA_HOME=' + javaHome\n\n    return javaHome\n\nclass ArgParser(ArgumentParser):\n    # Override parent to append the list of available commands\n    def format_help(self):\n        return ArgumentParser.format_help(self) + _format_commands()\n\n\n    def __init__(self):\n        self.java_initialized = False\n        # this doesn't resolve the right way, but can't figure out how to override _handle_conflict_resolve in _ActionsContainer\n        ArgumentParser.__init__(self, prog='mx', conflict_handler='resolve')\n\n        self.add_argument('-v', action='store_true', dest='verbose', help='enable verbose output')\n        self.add_argument('-V', action='store_true', dest='very_verbose', help='enable very verbose output')\n        self.add_argument('-w', action='store_true', dest='warn', help='enable warning messages')\n        self.add_argument('-p', '--primary-suite-path', help='set the primary suite directory', metavar='<path>')\n        self.add_argument('--dbg', type=int, dest='java_dbg_port', help='make Java processes wait on <port> for a debugger', metavar='<port>')\n        self.add_argument('-d', action='store_const', const=8000, dest='java_dbg_port', help='alias for \"-dbg 8000\"')\n        self.add_argument('--cp-pfx', dest='cp_prefix', help='class path prefix', metavar='<arg>')\n        self.add_argument('--cp-sfx', dest='cp_suffix', help='class path suffix', metavar='<arg>')\n        self.add_argument('--J', dest='java_args', help='Java VM arguments (e.g. --J @-dsa)', metavar='@<args>')\n        self.add_argument('--Jp', action='append', dest='java_args_pfx', help='prefix Java VM arguments (e.g. --Jp @-dsa)', metavar='@<args>', default=[])\n        self.add_argument('--Ja', action='append', dest='java_args_sfx', help='suffix Java VM arguments (e.g. --Ja @-dsa)', metavar='@<args>', default=[])\n        self.add_argument('--user-home', help='users home directory', metavar='<path>', default=os.path.expanduser('~'))\n        self.add_argument('--java-home', help='primary JDK directory (must be JDK 7 or later)', metavar='<path>')\n        self.add_argument('--extra-java-homes', help='secondary JDK directories separated by \"' + os.pathsep + '\"', metavar='<path>')\n        self.add_argument('--ignore-project', action='append', dest='ignored_projects', help='name of project to ignore', metavar='<name>', default=[])\n        self.add_argument('--kill-with-sigquit', action='store_true', dest='killwithsigquit', help='send sigquit first before killing child processes')\n        self.add_argument('--suite', action='append', dest='specific_suites', help='limit command to given suite', default=[])\n        self.add_argument('--src-suitemodel', help='mechanism for locating imported suites', metavar='<arg>', default='sibling')\n        self.add_argument('--dst-suitemodel', help='mechanism for placing cloned/pushed suites', metavar='<arg>', default='sibling')\n        self.add_argument('--suitemap', help='explicit remapping of suite names', metavar='<args>')\n        self.add_argument('--primary', action='store_true', help='limit command to primary suite')\n        self.add_argument('--no-download-progress', action='store_true', help='disable download progress meter')\n        self.add_argument('--version', action='store_true', help='print version and exit')\n        if get_os() != 'windows':\n            # Time outs are (currently) implemented with Unix specific functionality\n            self.add_argument('--timeout', help='timeout (in seconds) for command', type=int, default=0, metavar='<secs>')\n            self.add_argument('--ptimeout', help='timeout (in seconds) for subprocesses', type=int, default=0, metavar='<secs>')\n\n    def _parse_cmd_line(self, args=None):\n        if args is None:\n            args = sys.argv[1:]\n\n        self.add_argument('commandAndArgs', nargs=REMAINDER, metavar='command args...')\n\n        opts = self.parse_args()\n\n        # Give the timeout options a default value to avoid the need for hasattr() tests\n        opts.__dict__.setdefault('timeout', 0)\n        opts.__dict__.setdefault('ptimeout', 0)\n\n        if opts.very_verbose:\n            opts.verbose = True\n\n        if opts.java_home is None:\n            opts.java_home = os.environ.get('JAVA_HOME')\n        if opts.extra_java_homes is None:\n            opts.extra_java_homes = os.environ.get('EXTRA_JAVA_HOMES')\n\n        if opts.java_home is None or opts.java_home == '':\n            opts.java_home = _handle_missing_java_home()\n\n        if opts.user_home is None or opts.user_home == '':\n            abort('Could not find user home. Use --user-home option or ensure HOME environment variable is set.')\n\n        if opts.primary and _primary_suite:\n            opts.specific_suites.append(_primary_suite.name)\n\n        os.environ['JAVA_HOME'] = opts.java_home\n        os.environ['HOME'] = opts.user_home\n\n        opts.ignored_projects = opts.ignored_projects + os.environ.get('IGNORED_PROJECTS', '').split(',')\n\n        commandAndArgs = opts.__dict__.pop('commandAndArgs')\n        return opts, commandAndArgs\n\n    def _handle_conflict_resolve(self, action, conflicting_actions):\n        self._handle_conflict_error(action, conflicting_actions)\n\ndef _format_commands():\n    msg = '\\navailable commands:\\n\\n'\n    for cmd in sorted(_commands.iterkeys()):\n        c, _ = _commands[cmd][:2]\n        doc = c.__doc__\n        if doc is None:\n            doc = ''\n        msg += ' {0:<20} {1}\\n'.format(cmd, doc.split('\\n', 1)[0])\n    return msg + '\\n'\n\ndef java(requiredCompliance=None):\n    \"\"\"\n    Get a JavaConfig object containing Java commands launch details.\n    If requiredCompliance is None, the compliance level specified by --java-home/JAVA_HOME\n    is returned. Otherwise, the JavaConfig exactly matching requiredCompliance is returned\n    or None if there is no exact match.\n    \"\"\"\n    assert _java_homes\n    if not requiredCompliance:\n        return _java_homes[0]\n    for java in _java_homes:\n        if java.javaCompliance == requiredCompliance:\n            return java\n    return None\n\ndef run_java(args, nonZeroIsFatal=True, out=None, err=None, cwd=None, addDefaultArgs=True, javaConfig=None):\n    if not javaConfig:\n        javaConfig = java()\n    return run(javaConfig.format_cmd(args, addDefaultArgs), nonZeroIsFatal=nonZeroIsFatal, out=out, err=err, cwd=cwd)\n\ndef _kill_process_group(pid, sig):\n    pgid = os.getpgid(pid)\n    try:\n        os.killpg(pgid, sig)\n        return True\n    except:\n        log('Error killing subprocess ' + str(pgid) + ': ' + str(sys.exc_info()[1]))\n        return False\n\ndef _waitWithTimeout(process, args, timeout):\n    def _waitpid(pid):\n        while True:\n            try:\n                return os.waitpid(pid, os.WNOHANG)\n            except OSError, e:\n                if e.errno == errno.EINTR:\n                    continue\n                raise\n\n    def _returncode(status):\n        if os.WIFSIGNALED(status):\n            return -os.WTERMSIG(status)\n        elif os.WIFEXITED(status):\n            return os.WEXITSTATUS(status)\n        else:\n            # Should never happen\n            raise RuntimeError(\"Unknown child exit status!\")\n\n    end = time.time() + timeout\n    delay = 0.0005\n    while True:\n        (pid, status) = _waitpid(process.pid)\n        if pid == process.pid:\n            return _returncode(status)\n        remaining = end - time.time()\n        if remaining <= 0:\n            abort('Process timed out after {0} seconds: {1}'.format(timeout, ' '.join(args)))\n        delay = min(delay * 2, remaining, .05)\n        time.sleep(delay)\n\n# Makes the current subprocess accessible to the abort() function\n# This is a list of tuples of the subprocess.Popen or\n# multiprocessing.Process object and args.\n_currentSubprocesses = []\n\ndef _addSubprocess(p, args):\n    entry = (p, args)\n    _currentSubprocesses.append(entry)\n    return entry\n\ndef _removeSubprocess(entry):\n    if entry and entry in _currentSubprocesses:\n        try:\n            _currentSubprocesses.remove(entry)\n        except:\n            pass\n\ndef waitOn(p):\n    if get_os() == 'windows':\n        # on windows use a poll loop, otherwise signal does not get handled\n        retcode = None\n        while retcode == None:\n            retcode = p.poll()\n            time.sleep(0.05)\n    else:\n        retcode = p.wait()\n    return retcode\n\ndef run(args, nonZeroIsFatal=True, out=None, err=None, cwd=None, timeout=None, env=None):\n    \"\"\"\n    Run a command in a subprocess, wait for it to complete and return the exit status of the process.\n    If the exit status is non-zero and `nonZeroIsFatal` is true, then mx is exited with\n    the same exit status.\n    Each line of the standard output and error streams of the subprocess are redirected to\n    out and err if they are callable objects.\n    \"\"\"\n\n    assert isinstance(args, types.ListType), \"'args' must be a list: \" + str(args)\n    for arg in args:\n        assert isinstance(arg, types.StringTypes), 'argument is not a string: ' + str(arg)\n\n    if env is None:\n        env = os.environ\n\n    if _opts.verbose:\n        if _opts.very_verbose:\n            log('Environment variables:')\n            for key in sorted(env.keys()):\n                log('    ' + key + '=' + env[key])\n        log(' '.join(map(pipes.quote, args)))\n\n    if timeout is None and _opts.ptimeout != 0:\n        timeout = _opts.ptimeout\n\n    sub = None\n\n    try:\n        # On Unix, the new subprocess should be in a separate group so that a timeout alarm\n        # can use os.killpg() to kill the whole subprocess group\n        preexec_fn = None\n        creationflags = 0\n        if get_os() == 'windows':\n            creationflags = subprocess.CREATE_NEW_PROCESS_GROUP\n        elif timeout is not None:\n            preexec_fn = os.setsid\n\n        def redirect(stream, f):\n            for line in iter(stream.readline, ''):\n                f(line)\n            stream.close()\n        stdout = out if not callable(out) else subprocess.PIPE\n        stderr = err if not callable(err) else subprocess.PIPE\n        p = subprocess.Popen(args, cwd=cwd, stdout=stdout, stderr=stderr, preexec_fn=preexec_fn, creationflags=creationflags, env=env)\n        sub = _addSubprocess(p, args)\n        joiners = []\n        if callable(out):\n            t = Thread(target=redirect, args=(p.stdout, out))\n            # Don't make the reader thread a daemon otherwise output can be droppped\n            t.start()\n            joiners.append(t)\n        if callable(err):\n            t = Thread(target=redirect, args=(p.stderr, err))\n            # Don't make the reader thread a daemon otherwise output can be droppped\n            t.start()\n            joiners.append(t)\n        while any([t.is_alive() for t in joiners]):\n            # Need to use timeout otherwise all signals (including CTRL-C) are blocked\n            # see: http://bugs.python.org/issue1167930\n            for t in joiners:\n                t.join(10)\n        if timeout is None or timeout == 0:\n            retcode = waitOn(p)\n        else:\n            if get_os() == 'windows':\n                abort('Use of timeout not (yet) supported on Windows')\n            retcode = _waitWithTimeout(p, args, timeout)\n    except OSError as e:\n        log('Error executing \\'' + ' '.join(args) + '\\': ' + str(e))\n        if _opts.verbose:\n            raise e\n        abort(e.errno)\n    except KeyboardInterrupt:\n        abort(1)\n    finally:\n        _removeSubprocess(sub)\n\n    if retcode and nonZeroIsFatal:\n        if _opts.verbose:\n            if _opts.very_verbose:\n                raise subprocess.CalledProcessError(retcode, ' '.join(args))\n            else:\n                log('[exit code: ' + str(retcode) + ']')\n        abort(retcode)\n\n    return retcode\n\ndef exe_suffix(name):\n    \"\"\"\n    Gets the platform specific suffix for an executable\n    \"\"\"\n    if get_os() == 'windows':\n        return name + '.exe'\n    return name\n\ndef add_lib_prefix(name):\n    \"\"\"\n    Adds the platform specific library prefix to a name\n    \"\"\"\n    os = get_os()\n    if os == 'linux' or os == 'solaris' or os == 'darwin':\n        return 'lib' + name\n    return name\n\ndef add_lib_suffix(name):\n    \"\"\"\n    Adds the platform specific library suffix to a name\n    \"\"\"\n    os = get_os()\n    if os == 'windows':\n        return name + '.dll'\n    if os == 'linux' or os == 'solaris':\n        return name + '.so'\n    if os == 'darwin':\n        return name + '.dylib'\n    return name\n\n\"\"\"\nUtility for filtering duplicate lines.\n\"\"\"\nclass DuplicateSuppressingStream:\n    \"\"\"\n    Creates an object that will suppress duplicate lines sent to 'out'.\n    The lines considered for suppression are those that contain one of the\n    strings in 'restrictTo' if it is not None.\n    \"\"\"\n    def __init__(self, restrictTo=None, out=sys.stdout):\n        self.restrictTo = restrictTo\n        self.seen = set()\n        self.out = out\n        self.currentFilteredLineCount = 0\n        self.currentFilteredTime = None\n\n    def isSuppressionCandidate(self, line):\n        if self.restrictTo:\n            for p in self.restrictTo:\n                if p in line:\n                    return True\n            return False\n        else:\n            return True\n\n    def write(self, line):\n        if self.isSuppressionCandidate(line):\n            if line in self.seen:\n                self.currentFilteredLineCount += 1\n                if self.currentFilteredTime:\n                    if time.time() - self.currentFilteredTime > 1 * 60:\n                        self.out.write(\"  Filtered \" + str(self.currentFilteredLineCount) + \" repeated lines...\\n\")\n                        self.currentFilteredTime = time.time()\n                else:\n                    self.currentFilteredTime = time.time()\n                return\n            self.seen.add(line)\n        self.currentFilteredLineCount = 0\n        self.out.write(line)\n        self.currentFilteredTime = None\n\n\"\"\"\nA JavaCompliance simplifies comparing Java compliance values extracted from a JDK version string.\n\"\"\"\nclass JavaCompliance:\n    def __init__(self, ver):\n        m = re.match(r'1\\.(\\d+).*', ver)\n        assert m is not None, 'not a recognized version string: ' + ver\n        self.value = int(m.group(1))\n\n    def __str__(self):\n        return '1.' + str(self.value)\n\n    def __cmp__(self, other):\n        if isinstance(other, types.StringType):\n            other = JavaCompliance(other)\n\n        return cmp(self.value, other.value)\n\n    def __hash__(self):\n        return self.value.__hash__()\n\n\"\"\"\nA version specification as defined in JSR-56\n\"\"\"\nclass VersionSpec:\n    def __init__(self, versionString):\n        validChar = r'[\\x21-\\x25\\x27-\\x29\\x2c\\x2f-\\x5e\\x60-\\x7f]'\n        separator = r'[.\\-_]'\n        m = re.match(\"^\" + validChar + '+(' + separator + validChar + '+)*$', versionString)\n        assert m is not None, 'not a recognized version string: ' + versionString\n        self.versionString = versionString\n        self.parts = [int(f) if f.isdigit() else f for f in re.split(separator, versionString)]\n\n    def __str__(self):\n        return self.versionString\n\n    def __cmp__(self, other):\n        return cmp(self.parts, other.parts)\n\ndef _filter_non_existant_paths(paths):\n    return os.pathsep.join([path for path in paths.split(os.pathsep) if exists(path)])\n\n\"\"\"\nA JavaConfig object encapsulates info on how Java commands are run.\n\"\"\"\nclass JavaConfig:\n    def __init__(self, java_home, java_dbg_port):\n        self.jdk = java_home\n        self.debug_port = java_dbg_port\n        self.jar = exe_suffix(join(self.jdk, 'bin', 'jar'))\n        self.java = exe_suffix(join(self.jdk, 'bin', 'java'))\n        self.javac = exe_suffix(join(self.jdk, 'bin', 'javac'))\n        self.javap = exe_suffix(join(self.jdk, 'bin', 'javap'))\n        self.javadoc = exe_suffix(join(self.jdk, 'bin', 'javadoc'))\n        self.pack200 = exe_suffix(join(self.jdk, 'bin', 'pack200'))\n        self.toolsjar = join(self.jdk, 'lib', 'tools.jar')\n        self._bootclasspath = None\n        self._extdirs = None\n        self._endorseddirs = None\n\n        if not exists(self.java):\n            abort('Java launcher does not exist: ' + self.java)\n\n        def delAtAndSplit(s):\n            return shlex.split(s.lstrip('@'))\n\n        self.java_args = delAtAndSplit(_opts.java_args) if _opts.java_args else []\n        self.java_args_pfx = sum(map(delAtAndSplit, _opts.java_args_pfx), [])\n        self.java_args_sfx = sum(map(delAtAndSplit, _opts.java_args_sfx), [])\n\n        # Prepend the -d64 VM option only if the java command supports it\n        try:\n            output = subprocess.check_output([self.java, '-d64', '-version'], stderr=subprocess.STDOUT)\n            self.java_args = ['-d64'] + self.java_args\n        except subprocess.CalledProcessError as e:\n            try:\n                output = subprocess.check_output([self.java, '-version'], stderr=subprocess.STDOUT)\n            except subprocess.CalledProcessError as e:\n                print e.output\n                abort(e.returncode)\n\n        def _checkOutput(out):\n            return 'version' in out\n\n        # hotspot can print a warning, e.g. if there's a .hotspot_compiler file in the cwd\n        output = output.split('\\n')\n        version = None\n        for o in output:\n            if _checkOutput(o):\n                assert version is None\n                version = o\n\n        self.version = VersionSpec(version.split()[2].strip('\"'))\n        self.javaCompliance = JavaCompliance(self.version.versionString)\n\n        if self.debug_port is not None:\n            self.java_args += ['-Xdebug', '-Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=' + str(self.debug_port)]\n\n    def _init_classpaths(self):\n        _, binDir = _compile_mx_class('ClasspathDump')\n        self._bootclasspath, self._extdirs, self._endorseddirs = [x if x != 'null' else None for x in subprocess.check_output([self.java, '-cp', binDir, 'ClasspathDump'], stderr=subprocess.PIPE).split('|')]\n        if not self._bootclasspath or not self._extdirs or not self._endorseddirs:\n            warn(\"Could not find all classpaths: boot='\" + str(self._bootclasspath) + \"' extdirs='\" + str(self._extdirs) + \"' endorseddirs='\" + str(self._endorseddirs) + \"'\")\n        self._bootclasspath = _filter_non_existant_paths(self._bootclasspath)\n        self._extdirs = _filter_non_existant_paths(self._extdirs)\n        self._endorseddirs = _filter_non_existant_paths(self._endorseddirs)\n\n    def __hash__(self):\n        return hash(self.jdk)\n\n    def __cmp__(self, other):\n        if isinstance(other, JavaConfig):\n            return cmp(self.javaCompliance, other.javaCompliance)\n        raise TypeError()\n\n    def format_cmd(self, args, addDefaultArgs):\n        if addDefaultArgs:\n            return [self.java] + self.processArgs(args)\n        else:\n            return [self.java] + args\n\n    def processArgs(self, args):\n        return self.java_args_pfx + self.java_args + self.java_args_sfx + args\n\n    def bootclasspath(self):\n        if self._bootclasspath is None:\n            self._init_classpaths()\n        return self._bootclasspath\n\n    def extdirs(self):\n        if self._extdirs is None:\n            self._init_classpaths()\n        return self._extdirs\n\n    def endorseddirs(self):\n        if self._endorseddirs is None:\n            self._init_classpaths()\n        return self._endorseddirs\n\ndef check_get_env(key):\n    \"\"\"\n    Gets an environment variable, aborting with a useful message if it is not set.\n    \"\"\"\n    value = get_env(key)\n    if value is None:\n        abort('Required environment variable ' + key + ' must be set')\n    return value\n\ndef get_env(key, default=None):\n    \"\"\"\n    Gets an environment variable.\n    \"\"\"\n    value = os.environ.get(key, default)\n    return value\n\ndef logv(msg=None):\n    if _opts.verbose:\n        log(msg)\n\ndef log(msg=None):\n    \"\"\"\n    Write a message to the console.\n    All script output goes through this method thus allowing a subclass\n    to redirect it.\n    \"\"\"\n    if msg is None:\n        print\n    else:\n        print msg\n\ndef expand_project_in_class_path_arg(cpArg):\n    cp = []\n    for part in cpArg.split(os.pathsep):\n        if part.startswith('@'):\n            cp += classpath(part[1:]).split(os.pathsep)\n        else:\n            cp.append(part)\n    return os.pathsep.join(cp)\n\ndef expand_project_in_args(args):\n    for i in range(len(args)):\n        if args[i] == '-cp' or args[i] == '-classpath':\n            if i + 1 < len(args):\n                args[i + 1] = expand_project_in_class_path_arg(args[i + 1])\n            return\n\n\ndef gmake_cmd():\n    for a in ['make', 'gmake', 'gnumake']:\n        try:\n            output = subprocess.check_output([a, '--version'])\n            if 'GNU' in output:\n                return a\n        except:\n            pass\n    abort('Could not find a GNU make executable on the current path.')\n\ndef expandvars_in_property(value):\n    result = expandvars(value)\n    if '$' in result or '%' in result:\n        abort('Property contains an undefined environment variable: ' + value)\n    return result\n\ndef _send_sigquit():\n    for p, args in _currentSubprocesses:\n\n        def _isJava():\n            if args:\n                name = args[0].split(os.sep)[-1]\n                return name == \"java\"\n            return False\n\n        if p is not None and _isJava():\n            if get_os() == 'windows':\n                log(\"mx: implement me! want to send SIGQUIT to my child process\")\n            else:\n                _kill_process_group(p.pid, sig=signal.SIGQUIT)\n            time.sleep(0.1)\n\ndef abort(codeOrMessage):\n    \"\"\"\n    Aborts the program with a SystemExit exception.\n    If 'codeOrMessage' is a plain integer, it specifies the system exit status;\n    if it is None, the exit status is zero; if it has another type (such as a string),\n    the object's value is printed and the exit status is one.\n    \"\"\"\n\n    if _opts and _opts.killwithsigquit:\n        _send_sigquit()\n\n    def is_alive(p):\n        if isinstance(p, subprocess.Popen):\n            return p.poll() is None\n        assert isinstance(p, multiprocessing.Process), p\n        return p.is_alive()\n\n    for p, args in _currentSubprocesses:\n        if is_alive(p):\n            try:\n                if get_os() == 'windows':\n                    p.terminate()\n                else:\n                    _kill_process_group(p.pid, signal.SIGKILL)\n            except BaseException as e:\n                if is_alive(p):\n                    log('error while killing subprocess {} \"{}\": {}'.format(p.pid, ' '.join(args), e))\n\n    if _opts and _opts.verbose:\n        import traceback\n        traceback.print_stack()\n    raise SystemExit(codeOrMessage)\n\ndef download(path, urls, verbose=False):\n    \"\"\"\n    Attempts to downloads content for each URL in a list, stopping after the first successful download.\n    If the content cannot be retrieved from any URL, the program is aborted. The downloaded content\n    is written to the file indicated by 'path'.\n    \"\"\"\n    d = dirname(path)\n    if d != '' and not exists(d):\n        os.makedirs(d)\n\n    assert not path.endswith(os.sep)\n\n    _, binDir = _compile_mx_class('URLConnectionDownload')\n    command = [java().java, '-cp', binDir, 'URLConnectionDownload']\n    if _opts.no_download_progress or not sys.stderr.isatty():\n        command.append('--no-progress')\n    command.append(path)\n    command += urls\n    if run(command, nonZeroIsFatal=False) == 0:\n        return\n\n    abort('Could not download to ' + path + ' from any of the following URLs:\\n\\n    ' +\n              '\\n    '.join(urls) + '\\n\\nPlease use a web browser to do the download manually')\n\ndef update_file(path, content):\n    \"\"\"\n    Updates a file with some given content if the content differs from what's in\n    the file already. The return value indicates if the file was updated.\n    \"\"\"\n    existed = exists(path)\n    try:\n        old = None\n        if existed:\n            with open(path, 'rb') as f:\n                old = f.read()\n\n        if old == content:\n            return False\n\n        with open(path, 'wb') as f:\n            f.write(content)\n\n        log(('modified ' if existed else 'created ') + path)\n        return True\n    except IOError as e:\n        abort('Error while writing to ' + path + ': ' + str(e))\n\n# Builtin commands\n\ndef _defaultEcjPath():\n    return get_env('JDT', join(_primary_suite.mxDir, 'ecj.jar'))\n\nclass JavaCompileTask:\n    def __init__(self, args, proj, reason, javafilelist, jdk, outputDir, jdtJar, deps):\n        self.proj = proj\n        self.reason = reason\n        self.javafilelist = javafilelist\n        self.deps = deps\n        self.jdk = jdk\n        self.outputDir = outputDir\n        self.done = False\n        self.jdtJar = jdtJar\n        self.args = args\n\n    def __str__(self):\n        return self.proj.name\n\n    def logCompilation(self, compiler):\n        log('Compiling Java sources for {} with {}... [{}]'.format(self.proj.name, compiler, self.reason))\n\n    def execute(self):\n        argfileName = join(self.proj.dir, 'javafilelist.txt')\n        argfile = open(argfileName, 'wb')\n        argfile.write('\\n'.join(self.javafilelist))\n        argfile.close()\n\n        processorArgs = []\n\n        aps = self.proj.annotation_processors()\n        if len(aps) > 0:\n            processorPath = classpath(aps, resolve=True)\n            genDir = self.proj.source_gen_dir()\n            if exists(genDir):\n                shutil.rmtree(genDir)\n            os.mkdir(genDir)\n            processorArgs += ['-processorpath', join(processorPath), '-s', genDir]\n        else:\n            processorArgs += ['-proc:none']\n\n        args = self.args\n        jdk = self.jdk\n        outputDir = self.outputDir\n        compliance = str(jdk.javaCompliance)\n        cp = classpath(self.proj.name, includeSelf=True)\n        toBeDeleted = [argfileName]\n\n        try:\n            if not self.jdtJar:\n                mainJava = java()\n                if not args.error_prone:\n                    javac = args.alt_javac if args.alt_javac else mainJava.javac\n                    self.logCompilation('javac' if not args.alt_javac else args.alt_javac)\n                    javacCmd = [javac, '-g', '-J-Xmx1500m', '-source', compliance, '-target', compliance, '-classpath', cp, '-d', outputDir, '-bootclasspath', jdk.bootclasspath(), '-endorseddirs', jdk.endorseddirs(), '-extdirs', jdk.extdirs()]\n                    if jdk.debug_port is not None:\n                        javacCmd += ['-J-Xdebug', '-J-Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=' + str(jdk.debug_port)]\n                    javacCmd += processorArgs\n                    javacCmd += ['@' + argfile.name]\n\n                    if not args.warnAPI:\n                        javacCmd.append('-XDignore.symbol.file')\n                    run(javacCmd)\n                else:\n                    self.logCompilation('javac (with error-prone)')\n                    javaArgs = ['-Xmx1500m']\n                    javacArgs = ['-g', '-source', compliance, '-target', compliance, '-classpath', cp, '-d', outputDir, '-bootclasspath', jdk.bootclasspath(), '-endorseddirs', jdk.endorseddirs(), '-extdirs', jdk.extdirs()]\n                    javacArgs += processorArgs\n                    javacArgs += ['@' + argfile.name]\n                    if not args.warnAPI:\n                        javacArgs.append('-XDignore.symbol.file')\n                    run_java(javaArgs + ['-cp', os.pathsep.join([mainJava.toolsjar, args.error_prone]), 'com.google.errorprone.ErrorProneCompiler'] + javacArgs)\n            else:\n                self.logCompilation('JDT')\n\n                jdtVmArgs = ['-Xmx1500m', '-jar', self.jdtJar]\n\n                jdtArgs = ['-' + compliance,\n                         '-cp', cp, '-g', '-enableJavadoc',\n                         '-d', outputDir,\n                         '-bootclasspath', jdk.bootclasspath(),\n                         '-endorseddirs', jdk.endorseddirs(),\n                         '-extdirs', jdk.extdirs()]\n                jdtArgs += processorArgs\n\n                jdtProperties = join(self.proj.dir, '.settings', 'org.eclipse.jdt.core.prefs')\n                rootJdtProperties = join(self.proj.suite.mxDir, 'eclipse-settings', 'org.eclipse.jdt.core.prefs')\n                if not exists(jdtProperties) or os.path.getmtime(jdtProperties) < os.path.getmtime(rootJdtProperties):\n                    # Try to fix a missing properties file by running eclipseinit\n                    _eclipseinit_project(self.proj)\n                if not exists(jdtProperties):\n                    log('JDT properties file {0} not found'.format(jdtProperties))\n                else:\n                    with open(jdtProperties) as fp:\n                        origContent = fp.read()\n                        content = origContent\n                        if args.jdt_warning_as_error:\n                            content = content.replace('=warning', '=error')\n                        if not args.jdt_show_task_tags:\n                            content = content + '\\norg.eclipse.jdt.core.compiler.problem.tasks=ignore'\n                    if origContent != content:\n                        jdtPropertiesTmp = jdtProperties + '.tmp'\n                        with open(jdtPropertiesTmp, 'w') as fp:\n                            fp.write(content)\n                        toBeDeleted.append(jdtPropertiesTmp)\n                        jdtArgs += ['-properties', jdtPropertiesTmp]\n                    else:\n                        jdtArgs += ['-properties', jdtProperties]\n                jdtArgs.append('@' + argfile.name)\n\n                run_java(jdtVmArgs + jdtArgs)\n        finally:\n            for n in toBeDeleted:\n                os.remove(n)\n            self.done = True\n\ndef build(args, parser=None):\n    \"\"\"compile the Java and C sources, linking the latter\n\n    Compile all the Java source code using the appropriate compilers\n    and linkers for the various source code types.\"\"\"\n\n    suppliedParser = parser is not None\n    if not suppliedParser:\n        parser = ArgumentParser(prog='mx build')\n\n    parser = parser if parser is not None else ArgumentParser(prog='mx build')\n    parser.add_argument('-f', action='store_true', dest='force', help='force build (disables timestamp checking)')\n    parser.add_argument('-c', action='store_true', dest='clean', help='removes existing build output')\n    parser.add_argument('-p', action='store_true', dest='parallelize', help='parallelizes Java compilation')\n    parser.add_argument('--source', dest='compliance', help='Java compliance level for projects without an explicit one')\n    parser.add_argument('--Wapi', action='store_true', dest='warnAPI', help='show warnings about using internal APIs')\n    parser.add_argument('--projects', action='store', help='comma separated projects to build (omit to build all projects)')\n    parser.add_argument('--only', action='store', help='comma separated projects to build, without checking their dependencies (omit to build all projects)')\n    parser.add_argument('--no-java', action='store_false', dest='java', help='do not build Java projects')\n    parser.add_argument('--no-native', action='store_false', dest='native', help='do not build native projects')\n    parser.add_argument('--jdt-warning-as-error', action='store_true', help='convert all Eclipse batch compiler warnings to errors')\n    parser.add_argument('--jdt-show-task-tags', action='store_true', help='show task tags as Eclipse batch compiler warnings')\n    parser.add_argument('--alt-javac', dest='alt_javac', help='path to alternative javac executable', metavar='<path>')\n    compilerSelect = parser.add_mutually_exclusive_group()\n    compilerSelect.add_argument('--error-prone', dest='error_prone', help='path to error-prone.jar', metavar='<path>')\n    compilerSelect.add_argument('--jdt', help='path to ecj.jar, the Eclipse batch compiler', default=_defaultEcjPath(), metavar='<path>')\n    compilerSelect.add_argument('--force-javac', action='store_true', dest='javac', help='use javac whether ecj.jar is found or not')\n\n    if suppliedParser:\n        parser.add_argument('remainder', nargs=REMAINDER, metavar='...')\n\n    args = parser.parse_args(args)\n\n    jdtJar = None\n    if not args.javac and args.jdt is not None:\n        if not args.jdt.endswith('.jar'):\n            abort('Path for Eclipse batch compiler does not look like a jar file: ' + args.jdt)\n        jdtJar = args.jdt\n        if not exists(jdtJar):\n            if os.path.abspath(jdtJar) == os.path.abspath(_defaultEcjPath()) and get_env('JDT', None) is None:\n                # Silently ignore JDT if default location is used but does not exist\n                jdtJar = None\n            else:\n                abort('Eclipse batch compiler jar does not exist: ' + args.jdt)\n\n    if args.only is not None:\n        # N.B. This build will not include dependencies including annotation processor dependencies\n        sortedProjects = [project(name) for name in args.only.split(',')]\n    else:\n        if args.projects is not None:\n            projectNames = args.projects.split(',')\n        else:\n            projectNames = None\n\n        projects = _projects_opt_limit_to_suites(projects_from_names(projectNames))\n        # N.B. Limiting to a suite only affects the starting set of projects. Dependencies in other suites will still be compiled\n        sortedProjects = sorted_project_deps(projects, includeAnnotationProcessors=True)\n\n    if args.java:\n        ideinit([], refreshOnly=True, buildProcessorJars=False)\n\n    def prepareOutputDirs(p, clean):\n        outputDir = p.output_dir()\n        if exists(outputDir):\n            if clean:\n                log('Cleaning {0}...'.format(outputDir))\n                shutil.rmtree(outputDir)\n                os.mkdir(outputDir)\n        else:\n            os.mkdir(outputDir)\n        genDir = p.source_gen_dir()\n        if genDir != '' and exists(genDir) and clean:\n            log('Cleaning {0}...'.format(genDir))\n            for f in os.listdir(genDir):\n                shutil.rmtree(join(genDir, f))\n        return outputDir\n\n    tasks = {}\n    for p in sortedProjects:\n        if p.native:\n            if args.native:\n                log('Calling GNU make {0}...'.format(p.dir))\n\n                if args.clean:\n                    run([gmake_cmd(), 'clean'], cwd=p.dir)\n\n                run([gmake_cmd()], cwd=p.dir)\n            continue\n        else:\n            if not args.java:\n                continue\n            if exists(join(p.dir, 'plugin.xml')):  # eclipse plugin project\n                continue\n\n        # skip building this Java project if its Java compliance level is \"higher\" than the configured JDK\n        requiredCompliance = p.javaCompliance if p.javaCompliance else JavaCompliance(args.compliance) if args.compliance else None\n        jdk = java(requiredCompliance)\n        assert jdk\n\n        outputDir = prepareOutputDirs(p, args.clean)\n\n        sourceDirs = p.source_dirs()\n        buildReason = 'forced build' if args.force else None\n        taskDeps = []\n        if not buildReason:\n            for dep in p.all_deps([], includeLibs=False, includeAnnotationProcessors=True):\n                taskDep = tasks.get(dep.name)\n                if taskDep:\n                    if not buildReason:\n                        buildReason = dep.name + ' rebuilt'\n                    taskDeps.append(taskDep)\n\n        jasminAvailable = None\n        javafilelist = []\n        for sourceDir in sourceDirs:\n            for root, _, files in os.walk(sourceDir):\n                javafiles = [join(root, name) for name in files if name.endswith('.java') and name != 'package-info.java']\n                javafilelist += javafiles\n\n                # Copy all non Java resources or assemble Jasmin files\n                nonjavafilelist = [join(root, name) for name in files if not name.endswith('.java')]\n                for src in nonjavafilelist:\n                    if src.endswith('.jasm'):\n                        className = None\n                        with open(src) as f:\n                            for line in f:\n                                if line.startswith('.class '):\n                                    className = line.split()[-1]\n                                    break\n\n                        if className is not None:\n                            jasminOutputDir = p.jasmin_output_dir()\n                            classFile = join(jasminOutputDir, className.replace('/', os.sep) + '.class')\n                            if exists(dirname(classFile)) and (not exists(classFile) or os.path.getmtime(classFile) < os.path.getmtime(src)):\n                                if jasminAvailable is None:\n                                    try:\n                                        with open(os.devnull) as devnull:\n                                            subprocess.call('jasmin', stdout=devnull, stderr=subprocess.STDOUT)\n                                        jasminAvailable = True\n                                    except OSError:\n                                        jasminAvailable = False\n\n                                if jasminAvailable:\n                                    log('Assembling Jasmin file ' + src)\n                                    run(['jasmin', '-d', jasminOutputDir, src])\n                                else:\n                                    log('The jasmin executable could not be found - skipping ' + src)\n                                    with file(classFile, 'a'):\n                                        os.utime(classFile, None)\n\n                        else:\n                            log('could not file .class directive in Jasmin source: ' + src)\n                    else:\n                        dst = join(outputDir, src[len(sourceDir) + 1:])\n                        if not exists(dirname(dst)):\n                            os.makedirs(dirname(dst))\n                        if exists(dirname(dst)) and (not exists(dst) or os.path.getmtime(dst) < os.path.getmtime(src)):\n                            shutil.copyfile(src, dst)\n\n                if not buildReason:\n                    for javafile in javafiles:\n                        classfile = TimeStampFile(outputDir + javafile[len(sourceDir):-len('java')] + 'class')\n                        if not classfile.exists() or classfile.isOlderThan(javafile):\n                            buildReason = 'class file(s) out of date'\n                            break\n\n        apsOutOfDate = p.update_current_annotation_processors_file()\n        if apsOutOfDate:\n            buildReason = 'annotation processor(s) changed'\n\n        if not buildReason:\n            logv('[all class files for {0} are up to date - skipping]'.format(p.name))\n            continue\n\n        if len(javafilelist) == 0:\n            logv('[no Java sources for {0} - skipping]'.format(p.name))\n            continue\n\n        task = JavaCompileTask(args, p, buildReason, javafilelist, jdk, outputDir, jdtJar, taskDeps)\n\n        if args.parallelize:\n            # Best to initialize class paths on main process\n            jdk.bootclasspath()\n            task.proc = None\n            tasks[p.name] = task\n        else:\n            task.execute()\n\n    if args.parallelize:\n\n        def joinTasks(tasks):\n            failed = []\n            for t in tasks:\n                t.proc.join()\n                _removeSubprocess(t.sub)\n                if t.proc.exitcode != 0:\n                    failed.append(t)\n            return failed\n\n        def checkTasks(tasks):\n            active = []\n            for t in tasks:\n                if t.proc.is_alive():\n                    active.append(t)\n                else:\n                    if t.proc.exitcode != 0:\n                        return ([], joinTasks(tasks))\n            return (active, [])\n\n        def remainingDepsDepth(task):\n            if task._d is None:\n                incompleteDeps = [d for d in task.deps if d.proc is None or d.proc.is_alive()]\n                if len(incompleteDeps) == 0:\n                    task._d = 0\n                else:\n                    task._d = max([remainingDepsDepth(t) for t in incompleteDeps]) + 1\n            return task._d\n\n        def compareTasks(t1, t2):\n            d = remainingDepsDepth(t1) - remainingDepsDepth(t2)\n            if d == 0:\n                t1Work = (1 + len(t1.proj.annotation_processors())) * len(t1.javafilelist)\n                t2Work = (1 + len(t2.proj.annotation_processors())) * len(t2.javafilelist)\n                d = t1Work - t2Work\n            return d\n\n        def sortWorklist(tasks):\n            for t in tasks:\n                t._d = None\n            return sorted(tasks, compareTasks)\n\n        cpus = multiprocessing.cpu_count()\n        worklist = sortWorklist(tasks.values())\n        active = []\n        failed = []\n        while len(worklist) != 0:\n            while True:\n                active, failed = checkTasks(active)\n                if len(failed) != 0:\n                    assert not active, active\n                    break\n                if len(active) == cpus:\n                    # Sleep for 1 second\n                    time.sleep(1)\n                else:\n                    break\n\n            if len(failed) != 0:\n                break\n\n            def executeTask(task):\n                # Clear sub-process list cloned from parent process\n                del _currentSubprocesses[:]\n                task.execute()\n\n            def depsDone(task):\n                for d in task.deps:\n                    if d.proc is None or d.proc.exitcode is None:\n                        return False\n                return True\n\n            for task in worklist:\n                if depsDone(task):\n                    worklist.remove(task)\n                    task.proc = multiprocessing.Process(target=executeTask, args=(task,))\n                    task.proc.start()\n                    active.append(task)\n                    task.sub = _addSubprocess(task.proc, ['JavaCompileTask', str(task)])\n                if len(active) == cpus:\n                    break\n\n            worklist = sortWorklist(worklist)\n\n        failed += joinTasks(active)\n        if len(failed):\n            for t in failed:\n                log('Compiling {} failed'.format(t.proj.name))\n            abort('{} Java compilation tasks failed'.format(len(failed)))\n\n    for dist in _dists.values():\n        archive(['@' + dist.name])\n\n    if suppliedParser:\n        return args\n    return None\n\ndef build_suite(s):\n    '''build all projects in suite (for dynamic import)'''\n    project_names = [p.name for p in s.projects]\n    command_function('build')(['--projects', ','.join(project_names)])\n\ndef _chunk_files_for_command_line(files, limit=None, pathFunction=None):\n    \"\"\"\n    Returns a generator for splitting up a list of files into chunks such that the\n    size of the space separated file paths in a chunk is less than a given limit.\n    This is used to work around system command line length limits.\n    \"\"\"\n    chunkSize = 0\n    chunkStart = 0\n    if limit is None:\n        commandLinePrefixAllowance = 3000\n        if get_os() == 'windows':\n            # The CreateProcess function on Windows limits the length of a command line to\n            # 32,768 characters (http://msdn.microsoft.com/en-us/library/ms682425%28VS.85%29.aspx)\n            limit = 32768 - commandLinePrefixAllowance\n        else:\n            # Using just SC_ARG_MAX without extra downwards adjustment\n            # results in \"[Errno 7] Argument list too long\" on MacOS.\n            syslimit = os.sysconf('SC_ARG_MAX') - 20000\n            limit = syslimit - commandLinePrefixAllowance\n    for i in range(len(files)):\n        path = files[i] if pathFunction is None else pathFunction(files[i])\n        size = len(path) + 1\n        if chunkSize + size < limit:\n            chunkSize += size\n        else:\n            assert i > chunkStart\n            yield files[chunkStart:i]\n            chunkStart = i\n            chunkSize = 0\n    if chunkStart == 0:\n        assert chunkSize < limit\n        yield files\n\ndef eclipseformat(args):\n    \"\"\"run the Eclipse Code Formatter on the Java sources\n\n    The exit code 1 denotes that at least one file was modified.\"\"\"\n\n    parser = ArgumentParser(prog='mx eclipseformat')\n    parser.add_argument('-e', '--eclipse-exe', help='location of the Eclipse executable')\n    parser.add_argument('-C', '--no-backup', action='store_false', dest='backup', help='do not save backup of modified files')\n    parser.add_argument('--projects', action='store', help='comma separated projects to process (omit to process all projects)')\n\n    args = parser.parse_args(args)\n    if args.eclipse_exe is None:\n        args.eclipse_exe = os.environ.get('ECLIPSE_EXE')\n    if args.eclipse_exe is None:\n        abort('Could not find Eclipse executable. Use -e option or ensure ECLIPSE_EXE environment variable is set.')\n\n    # Maybe an Eclipse installation dir was specified - look for the executable in it\n    if isdir(args.eclipse_exe):\n        args.eclipse_exe = join(args.eclipse_exe, exe_suffix('eclipse'))\n        warn(\"The eclipse-exe was a directory, now using \" + args.eclipse_exe)\n\n    if not os.path.isfile(args.eclipse_exe):\n        abort('File does not exist: ' + args.eclipse_exe)\n    if not os.access(args.eclipse_exe, os.X_OK):\n        abort('Not an executable file: ' + args.eclipse_exe)\n\n    eclipseinit([], buildProcessorJars=False)\n\n    # build list of projects to be processed\n    projects = sorted_deps()\n    if args.projects is not None:\n        projects = [project(name) for name in args.projects.split(',')]\n\n    class Batch:\n        def __init__(self, settingsDir, javaCompliance):\n            self.path = join(settingsDir, 'org.eclipse.jdt.core.prefs')\n            self.javaCompliance = javaCompliance\n            self.javafiles = list()\n            with open(join(settingsDir, 'org.eclipse.jdt.ui.prefs')) as fp:\n                jdtUiPrefs = fp.read()\n            self.removeTrailingWhitespace = 'sp_cleanup.remove_trailing_whitespaces_all=true' in jdtUiPrefs\n            if self.removeTrailingWhitespace:\n                assert 'sp_cleanup.remove_trailing_whitespaces=true' in jdtUiPrefs and 'sp_cleanup.remove_trailing_whitespaces_ignore_empty=false' in jdtUiPrefs\n\n        def settings(self):\n            with open(self.path) as fp:\n                return fp.read() + java(self.javaCompliance).java + str(self.removeTrailingWhitespace)\n\n    class FileInfo:\n        def __init__(self, path):\n            self.path = path\n            with open(path) as fp:\n                self.content = fp.read()\n            self.times = (os.path.getatime(path), os.path.getmtime(path))\n\n        def update(self, removeTrailingWhitespace):\n            with open(self.path) as fp:\n                content = fp.read()\n\n            if self.content != content:\n                # Only apply *after* formatting to match the order in which the IDE does it\n                if removeTrailingWhitespace:\n                    content, n = re.subn(r'[ \\t]+$', '', content, flags=re.MULTILINE)\n                    if n != 0 and self.content == content:\n                        # undo on-disk changes made by the Eclipse formatter\n                        with open(self.path, 'w') as fp:\n                            fp.write(content)\n\n                if self.content != content:\n                    self.diff = difflib.unified_diff(self.content.splitlines(1), content.splitlines(1))\n                    self.content = content\n                    return True\n\n            # reset access and modification time of file\n            os.utime(self.path, self.times)\n\n    modified = list()\n    batches = dict()  # all sources with the same formatting settings are formatted together\n    for p in projects:\n        if p.native:\n            continue\n        sourceDirs = p.source_dirs()\n\n        batch = Batch(join(p.dir, '.settings'), p.javaCompliance)\n\n        if not exists(batch.path):\n            if _opts.verbose:\n                log('[no Eclipse Code Formatter preferences at {0} - skipping]'.format(batch.path))\n            continue\n\n        for sourceDir in sourceDirs:\n            for root, _, files in os.walk(sourceDir):\n                for f in [join(root, name) for name in files if name.endswith('.java')]:\n                    batch.javafiles.append(FileInfo(f))\n        if len(batch.javafiles) == 0:\n            logv('[no Java sources in {0} - skipping]'.format(p.name))\n            continue\n\n        res = batches.setdefault(batch.settings(), batch)\n        if res is not batch:\n            res.javafiles = res.javafiles + batch.javafiles\n\n    log(\"we have: \" + str(len(batches)) + \" batches\")\n    for batch in batches.itervalues():\n        for chunk in _chunk_files_for_command_line(batch.javafiles, pathFunction=lambda f: f.path):\n            run([args.eclipse_exe,\n                '-nosplash',\n                '-application',\n                'org.eclipse.jdt.core.JavaCodeFormatter',\n                '-vm', java(batch.javaCompliance).java,\n                '-config', batch.path]\n                + [f.path for f in chunk])\n            for fi in chunk:\n                if fi.update(batch.removeTrailingWhitespace):\n                    modified.append(fi)\n\n    log('{0} files were modified'.format(len(modified)))\n\n    if len(modified) != 0:\n        arcbase = _primary_suite.dir\n        if args.backup:\n            backup = os.path.abspath('eclipseformat.backup.zip')\n            zf = zipfile.ZipFile(backup, 'w', zipfile.ZIP_DEFLATED)\n        for fi in modified:\n            name = os.path.relpath(fi.path, arcbase)\n            log(' - {0}'.format(name))\n            log('Changes:')\n            log(''.join(fi.diff))\n            if args.backup:\n                arcname = name.replace(os.sep, '/')\n                zf.writestr(arcname, fi.content)\n        if args.backup:\n            zf.close()\n            log('Wrote backup of {0} modified files to {1}'.format(len(modified), backup))\n        return 1\n    return 0\n\ndef processorjars():\n    for s in suites(True):\n        _processorjars_suite(s)\n\ndef _processorjars_suite(s):\n    projs = set()\n    candidates = sorted_project_deps(s.projects)\n    for p in candidates:\n        if _isAnnotationProcessorDependency(p):\n            projs.add(p)\n\n    if len(projs) <= 0:\n        return []\n\n    pnames = [p.name for p in projs]\n    build(['--jdt-warning-as-error', '--projects', \",\".join(pnames)])\n    return archive(pnames)\n\ndef pylint(args):\n    \"\"\"run pylint (if available) over Python source files (found by 'hg locate' or by tree walk with -walk)\"\"\"\n\n    parser = ArgumentParser(prog='mx pylint')\n    parser.add_argument('--walk', action='store_true', help='use tree walk find .py files')\n    args = parser.parse_args(args)\n\n    rcfile = join(dirname(__file__), '.pylintrc')\n    if not exists(rcfile):\n        log('pylint configuration file does not exist: ' + rcfile)\n        return\n\n    try:\n        output = subprocess.check_output(['pylint', '--version'], stderr=subprocess.STDOUT)\n        m = re.match(r'.*pylint (\\d+)\\.(\\d+)\\.(\\d+).*', output, re.DOTALL)\n        if not m:\n            log('could not determine pylint version from ' + output)\n            return\n        major, minor, micro = (int(m.group(1)), int(m.group(2)), int(m.group(3)))\n        if major < 1:\n            log('require pylint version >= 1 (got {0}.{1}.{2})'.format(major, minor, micro))\n            return\n    except BaseException:\n        log('pylint is not available')\n        return\n\n    def findfiles_by_walk():\n        result = []\n        for suite in suites(True):\n            for root, dirs, files in os.walk(suite.dir):\n                for f in files:\n                    if f.endswith('.py'):\n                        pyfile = join(root, f)\n                        result.append(pyfile)\n                if 'bin' in dirs:\n                    dirs.remove('bin')\n                if 'lib' in dirs:\n                    # avoids downloaded .py files\n                    dirs.remove('lib')\n        return result\n\n    def findfiles_by_hg():\n        result = []\n        for suite in suites(True):\n            versioned = subprocess.check_output(['hg', 'locate', '-f'], stderr=subprocess.STDOUT, cwd=suite.dir).split(os.linesep)\n            for f in versioned:\n                if f.endswith('.py') and exists(f):\n                    result.append(f)\n        return result\n\n    # Perhaps we should just look in suite.mxDir directories for .py files?\n    if args.walk:\n        pyfiles = findfiles_by_walk()\n    else:\n        pyfiles = findfiles_by_hg()\n\n    env = os.environ.copy()\n\n    pythonpath = dirname(__file__)\n    for suite in suites(True):\n        pythonpath = os.pathsep.join([pythonpath, suite.mxDir])\n\n    env['PYTHONPATH'] = pythonpath\n\n    for pyfile in pyfiles:\n        log('Running pylint on ' + pyfile + '...')\n        run(['pylint', '--reports=n', '--rcfile=' + rcfile, pyfile], env=env)\n\n\"\"\"\nUtility for creating and updating a zip file atomically.\n\"\"\"\nclass Archiver:\n    def __init__(self, path):\n        self.path = path\n\n    def __enter__(self):\n        if self.path:\n            fd, tmp = tempfile.mkstemp(suffix='', prefix=basename(self.path) + '.', dir=dirname(self.path))\n            self.tmpFd = fd\n            self.tmpPath = tmp\n            self.zf = zipfile.ZipFile(tmp, 'w')\n        else:\n            self.tmpFd = None\n            self.tmpPath = None\n            self.zf = None\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if self.zf:\n            self.zf.close()\n            os.close(self.tmpFd)\n            # Correct the permissions on the temporary file which is created with restrictive permissions\n            os.chmod(self.tmpPath, 0o666 & ~currentUmask)\n            # Atomic on Unix\n            shutil.move(self.tmpPath, self.path)\n\ndef _archive(args):\n    archive(args)\n    return 0\n\ndef archive(args):\n    \"\"\"create jar files for projects and distributions\"\"\"\n    parser = ArgumentParser(prog='mx archive')\n    parser.add_argument('names', nargs=REMAINDER, metavar='[<project>|@<distribution>]...')\n    args = parser.parse_args(args)\n\n    archives = []\n    for name in args.names:\n        if name.startswith('@'):\n            dname = name[1:]\n            d = distribution(dname)\n            d.make_archive()\n            archives.append(d.path)\n        else:\n            p = project(name)\n            archives.append(p.make_archive())\n\n    logv(\"generated archives: \" + str(archives))\n    return archives\n\ndef canonicalizeprojects(args):\n    \"\"\"process all project files to canonicalize the dependencies\n\n    The exit code of this command reflects how many files were updated.\"\"\"\n\n    changedFiles = 0\n    for s in suites(True):\n        projectsFile = join(s.mxDir, 'projects')\n        if not exists(projectsFile):\n            continue\n        with open(projectsFile) as f:\n            out = StringIO.StringIO()\n            pattern = re.compile('project@([^@]+)@dependencies=.*')\n            lineNo = 1\n            for line in f:\n                line = line.strip()\n                m = pattern.match(line)\n                p = project(m.group(1), fatalIfMissing=False) if m else None\n                if m is None or p is None:\n                    out.write(line + '\\n')\n                else:\n                    for pkg in p.defined_java_packages():\n                        if not pkg.startswith(p.name):\n                            abort('package in {0} does not have prefix matching project name: {1}'.format(p, pkg))\n\n                    ignoredDeps = set([name for name in p.deps if project(name, False) is not None])\n                    for pkg in p.imported_java_packages():\n                        for name in p.deps:\n                            dep = project(name, False)\n                            if dep is None:\n                                ignoredDeps.discard(name)\n                            else:\n                                if pkg in dep.defined_java_packages():\n                                    ignoredDeps.discard(name)\n                                if pkg in dep.extended_java_packages():\n                                    ignoredDeps.discard(name)\n                    if len(ignoredDeps) != 0:\n                        candidates = set()\n                        # Compute dependencies based on projects required by p\n                        for d in sorted_deps():\n                            if not d.defined_java_packages().isdisjoint(p.imported_java_packages()):\n                                candidates.add(d)\n                        # Remove non-canonical candidates\n                        for c in list(candidates):\n                            candidates.difference_update(c.all_deps([], False, False))\n                        candidates = [d.name for d in candidates]\n\n                        abort('{0}:{1}: {2} does not use any packages defined in these projects: {3}\\nComputed project dependencies: {4}'.format(\n                            projectsFile, lineNo, p, ', '.join(ignoredDeps), ','.join(candidates)))\n\n                    out.write('project@' + m.group(1) + '@dependencies=' + ','.join(p.canonical_deps()) + '\\n')\n                lineNo = lineNo + 1\n            content = out.getvalue()\n        if update_file(projectsFile, content):\n            changedFiles += 1\n    return changedFiles\n\nclass TimeStampFile:\n    def __init__(self, path):\n        self.path = path\n        self.timestamp = os.path.getmtime(path) if exists(path) else None\n\n    def isOlderThan(self, arg):\n        if not self.timestamp:\n            return True\n        if isinstance(arg, TimeStampFile):\n            if arg.timestamp is None:\n                return False\n            else:\n                return arg.timestamp > self.timestamp\n        elif isinstance(arg, types.ListType):\n            files = arg\n        else:\n            files = [arg]\n        for f in files:\n            if os.path.getmtime(f) > self.timestamp:\n                return True\n        return False\n\n    def exists(self):\n        return exists(self.path)\n\n    def touch(self):\n        if exists(self.path):\n            os.utime(self.path, None)\n        else:\n            if not isdir(dirname(self.path)):\n                os.makedirs(dirname(self.path))\n            file(self.path, 'a')\n\ndef checkstyle(args):\n    \"\"\"run Checkstyle on the Java sources\n\n   Run Checkstyle over the Java sources. Any errors or warnings\n   produced by Checkstyle result in a non-zero exit code.\"\"\"\n\n    parser = ArgumentParser(prog='mx checkstyle')\n\n    parser.add_argument('-f', action='store_true', dest='force', help='force checking (disables timestamp checking)')\n    args = parser.parse_args(args)\n\n    totalErrors = 0\n    for p in projects_opt_limit_to_suites():\n        if p.native:\n            continue\n        sourceDirs = p.source_dirs()\n\n        csConfig = join(p.dir, '.checkstyle')\n\n        # skip checking this Java project if its Java compliance level is \"higher\" than the configured JDK\n        jdk = java(p.javaCompliance)\n        assert jdk\n\n        if not exists(csConfig):\n            log('Excluding {0} from checking: {1} is missing'.format(p.name, csConfig))\n            continue\n\n        for sourceDir in sourceDirs:\n            javafilelist = []\n            for root, _, files in os.walk(sourceDir):\n                javafilelist += [join(root, name) for name in files if name.endswith('.java') and name != 'package-info.java']\n            if len(javafilelist) == 0:\n                logv('[no Java sources in {0} - skipping]'.format(sourceDir))\n                continue\n\n            timestamp = TimeStampFile(join(p.suite.mxDir, 'checkstyle-timestamps', sourceDir[len(p.suite.dir) + 1:].replace(os.sep, '_') + '.timestamp'))\n            mustCheck = False\n            if not args.force and timestamp.exists():\n                mustCheck = timestamp.isOlderThan(javafilelist)\n            else:\n                mustCheck = True\n\n            if not mustCheck:\n                if _opts.verbose:\n                    log('[all Java sources in {0} already checked - skipping]'.format(sourceDir))\n                continue\n\n            dotCheckstyleXML = xml.dom.minidom.parse(csConfig)\n            localCheckConfig = dotCheckstyleXML.getElementsByTagName('local-check-config')[0]\n            configLocation = localCheckConfig.getAttribute('location')\n            configType = localCheckConfig.getAttribute('type')\n            if configType == 'project':\n                # Eclipse plugin \"Project Relative Configuration\" format:\n                #\n                #  '/<project_name>/<suffix>'\n                #\n                if configLocation.startswith('/'):\n                    name, _, suffix = configLocation.lstrip('/').partition('/')\n                    config = join(project(name).dir, suffix)\n                else:\n                    config = join(p.dir, configLocation)\n            else:\n                logv('[unknown Checkstyle configuration type \"' + configType + '\" in {0} - skipping]'.format(sourceDir))\n                continue\n\n            exclude = join(p.dir, '.checkstyle.exclude')\n\n            if exists(exclude):\n                with open(exclude) as f:\n                    # Convert patterns to OS separators\n                    patterns = [name.rstrip().replace('/', os.sep) for name in f.readlines()]\n                def match(name):\n                    for p in patterns:\n                        if p in name:\n                            if _opts.verbose:\n                                log('excluding: ' + name)\n                            return True\n                    return False\n\n                javafilelist = [name for name in javafilelist if not match(name)]\n\n            auditfileName = join(p.dir, 'checkstyleOutput.txt')\n            log('Running Checkstyle on {0} using {1}...'.format(sourceDir, config))\n\n            try:\n                for chunk in _chunk_files_for_command_line(javafilelist):\n                    try:\n                        run_java(['-Xmx1g', '-jar', library('CHECKSTYLE').get_path(True), '-f', 'xml', '-c', config, '-o', auditfileName] + chunk, nonZeroIsFatal=False)\n                    finally:\n                        if exists(auditfileName):\n                            errors = []\n                            source = [None]\n                            def start_element(name, attrs):\n                                if name == 'file':\n                                    source[0] = attrs['name']\n                                elif name == 'error':\n                                    errors.append('{}:{}: {}'.format(source[0], attrs['line'], attrs['message']))\n\n                            xp = xml.parsers.expat.ParserCreate()\n                            xp.StartElementHandler = start_element\n                            with open(auditfileName) as fp:\n                                xp.ParseFile(fp)\n                            if len(errors) != 0:\n                                map(log, errors)\n                                totalErrors = totalErrors + len(errors)\n                            else:\n                                timestamp.touch()\n            finally:\n                if exists(auditfileName):\n                    os.unlink(auditfileName)\n    return totalErrors\n\ndef clean(args, parser=None):\n    \"\"\"remove all class files, images, and executables\n\n    Removes all files created by a build, including Java class files, executables, and\n    generated images.\n    \"\"\"\n\n    suppliedParser = parser is not None\n\n    parser = parser if suppliedParser else ArgumentParser(prog='mx clean')\n    parser.add_argument('--no-native', action='store_false', dest='native', help='do not clean native projects')\n    parser.add_argument('--no-java', action='store_false', dest='java', help='do not clean Java projects')\n    parser.add_argument('--projects', action='store', help='comma separated projects to clean (omit to clean all projects)')\n    parser.add_argument('--no-dist', action='store_false', dest='dist', help='do not delete distributions')\n\n    args = parser.parse_args(args)\n\n    if args.projects is not None:\n        projects = [project(name) for name in args.projects.split(',')]\n    else:\n        projects = projects_opt_limit_to_suites()\n\n    def _rmtree(dirPath):\n        path = dirPath\n        if get_os() == 'windows':\n            path = unicode(\"\\\\\\\\?\\\\\" + dirPath)\n        shutil.rmtree(path)\n\n    def _rmIfExists(name):\n        if os.path.isfile(name):\n            os.unlink(name)\n\n    for p in projects:\n        if p.native:\n            if args.native:\n                run([gmake_cmd(), '-C', p.dir, 'clean'])\n        else:\n            if args.java:\n                genDir = p.source_gen_dir()\n                if genDir != '' and exists(genDir):\n                    log('Clearing {0}...'.format(genDir))\n                    for f in os.listdir(genDir):\n                        _rmtree(join(genDir, f))\n\n\n                outputDir = p.output_dir()\n                if outputDir != '' and exists(outputDir):\n                    log('Removing {0}...'.format(outputDir))\n                    _rmtree(outputDir)\n\n            for configName in ['netbeans-config.zip', 'eclipse-config.zip']:\n                config = TimeStampFile(join(p.suite.mxDir, configName))\n                if config.exists():\n                    os.unlink(config.path)\n\n    if args.dist:\n        for d in _dists.keys():\n            log('Removing distribution {0}...'.format(d))\n            _rmIfExists(distribution(d).path)\n            _rmIfExists(distribution(d).sourcesPath)\n\n\n    if suppliedParser:\n        return args\n\ndef about(args):\n    \"\"\"show the 'man page' for mx\"\"\"\n    print __doc__\n\ndef help_(args):\n    \"\"\"show help for a given command\n\nWith no arguments, print a list of commands and short help for each command.\n\nGiven a command name, print help for that command.\"\"\"\n    if len(args) == 0:\n        _argParser.print_help()\n        return\n\n    name = args[0]\n    if not _commands.has_key(name):\n        hits = [c for c in _commands.iterkeys() if c.startswith(name)]\n        if len(hits) == 1:\n            name = hits[0]\n        elif len(hits) == 0:\n            abort('mx: unknown command \\'{0}\\'\\n{1}use \"mx help\" for more options'.format(name, _format_commands()))\n        else:\n            abort('mx: command \\'{0}\\' is ambiguous\\n    {1}'.format(name, ' '.join(hits)))\n\n    value = _commands[name]\n    (func, usage) = value[:2]\n    doc = func.__doc__\n    if len(value) > 2:\n        docArgs = value[2:]\n        fmtArgs = []\n        for d in docArgs:\n            if isinstance(d, Callable):\n                fmtArgs += [d()]\n            else:\n                fmtArgs += [str(d)]\n        doc = doc.format(*fmtArgs)\n    print 'mx {0} {1}\\n\\n{2}\\n'.format(name, usage, doc)\n\ndef projectgraph(args, suite=None):\n    \"\"\"create graph for project structure (\"mx projectgraph | dot -Tpdf -oprojects.pdf\" or \"mx projectgraph --igv\")\"\"\"\n\n    parser = ArgumentParser(prog='mx projectgraph')\n    parser.add_argument('--igv', action='store_true', help='output to IGV listening on 127.0.0.1:4444')\n    parser.add_argument('--igv-format', action='store_true', help='output graph in IGV format')\n\n    args = parser.parse_args(args)\n\n    if args.igv or args.igv_format:\n        ids = {}\n        nextToIndex = {}\n        igv = XMLDoc()\n        igv.open('graphDocument')\n        igv.open('group')\n        igv.open('properties')\n        igv.element('p', {'name' : 'name'}, 'GraalProjectDependencies')\n        igv.close('properties')\n        igv.open('graph', {'name' : 'dependencies'})\n        igv.open('nodes')\n        for p in sorted_deps(includeLibs=True, includeJreLibs=True):\n            ident = len(ids)\n            ids[p.name] = str(ident)\n            igv.open('node', {'id' : str(ident)})\n            igv.open('properties')\n            igv.element('p', {'name' : 'name'}, p.name)\n            igv.close('properties')\n            igv.close('node')\n        igv.close('nodes')\n        igv.open('edges')\n        for p in projects():\n            fromIndex = 0\n            for dep in p.canonical_deps():\n                toIndex = nextToIndex.get(dep, 0)\n                nextToIndex[dep] = toIndex + 1\n                igv.element('edge', {'from' : ids[p.name], 'fromIndex' : str(fromIndex), 'to' : ids[dep], 'toIndex' : str(toIndex), 'label' : 'dependsOn'})\n                fromIndex = fromIndex + 1\n        igv.close('edges')\n        igv.close('graph')\n        igv.close('group')\n        igv.close('graphDocument')\n\n        if args.igv:\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            s.connect(('127.0.0.1', 4444))\n            s.send(igv.xml())\n        else:\n            print igv.xml(indent='  ', newl='\\n')\n        return\n\n    print 'digraph projects {'\n    print 'rankdir=BT;'\n    print 'node [shape=rect];'\n    for p in projects():\n        for dep in p.canonical_deps():\n            print '\"' + p.name + '\"->\"' + dep + '\"'\n    print '}'\n\ndef _source_locator_memento(deps):\n    slm = XMLDoc()\n    slm.open('sourceLookupDirector')\n    slm.open('sourceContainers', {'duplicates' : 'false'})\n\n    javaCompliance = None\n\n    for dep in deps:\n        if dep.isLibrary():\n            if hasattr(dep, 'eclipse.container'):\n                memento = XMLDoc().element('classpathContainer', {'path' : getattr(dep, 'eclipse.container')}).xml(standalone='no')\n                slm.element('classpathContainer', {'memento' : memento, 'typeId':'org.eclipse.jdt.launching.sourceContainer.classpathContainer'})\n            elif dep.get_source_path(resolve=True):\n                memento = XMLDoc().element('archive', {'detectRoot' : 'true', 'path' : dep.get_source_path(resolve=True)}).xml(standalone='no')\n                slm.element('container', {'memento' : memento, 'typeId':'org.eclipse.debug.core.containerType.externalArchive'})\n        elif dep.isProject():\n            if dep.native:\n                continue\n            memento = XMLDoc().element('javaProject', {'name' : dep.name}).xml(standalone='no')\n            slm.element('container', {'memento' : memento, 'typeId':'org.eclipse.jdt.launching.sourceContainer.javaProject'})\n            if javaCompliance is None or dep.javaCompliance > javaCompliance:\n                javaCompliance = dep.javaCompliance\n\n    if javaCompliance:\n        memento = XMLDoc().element('classpathContainer', {'path' : 'org.eclipse.jdt.launching.JRE_CONTAINER/org.eclipse.jdt.internal.debug.ui.launcher.StandardVMType/JavaSE-' + str(javaCompliance)}).xml(standalone='no')\n        slm.element('classpathContainer', {'memento' : memento, 'typeId':'org.eclipse.jdt.launching.sourceContainer.classpathContainer'})\n    else:\n        memento = XMLDoc().element('classpathContainer', {'path' : 'org.eclipse.jdt.launching.JRE_CONTAINER'}).xml(standalone='no')\n        slm.element('classpathContainer', {'memento' : memento, 'typeId':'org.eclipse.jdt.launching.sourceContainer.classpathContainer'})\n\n    slm.close('sourceContainers')\n    slm.close('sourceLookupDirector')\n    return slm\n\ndef make_eclipse_attach(suite, hostname, port, name=None, deps=None):\n    \"\"\"\n    Creates an Eclipse launch configuration file for attaching to a Java process.\n    \"\"\"\n    if deps is None:\n        deps = []\n    slm = _source_locator_memento(deps)\n    launch = XMLDoc()\n    launch.open('launchConfiguration', {'type' : 'org.eclipse.jdt.launching.remoteJavaApplication'})\n    launch.element('stringAttribute', {'key' : 'org.eclipse.debug.core.source_locator_id', 'value' : 'org.eclipse.jdt.launching.sourceLocator.JavaSourceLookupDirector'})\n    launch.element('stringAttribute', {'key' : 'org.eclipse.debug.core.source_locator_memento', 'value' : '%s'})\n    launch.element('booleanAttribute', {'key' : 'org.eclipse.jdt.launching.ALLOW_TERMINATE', 'value' : 'true'})\n    launch.open('mapAttribute', {'key' : 'org.eclipse.jdt.launching.CONNECT_MAP'})\n    launch.element('mapEntry', {'key' : 'hostname', 'value' : hostname})\n    launch.element('mapEntry', {'key' : 'port', 'value' : port})\n    launch.close('mapAttribute')\n    launch.element('stringAttribute', {'key' : 'org.eclipse.jdt.launching.PROJECT_ATTR', 'value' : ''})\n    launch.element('stringAttribute', {'key' : 'org.eclipse.jdt.launching.VM_CONNECTOR_ID', 'value' : 'org.eclipse.jdt.launching.socketAttachConnector'})\n    launch.close('launchConfiguration')\n    launch = launch.xml(newl='\\n', standalone='no') % slm.xml(escape=True, standalone='no')\n\n    if name is None:\n        if len(suites()) == 1:\n            suitePrefix = ''\n        else:\n            suitePrefix = suite.name + '-'\n        name = suitePrefix + 'attach-' + hostname + '-' + port\n    eclipseLaunches = join(suite.mxDir, 'eclipse-launches')\n    if not exists(eclipseLaunches):\n        os.makedirs(eclipseLaunches)\n    launchFile = join(eclipseLaunches, name + '.launch')\n    return update_file(launchFile, launch), launchFile\n\ndef make_eclipse_launch(javaArgs, jre, name=None, deps=None):\n    \"\"\"\n    Creates an Eclipse launch configuration file for running/debugging a Java command.\n    \"\"\"\n    if deps is None:\n        deps = []\n    mainClass = None\n    vmArgs = []\n    appArgs = []\n    cp = None\n    argsCopy = list(reversed(javaArgs))\n    while len(argsCopy) != 0:\n        a = argsCopy.pop()\n        if a == '-jar':\n            mainClass = '-jar'\n            appArgs = list(reversed(argsCopy))\n            break\n        if a == '-cp' or a == '-classpath':\n            assert len(argsCopy) != 0\n            cp = argsCopy.pop()\n            vmArgs.append(a)\n            vmArgs.append(cp)\n        elif a.startswith('-'):\n            vmArgs.append(a)\n        else:\n            mainClass = a\n            appArgs = list(reversed(argsCopy))\n            break\n\n    if mainClass is None:\n        log('Cannot create Eclipse launch configuration without main class or jar file: java ' + ' '.join(javaArgs))\n        return False\n\n    if name is None:\n        if mainClass == '-jar':\n            name = basename(appArgs[0])\n            if len(appArgs) > 1 and not appArgs[1].startswith('-'):\n                name = name + '_' + appArgs[1]\n        else:\n            name = mainClass\n        name = time.strftime('%Y-%m-%d-%H%M%S_' + name)\n\n    if cp is not None:\n        for e in cp.split(os.pathsep):\n            for s in suites():\n                deps += [p for p in s.projects if e == p.output_dir()]\n                deps += [l for l in s.libs if e == l.get_path(False)]\n\n    slm = _source_locator_memento(deps)\n\n    launch = XMLDoc()\n    launch.open('launchConfiguration', {'type' : 'org.eclipse.jdt.launching.localJavaApplication'})\n    launch.element('stringAttribute', {'key' : 'org.eclipse.debug.core.source_locator_id', 'value' : 'org.eclipse.jdt.launching.sourceLocator.JavaSourceLookupDirector'})\n    launch.element('stringAttribute', {'key' : 'org.eclipse.debug.core.source_locator_memento', 'value' : '%s'})\n    launch.element('stringAttribute', {'key' : 'org.eclipse.jdt.launching.JRE_CONTAINER', 'value' : 'org.eclipse.jdt.launching.JRE_CONTAINER/org.eclipse.jdt.internal.debug.ui.launcher.StandardVMType/' + jre})\n    launch.element('stringAttribute', {'key' : 'org.eclipse.jdt.launching.MAIN_TYPE', 'value' : mainClass})\n    launch.element('stringAttribute', {'key' : 'org.eclipse.jdt.launching.PROGRAM_ARGUMENTS', 'value' : ' '.join(appArgs)})\n    launch.element('stringAttribute', {'key' : 'org.eclipse.jdt.launching.PROJECT_ATTR', 'value' : ''})\n    launch.element('stringAttribute', {'key' : 'org.eclipse.jdt.launching.VM_ARGUMENTS', 'value' : ' '.join(vmArgs)})\n    launch.close('launchConfiguration')\n    launch = launch.xml(newl='\\n', standalone='no') % slm.xml(escape=True, standalone='no')\n\n    eclipseLaunches = join('mx', 'eclipse-launches')\n    if not exists(eclipseLaunches):\n        os.makedirs(eclipseLaunches)\n    return update_file(join(eclipseLaunches, name + '.launch'), launch)\n\ndef eclipseinit(args, buildProcessorJars=True, refreshOnly=False):\n    \"\"\"(re)generate Eclipse project configurations and working sets\"\"\"\n    for s in suites(True):\n        _eclipseinit_suite(args, s, buildProcessorJars, refreshOnly)\n\n    generate_eclipse_workingsets()\n\ndef _check_ide_timestamp(suite, configZip, ide):\n    \"\"\"return True if and only if the projects file, imports file, eclipse-settings files, and mx itself are all older than configZip\"\"\"\n    projectsFile = join(suite.mxDir, 'projects')\n    if configZip.isOlderThan(projectsFile):\n        return False\n    if configZip.isOlderThan(suite.import_timestamp()):\n        return False\n    # Assume that any mx change might imply changes to the generated IDE files\n    if configZip.isOlderThan(__file__):\n        return False\n\n    if ide == 'eclipse':\n        eclipseSettingsDir = join(suite.mxDir, 'eclipse-settings')\n        if exists(eclipseSettingsDir):\n            for name in os.listdir(eclipseSettingsDir):\n                path = join(eclipseSettingsDir, name)\n                if configZip.isOlderThan(path):\n                    return False\n    return True\n\ndef _eclipseinit_project(p, files=None, libFiles=None):\n    assert java(p.javaCompliance)\n\n    if not exists(p.dir):\n        os.makedirs(p.dir)\n\n    out = XMLDoc()\n    out.open('classpath')\n\n    for src in p.srcDirs:\n        srcDir = join(p.dir, src)\n        if not exists(srcDir):\n            os.mkdir(srcDir)\n        out.element('classpathentry', {'kind' : 'src', 'path' : src})\n\n    if len(p.annotation_processors()) > 0:\n        genDir = p.source_gen_dir()\n        if not exists(genDir):\n            os.mkdir(genDir)\n        out.element('classpathentry', {'kind' : 'src', 'path' : 'src_gen'})\n        if files:\n            files.append(genDir)\n\n    # Every Java program depends on a JRE\n    out.element('classpathentry', {'kind' : 'con', 'path' : 'org.eclipse.jdt.launching.JRE_CONTAINER/org.eclipse.jdt.internal.debug.ui.launcher.StandardVMType/JavaSE-' + str(p.javaCompliance)})\n\n    if exists(join(p.dir, 'plugin.xml')):  # eclipse plugin project\n        out.element('classpathentry', {'kind' : 'con', 'path' : 'org.eclipse.pde.core.requiredPlugins'})\n\n    containerDeps = set()\n    libraryDeps = set()\n    projectDeps = set()\n\n    for dep in p.all_deps([], True):\n        if dep == p:\n            continue\n        if dep.isLibrary():\n            if hasattr(dep, 'eclipse.container'):\n                container = getattr(dep, 'eclipse.container')\n                containerDeps.add(container)\n                libraryDeps -= set(dep.all_deps([], True))\n            else:\n                libraryDeps.add(dep)\n        elif dep.isProject():\n            projectDeps.add(dep)\n\n    for dep in containerDeps:\n        out.element('classpathentry', {'exported' : 'true', 'kind' : 'con', 'path' : dep})\n\n    for dep in libraryDeps:\n        path = dep.path\n        dep.get_path(resolve=True)\n\n        # Relative paths for \"lib\" class path entries have various semantics depending on the Eclipse\n        # version being used (e.g. see https://bugs.eclipse.org/bugs/show_bug.cgi?id=274737) so it's\n        # safest to simply use absolute paths.\n\n        # It's important to use dep.suite as the location for when one suite references\n        # a library in another suite.\n        path = _make_absolute(path, dep.suite.dir)\n\n        attributes = {'exported' : 'true', 'kind' : 'lib', 'path' : path}\n\n        sourcePath = dep.get_source_path(resolve=True)\n        if sourcePath is not None:\n            attributes['sourcepath'] = sourcePath\n        out.element('classpathentry', attributes)\n        if libFiles:\n            libFiles.append(path)\n\n    for dep in projectDeps:\n        out.element('classpathentry', {'combineaccessrules' : 'false', 'exported' : 'true', 'kind' : 'src', 'path' : '/' + dep.name})\n\n    out.element('classpathentry', {'kind' : 'output', 'path' : getattr(p, 'eclipse.output', 'bin')})\n    out.close('classpath')\n    classpathFile = join(p.dir, '.classpath')\n    update_file(classpathFile, out.xml(indent='\\t', newl='\\n'))\n    if files:\n        files.append(classpathFile)\n\n    csConfig = join(project(p.checkstyleProj).dir, '.checkstyle_checks.xml')\n    if exists(csConfig):\n        out = XMLDoc()\n\n        dotCheckstyle = join(p.dir, \".checkstyle\")\n        checkstyleConfigPath = '/' + p.checkstyleProj + '/.checkstyle_checks.xml'\n        out.open('fileset-config', {'file-format-version' : '1.2.0', 'simple-config' : 'true'})\n        out.open('local-check-config', {'name' : 'Checks', 'location' : checkstyleConfigPath, 'type' : 'project', 'description' : ''})\n        out.element('additional-data', {'name' : 'protect-config-file', 'value' : 'false'})\n        out.close('local-check-config')\n        out.open('fileset', {'name' : 'all', 'enabled' : 'true', 'check-config-name' : 'Checks', 'local' : 'true'})\n        out.element('file-match-pattern', {'match-pattern' : '.', 'include-pattern' : 'true'})\n        out.close('fileset')\n        out.open('filter', {'name' : 'all', 'enabled' : 'true', 'check-config-name' : 'Checks', 'local' : 'true'})\n        out.element('filter-data', {'value' : 'java'})\n        out.close('filter')\n\n        exclude = join(p.dir, '.checkstyle.exclude')\n        if exists(exclude):\n            out.open('filter', {'name' : 'FilesFromPackage', 'enabled' : 'true'})\n            with open(exclude) as f:\n                for line in f:\n                    if not line.startswith('#'):\n                        line = line.strip()\n                        exclDir = join(p.dir, line)\n                        assert isdir(exclDir), 'excluded source directory listed in ' + exclude + ' does not exist or is not a directory: ' + exclDir\n                    out.element('filter-data', {'value' : line})\n            out.close('filter')\n\n        out.close('fileset-config')\n        update_file(dotCheckstyle, out.xml(indent='  ', newl='\\n'))\n        if files:\n            files.append(dotCheckstyle)\n    else:\n        # clean up existing .checkstyle file\n        dotCheckstyle = join(p.dir, \".checkstyle\")\n        if exists(dotCheckstyle):\n            os.unlink(dotCheckstyle)\n\n    out = XMLDoc()\n    out.open('projectDescription')\n    out.element('name', data=p.name)\n    out.element('comment', data='')\n    out.element('projects', data='')\n    out.open('buildSpec')\n    out.open('buildCommand')\n    out.element('name', data='org.eclipse.jdt.core.javabuilder')\n    out.element('arguments', data='')\n    out.close('buildCommand')\n    if exists(csConfig):\n        out.open('buildCommand')\n        out.element('name', data='net.sf.eclipsecs.core.CheckstyleBuilder')\n        out.element('arguments', data='')\n        out.close('buildCommand')\n    if exists(join(p.dir, 'plugin.xml')):  # eclipse plugin project\n        for buildCommand in ['org.eclipse.pde.ManifestBuilder', 'org.eclipse.pde.SchemaBuilder']:\n            out.open('buildCommand')\n            out.element('name', data=buildCommand)\n            out.element('arguments', data='')\n            out.close('buildCommand')\n\n    # The path should always be p.name/dir. independent of where the workspace actually is.\n    # So we use the parent folder of the project, whatever that is, to generate such a relative path.\n    logicalWorkspaceRoot = os.path.dirname(p.dir)\n    binFolder = os.path.relpath(p.output_dir(), logicalWorkspaceRoot)\n\n    if _isAnnotationProcessorDependency(p):\n        refreshFile = os.path.relpath(join(p.dir, p.name + '.jar'), logicalWorkspaceRoot)\n        _genEclipseBuilder(out, p, 'Jar', 'archive ' + p.name, refresh=True, refreshFile=refreshFile, relevantResources=[binFolder], async=True, xmlIndent='', xmlStandalone='no')\n\n    out.close('buildSpec')\n    out.open('natures')\n    out.element('nature', data='org.eclipse.jdt.core.javanature')\n    if exists(csConfig):\n        out.element('nature', data='net.sf.eclipsecs.core.CheckstyleNature')\n    if exists(join(p.dir, 'plugin.xml')):  # eclipse plugin project\n        out.element('nature', data='org.eclipse.pde.PluginNature')\n    out.close('natures')\n    out.close('projectDescription')\n    projectFile = join(p.dir, '.project')\n    update_file(projectFile, out.xml(indent='\\t', newl='\\n'))\n    if files:\n        files.append(projectFile)\n\n    settingsDir = join(p.dir, \".settings\")\n    if not exists(settingsDir):\n        os.mkdir(settingsDir)\n\n    # collect the defaults from mxtool\n    defaultEclipseSettingsDir = join(dirname(__file__), 'eclipse-settings')\n    esdict = {}\n    if exists(defaultEclipseSettingsDir):\n        for name in os.listdir(defaultEclipseSettingsDir):\n            if isfile(join(defaultEclipseSettingsDir, name)):\n                esdict[name] = os.path.abspath(join(defaultEclipseSettingsDir, name))\n\n    # check for suite overrides\n    eclipseSettingsDir = join(p.suite.mxDir, 'eclipse-settings')\n    if exists(eclipseSettingsDir):\n        for name in os.listdir(eclipseSettingsDir):\n            if isfile(join(eclipseSettingsDir, name)):\n                esdict[name] = os.path.abspath(join(eclipseSettingsDir, name))\n\n    # check for project overrides\n    projectSettingsDir = join(p.dir, 'eclipse-settings')\n    if exists(projectSettingsDir):\n        for name in os.listdir(projectSettingsDir):\n            if isfile(join(projectSettingsDir, name)):\n                esdict[name] = os.path.abspath(join(projectSettingsDir, name))\n\n    # copy a possibly modified file to the project's .settings directory\n    for name, path in esdict.iteritems():\n        # ignore this file altogether if this project has no annotation processors\n        if name == \"org.eclipse.jdt.apt.core.prefs\" and not len(p.annotation_processors()) > 0:\n            continue\n\n        with open(path) as f:\n            content = f.read()\n        content = content.replace('${javaCompliance}', str(p.javaCompliance))\n        if len(p.annotation_processors()) > 0:\n            content = content.replace('org.eclipse.jdt.core.compiler.processAnnotations=disabled', 'org.eclipse.jdt.core.compiler.processAnnotations=enabled')\n        update_file(join(settingsDir, name), content)\n        if files:\n            files.append(join(settingsDir, name))\n\n    if len(p.annotation_processors()) > 0:\n        out = XMLDoc()\n        out.open('factorypath')\n        out.element('factorypathentry', {'kind' : 'PLUGIN', 'id' : 'org.eclipse.jst.ws.annotations.core', 'enabled' : 'true', 'runInBatchMode' : 'false'})\n        for ap in p.annotation_processors():\n            for dep in dependency(ap).all_deps([], True):\n                if dep.isLibrary():\n                    # Relative paths for \"lib\" class path entries have various semantics depending on the Eclipse\n                    # version being used (e.g. see https://bugs.eclipse.org/bugs/show_bug.cgi?id=274737) so it's\n                    # safest to simply use absolute paths.\n                    path = _make_absolute(dep.get_path(resolve=True), p.suite.dir)\n                    out.element('factorypathentry', {'kind' : 'EXTJAR', 'id' : path, 'enabled' : 'true', 'runInBatchMode' : 'false'})\n                    if files:\n                        files.append(path)\n                elif dep.isProject():\n                    out.element('factorypathentry', {'kind' : 'WKSPJAR', 'id' : '/' + dep.name + '/' + dep.name + '.jar', 'enabled' : 'true', 'runInBatchMode' : 'false'})\n        out.close('factorypath')\n        update_file(join(p.dir, '.factorypath'), out.xml(indent='\\t', newl='\\n'))\n        if files:\n            files.append(join(p.dir, '.factorypath'))\n\ndef _eclipseinit_suite(args, suite, buildProcessorJars=True, refreshOnly=False):\n    configZip = TimeStampFile(join(suite.mxDir, 'eclipse-config.zip'))\n    configLibsZip = join(suite.mxDir, 'eclipse-config-libs.zip')\n    if refreshOnly and not configZip.exists():\n        return\n\n    if _check_ide_timestamp(suite, configZip, 'eclipse'):\n        logv('[Eclipse configurations are up to date - skipping]')\n        return\n\n\n\n    files = []\n    libFiles = []\n    if buildProcessorJars:\n        files += _processorjars_suite(suite)\n\n    projToDist = dict()\n    for dist in _dists.values():\n        distDeps = dist.sorted_deps()\n        for p in distDeps:\n            projToDist[p.name] = (dist, [dep.name for dep in distDeps])\n\n    for p in suite.projects:\n        if p.native:\n            continue\n        _eclipseinit_project(p)\n\n    _, launchFile = make_eclipse_attach(suite, 'localhost', '8000', deps=sorted_deps(projectNames=None, includeLibs=True))\n    files.append(launchFile)\n\n    _zip_files(files, suite.dir, configZip.path)\n    _zip_files(libFiles, suite.dir, configLibsZip)\n\n    # Create an Eclipse project for each distribution that will create/update the archive\n    # for the distribution whenever any project of the distribution is updated.\n    for dist in suite.dists:\n        if hasattr(dist, 'subDir'):\n            projectDir = join(suite.dir, dist.subDir, dist.name + '.dist')\n        else:\n            projectDir = join(suite.dir, dist.name + '.dist')\n        if not exists(projectDir):\n            os.makedirs(projectDir)\n        distProjects = [d for d in dist.sorted_deps() if d.isProject()]\n        relevantResources = []\n        for p in distProjects:\n            for srcDir in p.source_dirs():\n                relevantResources.append(join(p.name, os.path.relpath(srcDir, p.dir)))\n            relevantResources.append(join(p.name, os.path.relpath(p.output_dir(), p.dir)))\n        out = XMLDoc()\n        out.open('projectDescription')\n        out.element('name', data=dist.name)\n        out.element('comment', data='Updates ' + dist.path + ' if a project dependency of ' + dist.name + ' is updated')\n        out.open('projects')\n        for p in distProjects:\n            out.element('project', data=p.name)\n        out.close('projects')\n        out.open('buildSpec')\n        dist.dir = projectDir\n        dist.javaCompliance = max([p.javaCompliance for p in distProjects])\n        _genEclipseBuilder(out, dist, 'Create' + dist.name + 'Dist', 'archive @' + dist.name, relevantResources=relevantResources, logToFile=True, refresh=False, async=True)\n        out.close('buildSpec')\n        out.open('natures')\n        out.element('nature', data='org.eclipse.jdt.core.javanature')\n        out.close('natures')\n        out.close('projectDescription')\n        projectFile = join(projectDir, '.project')\n        update_file(projectFile, out.xml(indent='\\t', newl='\\n'))\n        files.append(projectFile)\n\ndef _zip_files(files, baseDir, zipPath):\n    fd, tmp = tempfile.mkstemp(suffix='', prefix=basename(zipPath), dir=baseDir)\n    try:\n        zf = zipfile.ZipFile(tmp, 'w')\n        for f in sorted(set(files)):\n            relpath = os.path.relpath(f, baseDir)\n            arcname = relpath.replace(os.sep, '/')\n            zf.write(f, arcname)\n        zf.close()\n        os.close(fd)\n        # Atomic on Unix\n        shutil.move(tmp, zipPath)\n        # Correct the permissions on the temporary file which is created with restrictive permissions\n        os.chmod(zipPath, 0o666 & ~currentUmask)\n    finally:\n        if exists(tmp):\n            os.remove(tmp)\n\ndef _isAnnotationProcessorDependency(p):\n    \"\"\"\n    Determines if a given project is part of an annotation processor.\n    \"\"\"\n    return p in sorted_deps(annotation_processors())\n\ndef _genEclipseBuilder(dotProjectDoc, p, name, mxCommand, refresh=True, refreshFile=None, relevantResources=None, async=False, logToConsole=False, logToFile=False, appendToLogFile=True, xmlIndent='\\t', xmlStandalone=None):\n    externalToolDir = join(p.dir, '.externalToolBuilders')\n    launchOut = XMLDoc()\n    consoleOn = 'true' if logToConsole else 'false'\n    launchOut.open('launchConfiguration', {'type' : 'org.eclipse.ui.externaltools.ProgramBuilderLaunchConfigurationType'})\n    launchOut.element('booleanAttribute', {'key' : 'org.eclipse.debug.core.capture_output', 'value': consoleOn})\n    launchOut.open('mapAttribute', {'key' : 'org.eclipse.debug.core.environmentVariables'})\n    launchOut.element('mapEntry', {'key' : 'JAVA_HOME', 'value' : java(p.javaCompliance).jdk})\n    launchOut.element('mapEntry', {'key' : 'EXTRA_JAVA_HOMES', 'value' : _opts.extra_java_homes})\n    launchOut.close('mapAttribute')\n\n    if refresh:\n        if refreshFile is None:\n            refreshScope = '${project}'\n        else:\n            refreshScope = '${working_set:<?xml version=\"1.0\" encoding=\"UTF-8\"?><resources><item path=\"' + refreshFile + '\" type=\"1\"/></resources>}'\n\n        launchOut.element('booleanAttribute', {'key' : 'org.eclipse.debug.core.ATTR_REFRESH_RECURSIVE', 'value':  'false'})\n        launchOut.element('stringAttribute', {'key' : 'org.eclipse.debug.core.ATTR_REFRESH_SCOPE', 'value':  refreshScope})\n\n    if relevantResources is not None:\n        resources = '${working_set:<?xml version=\"1.0\" encoding=\"UTF-8\"?><resources>'\n        for relevantResource in relevantResources:\n            resources += '<item path=\"' + relevantResource + '\" type=\"2\" />'\n        resources += '</resources>}'\n        launchOut.element('stringAttribute', {'key' : 'org.eclipse.ui.externaltools.ATTR_BUILD_SCOPE', 'value': resources})\n\n    launchOut.element('booleanAttribute', {'key' : 'org.eclipse.debug.ui.ATTR_CONSOLE_OUTPUT_ON', 'value': consoleOn})\n    launchOut.element('booleanAttribute', {'key' : 'org.eclipse.debug.ui.ATTR_LAUNCH_IN_BACKGROUND', 'value': 'true' if async else 'false'})\n    if logToFile:\n        logFile = join(externalToolDir, name + '.log')\n        launchOut.element('stringAttribute', {'key' : 'org.eclipse.debug.ui.ATTR_CAPTURE_IN_FILE', 'value': logFile})\n        launchOut.element('booleanAttribute', {'key' : 'org.eclipse.debug.ui.ATTR_APPEND_TO_FILE', 'value': 'true' if appendToLogFile else 'false'})\n\n    # expect to find the OS command to invoke mx in the same directory\n    baseDir = dirname(os.path.abspath(__file__))\n\n    cmd = 'mx'\n    if get_os() == 'windows':\n        cmd = 'mx.cmd'\n    cmdPath = join(baseDir, cmd)\n    if not os.path.exists(cmdPath):\n        # backwards compatibility for when the commands lived in parent of mxtool\n        if cmd == 'mx':\n            cmd = 'mx.sh'\n        cmdPath = join(dirname(baseDir), cmd)\n        if not os.path.exists(cmdPath):\n            abort('cannot locate ' + cmd)\n\n    launchOut.element('stringAttribute', {'key' : 'org.eclipse.ui.externaltools.ATTR_LOCATION', 'value':  cmdPath})\n    launchOut.element('stringAttribute', {'key' : 'org.eclipse.ui.externaltools.ATTR_RUN_BUILD_KINDS', 'value': 'auto,full,incremental'})\n    launchOut.element('stringAttribute', {'key' : 'org.eclipse.ui.externaltools.ATTR_TOOL_ARGUMENTS', 'value': mxCommand})\n    launchOut.element('booleanAttribute', {'key' : 'org.eclipse.ui.externaltools.ATTR_TRIGGERS_CONFIGURED', 'value': 'true'})\n    launchOut.element('stringAttribute', {'key' : 'org.eclipse.ui.externaltools.ATTR_WORKING_DIRECTORY', 'value': p.suite.dir})\n\n\n    launchOut.close('launchConfiguration')\n\n    if not exists(externalToolDir):\n        os.makedirs(externalToolDir)\n    update_file(join(externalToolDir, name + '.launch'), launchOut.xml(indent=xmlIndent, standalone=xmlStandalone, newl='\\n'))\n\n    dotProjectDoc.open('buildCommand')\n    dotProjectDoc.element('name', data='org.eclipse.ui.externaltools.ExternalToolBuilder')\n    dotProjectDoc.element('triggers', data='auto,full,incremental,')\n    dotProjectDoc.open('arguments')\n    dotProjectDoc.open('dictionary')\n    dotProjectDoc.element('key', data='LaunchConfigHandle')\n    dotProjectDoc.element('value', data='<project>/.externalToolBuilders/' + name + '.launch')\n    dotProjectDoc.close('dictionary')\n    dotProjectDoc.open('dictionary')\n    dotProjectDoc.element('key', data='incclean')\n    dotProjectDoc.element('value', data='true')\n    dotProjectDoc.close('dictionary')\n    dotProjectDoc.close('arguments')\n    dotProjectDoc.close('buildCommand')\n\ndef generate_eclipse_workingsets():\n    \"\"\"\n    Populate the workspace's working set configuration with working sets generated from project data for the primary suite\n    If the workspace already contains working set definitions, the existing ones will be retained and extended.\n    In case mx/env does not contain a WORKSPACE definition pointing to the workspace root directory, a parent search from the primary suite directory is performed.\n    If no workspace root directory can be identified, the primary suite directory is used and the user has to place the workingsets.xml file by hand.\n    \"\"\"\n\n    # identify the location where to look for workingsets.xml\n    wsfilename = 'workingsets.xml'\n    wsloc = '.metadata/.plugins/org.eclipse.ui.workbench'\n    if os.environ.has_key('WORKSPACE'):\n        expected_wsroot = os.environ['WORKSPACE']\n    else:\n        expected_wsroot = _primary_suite.dir\n\n    wsroot = _find_eclipse_wsroot(expected_wsroot)\n    if wsroot is None:\n        # failed to find it\n        wsroot = expected_wsroot\n\n    wsdir = join(wsroot, wsloc)\n    if not exists(wsdir):\n        wsdir = wsroot\n        logv('Could not find Eclipse metadata directory. Please place ' + wsfilename + ' in ' + wsloc + ' manually.')\n    wspath = join(wsdir, wsfilename)\n\n    def _add_to_working_set(key, value):\n        if not workingSets.has_key(key):\n            workingSets[key] = [value]\n        else:\n            workingSets[key].append(value)\n\n    # gather working set info from project data\n    workingSets = dict()\n    for p in projects():\n        if p.workingSets is None:\n            continue\n        for w in p.workingSets.split(\",\"):\n            _add_to_working_set(w, p.name)\n\n    # the mx metdata directories are included in the appropriate working sets\n    _add_to_working_set('MX', 'mxtool')\n    for suite in suites(True):\n        _add_to_working_set('MX', basename(suite.mxDir))\n\n    if exists(wspath):\n        wsdoc = _copy_workingset_xml(wspath, workingSets)\n    else:\n        wsdoc = _make_workingset_xml(workingSets)\n\n    update_file(wspath, wsdoc.xml(newl='\\n'))\n\ndef _find_eclipse_wsroot(wsdir):\n    md = join(wsdir, '.metadata')\n    if exists(md):\n        return wsdir\n    split = os.path.split(wsdir)\n    if split[0] == wsdir:  # root directory\n        return None\n    else:\n        return _find_eclipse_wsroot(split[0])\n\ndef _make_workingset_xml(workingSets):\n    wsdoc = XMLDoc()\n    wsdoc.open('workingSetManager')\n\n    for w in sorted(workingSets.keys()):\n        _workingset_open(wsdoc, w)\n        for p in workingSets[w]:\n            _workingset_element(wsdoc, p)\n        wsdoc.close('workingSet')\n\n    wsdoc.close('workingSetManager')\n    return wsdoc\n\ndef _copy_workingset_xml(wspath, workingSets):\n    target = XMLDoc()\n    target.open('workingSetManager')\n\n    parser = xml.parsers.expat.ParserCreate()\n\n    class ParserState(object):\n        def __init__(self):\n            self.current_ws_name = 'none yet'\n            self.current_ws = None\n            self.seen_ws = list()\n            self.seen_projects = list()\n            self.aggregate_ws = False\n            self.nested_ws = False\n\n    ps = ParserState()\n\n    # parsing logic\n    def _ws_start(name, attributes):\n        if name == 'workingSet':\n            if attributes.has_key('name'):\n                ps.current_ws_name = attributes['name']\n                if attributes.has_key('aggregate') and attributes['aggregate'] == 'true':\n                    ps.aggregate_ws = True\n                    ps.current_ws = None\n                elif workingSets.has_key(ps.current_ws_name):\n                    ps.current_ws = workingSets[ps.current_ws_name]\n                    ps.seen_ws.append(ps.current_ws_name)\n                    ps.seen_projects = list()\n                else:\n                    ps.current_ws = None\n            target.open(name, attributes)\n            parser.StartElementHandler = _ws_item\n\n    def _ws_end(name):\n        closeAndResetHandler = False\n        if name == 'workingSet':\n            if ps.aggregate_ws:\n                if ps.nested_ws:\n                    ps.nested_ws = False\n                else:\n                    ps.aggregate_ws = False\n                    closeAndResetHandler = True\n            else:\n                if not ps.current_ws is None:\n                    for p in ps.current_ws:\n                        if not p in ps.seen_projects:\n                            _workingset_element(target, p)\n                closeAndResetHandler = True\n            if closeAndResetHandler:\n                target.close('workingSet')\n                parser.StartElementHandler = _ws_start\n        elif name == 'workingSetManager':\n            # process all working sets that are new to the file\n            for w in sorted(workingSets.keys()):\n                if not w in ps.seen_ws:\n                    _workingset_open(target, w)\n                    for p in workingSets[w]:\n                        _workingset_element(target, p)\n                    target.close('workingSet')\n\n    def _ws_item(name, attributes):\n        if name == 'item':\n            if ps.current_ws is None:\n                target.element(name, attributes)\n            elif not attributes.has_key('elementID') and attributes.has_key('factoryID') and attributes.has_key('path') and attributes.has_key('type'):\n                target.element(name, attributes)\n                p_name = attributes['path'][1:]  # strip off the leading '/'\n                ps.seen_projects.append(p_name)\n            else:\n                p_name = attributes['elementID'][1:]  # strip off the leading '='\n                _workingset_element(target, p_name)\n                ps.seen_projects.append(p_name)\n        elif name == 'workingSet':\n            ps.nested_ws = True\n            target.element(name, attributes)\n\n    # process document\n    parser.StartElementHandler = _ws_start\n    parser.EndElementHandler = _ws_end\n    with open(wspath, 'r') as wsfile:\n        parser.ParseFile(wsfile)\n\n    target.close('workingSetManager')\n    return target\n\ndef _workingset_open(wsdoc, ws):\n    wsdoc.open('workingSet', {'editPageID': 'org.eclipse.jdt.ui.JavaWorkingSetPage', 'factoryID': 'org.eclipse.ui.internal.WorkingSetFactory', 'id': 'wsid_' + ws, 'label': ws, 'name': ws})\n\ndef _workingset_element(wsdoc, p):\n    wsdoc.element('item', {'elementID': '=' + p, 'factoryID': 'org.eclipse.jdt.ui.PersistableJavaElementFactory'})\n\ndef netbeansinit(args, refreshOnly=False, buildProcessorJars=True):\n    \"\"\"(re)generate NetBeans project configurations\"\"\"\n\n    for suite in suites(True):\n        _netbeansinit_suite(args, suite, refreshOnly, buildProcessorJars)\n\ndef _netbeansinit_suite(args, suite, refreshOnly=False, buildProcessorJars=True):\n    configZip = TimeStampFile(join(suite.mxDir, 'netbeans-config.zip'))\n    configLibsZip = join(suite.mxDir, 'eclipse-config-libs.zip')\n    if refreshOnly and not configZip.exists():\n        return\n\n    if _check_ide_timestamp(suite, configZip, 'netbeans'):\n        logv('[NetBeans configurations are up to date - skipping]')\n        return\n\n    updated = False\n    files = []\n    libFiles = []\n    jdks = set()\n    for p in suite.projects:\n        if p.native:\n            continue\n\n        if exists(join(p.dir, 'plugin.xml')):  # eclipse plugin project\n            continue\n\n        if not exists(join(p.dir, 'nbproject')):\n            os.makedirs(join(p.dir, 'nbproject'))\n\n        jdk = java(p.javaCompliance)\n\n        assert jdk\n\n        jdks.add(jdk)\n\n        out = XMLDoc()\n        out.open('project', {'name' : p.name, 'default' : 'default', 'basedir' : '.'})\n        out.element('description', data='Builds, tests, and runs the project ' + p.name + '.')\n        out.element('import', {'file' : 'nbproject/build-impl.xml'})\n        out.open('target', {'name' : '-post-compile'})\n        out.open('exec', {'executable' : sys.executable})\n        out.element('env', {'key' : 'JAVA_HOME', 'value' : jdk.jdk})\n        out.element('arg', {'value' : os.path.abspath(__file__)})\n        out.element('arg', {'value' : 'archive'})\n        out.element('arg', {'value' : '@GRAAL'})\n        out.close('exec')\n        out.close('target')\n        out.close('project')\n        updated = update_file(join(p.dir, 'build.xml'), out.xml(indent='\\t', newl='\\n')) or updated\n        files.append(join(p.dir, 'build.xml'))\n\n        out = XMLDoc()\n        out.open('project', {'xmlns' : 'http://www.netbeans.org/ns/project/1'})\n        out.element('type', data='org.netbeans.modules.java.j2seproject')\n        out.open('configuration')\n        out.open('data', {'xmlns' : 'http://www.netbeans.org/ns/j2se-project/3'})\n        out.element('name', data=p.name)\n        out.element('explicit-platform', {'explicit-source-supported' : 'true'})\n        out.open('source-roots')\n        out.element('root', {'id' : 'src.dir'})\n        if len(p.annotation_processors()) > 0:\n            out.element('root', {'id' : 'src.ap-source-output.dir'})\n        out.close('source-roots')\n        out.open('test-roots')\n        out.close('test-roots')\n        out.close('data')\n\n        firstDep = True\n        for dep in p.all_deps([], True):\n            if dep == p:\n                continue\n\n            if dep.isProject():\n                n = dep.name.replace('.', '_')\n                if firstDep:\n                    out.open('references', {'xmlns' : 'http://www.netbeans.org/ns/ant-project-references/1'})\n                    firstDep = False\n\n                out.open('reference')\n                out.element('foreign-project', data=n)\n                out.element('artifact-type', data='jar')\n                out.element('script', data='build.xml')\n                out.element('target', data='jar')\n                out.element('clean-target', data='clean')\n                out.element('id', data='jar')\n                out.close('reference')\n\n        if not firstDep:\n            out.close('references')\n\n        out.close('configuration')\n        out.close('project')\n        updated = update_file(join(p.dir, 'nbproject', 'project.xml'), out.xml(indent='    ', newl='\\n')) or updated\n        files.append(join(p.dir, 'nbproject', 'project.xml'))\n\n        out = StringIO.StringIO()\n        jdkPlatform = 'JDK_' + str(jdk.version)\n\n        annotationProcessorEnabled = \"false\"\n        annotationProcessorReferences = \"\"\n        annotationProcessorSrcFolder = \"\"\n        if len(p.annotation_processors()) > 0:\n            annotationProcessorEnabled = \"true\"\n            annotationProcessorSrcFolder = \"src.ap-source-output.dir=${build.generated.sources.dir}/ap-source-output\"\n\n        content = \"\"\"\nannotation.processing.enabled=\"\"\" + annotationProcessorEnabled + \"\"\"\nannotation.processing.enabled.in.editor=\"\"\" + annotationProcessorEnabled + \"\"\"\nannotation.processing.processors.list=\nannotation.processing.run.all.processors=true\napplication.title=\"\"\" + p.name + \"\"\"\napplication.vendor=mx\nbuild.classes.dir=${build.dir}\nbuild.classes.excludes=**/*.java,**/*.form\n# This directory is removed when the project is cleaned:\nbuild.dir=bin\nbuild.generated.dir=${build.dir}/generated\nbuild.generated.sources.dir=${build.dir}/generated-sources\n# Only compile against the classpath explicitly listed here:\nbuild.sysclasspath=ignore\nbuild.test.classes.dir=${build.dir}/test/classes\nbuild.test.results.dir=${build.dir}/test/results\n# Uncomment to specify the preferred debugger connection transport:\n#debug.transport=dt_socket\ndebug.classpath=\\\\\n    ${run.classpath}\ndebug.test.classpath=\\\\\n    ${run.test.classpath}\n# This directory is removed when the project is cleaned:\ndist.dir=dist\ndist.jar=${dist.dir}/\"\"\" + p.name + \"\"\".jar\ndist.javadoc.dir=${dist.dir}/javadoc\nendorsed.classpath=\nexcludes=\nincludes=**\njar.compress=false\n# Space-separated list of extra javac options\njavac.compilerargs=\njavac.deprecation=false\njavac.source=1.7\njavac.target=1.7\njavac.test.classpath=\\\\\n    ${javac.classpath}:\\\\\n    ${build.classes.dir}\njavadoc.additionalparam=\njavadoc.author=false\njavadoc.encoding=${source.encoding}\njavadoc.noindex=false\njavadoc.nonavbar=false\njavadoc.notree=false\njavadoc.private=false\njavadoc.splitindex=true\njavadoc.use=true\njavadoc.version=false\njavadoc.windowtitle=\nmain.class=\nmanifest.file=manifest.mf\nmeta.inf.dir=${src.dir}/META-INF\nmkdist.disabled=false\nplatforms.\"\"\" + jdkPlatform + \"\"\".home=\"\"\" + jdk.jdk + \"\"\"\nplatform.active=\"\"\" + jdkPlatform + \"\"\"\nrun.classpath=\\\\\n    ${javac.classpath}:\\\\\n    ${build.classes.dir}\n# Space-separated list of JVM arguments used when running the project\n# (you may also define separate properties like run-sys-prop.name=value instead of -Dname=value\n# or test-sys-prop.name=value to set system properties for unit tests):\nrun.jvmargs=\nrun.test.classpath=\\\\\n    ${javac.test.classpath}:\\\\\n    ${build.test.classes.dir}\ntest.src.dir=./test\n\"\"\" + annotationProcessorSrcFolder + \"\"\"\nsource.encoding=UTF-8\"\"\".replace(':', os.pathsep).replace('/', os.sep)\n        print >> out, content\n\n        mainSrc = True\n        for src in p.srcDirs:\n            srcDir = join(p.dir, src)\n            if not exists(srcDir):\n                os.mkdir(srcDir)\n            ref = 'file.reference.' + p.name + '-' + src\n            print >> out, ref + '=' + src\n            if mainSrc:\n                print >> out, 'src.dir=${' + ref + '}'\n                mainSrc = False\n            else:\n                print >> out, 'src.' + src + '.dir=${' + ref + '}'\n\n        javacClasspath = []\n\n        deps = p.all_deps([], True)\n        annotationProcessorOnlyDeps = []\n        if len(p.annotation_processors()) > 0:\n            for ap in p.annotation_processors():\n                apDep = dependency(ap)\n                if not apDep in deps:\n                    deps.append(apDep)\n                    annotationProcessorOnlyDeps.append(apDep)\n\n        annotationProcessorReferences = []\n\n        for dep in deps:\n            if dep == p:\n                continue\n\n            if dep.isLibrary():\n                path = dep.get_path(resolve=True)\n                if path:\n                    if os.sep == '\\\\':\n                        path = path.replace('\\\\', '\\\\\\\\')\n                    ref = 'file.reference.' + dep.name + '-bin'\n                    print >> out, ref + '=' + path\n                    libFiles.append(path)\n\n            elif dep.isProject():\n                n = dep.name.replace('.', '_')\n                relDepPath = os.path.relpath(dep.dir, p.dir).replace(os.sep, '/')\n                ref = 'reference.' + n + '.jar'\n                print >> out, 'project.' + n + '=' + relDepPath\n                print >> out, ref + '=${project.' + n + '}/dist/' + dep.name + '.jar'\n\n            if not dep in annotationProcessorOnlyDeps:\n                javacClasspath.append('${' + ref + '}')\n            else:\n                annotationProcessorReferences.append('${' + ref + '}')\n\n        print >> out, 'javac.classpath=\\\\\\n    ' + (os.pathsep + '\\\\\\n    ').join(javacClasspath)\n        print >> out, 'javac.processorpath=' + (os.pathsep + '\\\\\\n    ').join(['${javac.classpath}'] + annotationProcessorReferences)\n        print >> out, 'javac.test.processorpath=' + (os.pathsep + '\\\\\\n    ').join(['${javac.test.classpath}'] + annotationProcessorReferences)\n\n        updated = update_file(join(p.dir, 'nbproject', 'project.properties'), out.getvalue()) or updated\n        out.close()\n        files.append(join(p.dir, 'nbproject', 'project.properties'))\n\n    if updated:\n        log('If using NetBeans:')\n        log('  1. Ensure that the following platform(s) are defined (Tools -> Java Platforms):')\n        for jdk in jdks:\n            log('        JDK_' + str(jdk.version))\n        log('  2. Open/create a Project Group for the directory containing the projects (File -> Project Group -> New Group... -> Folder of Projects)')\n\n    _zip_files(files, suite.dir, configZip.path)\n    _zip_files(libFiles, suite.dir, configLibsZip)\n\ndef intellijinit(args, refreshOnly=False):\n    \"\"\"(re)generate Intellij project configurations\"\"\"\n\n    for suite in suites(True):\n        _intellij_suite(args, suite, refreshOnly)\n\ndef _intellij_suite(args, suite, refreshOnly=False):\n\n    libraries = set()\n\n    ideaProjectDirectory = join(suite.dir, '.idea')\n\n    if not exists(ideaProjectDirectory):\n        os.mkdir(ideaProjectDirectory)\n    nameFile = join(ideaProjectDirectory, '.name')\n    update_file(nameFile, \"Graal\")\n    modulesXml = XMLDoc()\n    modulesXml.open('project', attributes={'version': '4'})\n    modulesXml.open('component', attributes={'name': 'ProjectModuleManager'})\n    modulesXml.open('modules')\n\n\n    def _intellij_exclude_if_exists(xml, p, name):\n        path = join(p.dir, name)\n        if exists(path):\n            xml.element('excludeFolder', attributes={'url':'file://$MODULE_DIR$/' + name})\n\n    annotationProcessorProfiles = {}\n\n    def _complianceToIntellijLanguageLevel(compliance):\n        return 'JDK_1_' + str(compliance.value)\n\n    # create the modules (1 module  = 1 Intellij project)\n    for p in suite.projects:\n        if p.native:\n            continue\n\n        assert java(p.javaCompliance)\n\n        if not exists(p.dir):\n            os.makedirs(p.dir)\n\n        annotationProcessorProfileKey = tuple(p.annotation_processors())\n\n        if not annotationProcessorProfileKey in annotationProcessorProfiles:\n            annotationProcessorProfiles[annotationProcessorProfileKey] = [p]\n        else:\n            annotationProcessorProfiles[annotationProcessorProfileKey].append(p)\n\n        intellijLanguageLevel = _complianceToIntellijLanguageLevel(p.javaCompliance)\n\n        moduleXml = XMLDoc()\n        moduleXml.open('module', attributes={'type': 'JAVA_MODULE', 'version': '4'})\n\n        moduleXml.open('component', attributes={'name': 'NewModuleRootManager', 'LANGUAGE_LEVEL': intellijLanguageLevel, 'inherit-compiler-output': 'false'})\n        moduleXml.element('output', attributes={'url': 'file://$MODULE_DIR$/bin'})\n        moduleXml.element('exclude-output')\n\n        moduleXml.open('content', attributes={'url': 'file://$MODULE_DIR$'})\n        for src in p.srcDirs:\n            srcDir = join(p.dir, src)\n            if not exists(srcDir):\n                os.mkdir(srcDir)\n            moduleXml.element('sourceFolder', attributes={'url':'file://$MODULE_DIR$/' + src, 'isTestSource': 'false'})\n\n        if len(p.annotation_processors()) > 0:\n            genDir = p.source_gen_dir()\n            if not exists(genDir):\n                os.mkdir(genDir)\n            moduleXml.element('sourceFolder', attributes={'url':'file://$MODULE_DIR$/' + os.path.relpath(genDir, p.dir), 'isTestSource': 'false'})\n\n        for name in ['.externalToolBuilders', '.settings', 'nbproject']:\n            _intellij_exclude_if_exists(moduleXml, p, name)\n        moduleXml.close('content')\n\n        moduleXml.element('orderEntry', attributes={'type': 'jdk', 'jdkType': 'JavaSDK', 'jdkName': str(p.javaCompliance)})\n        moduleXml.element('orderEntry', attributes={'type': 'sourceFolder', 'forTests': 'false'})\n\n        deps = p.all_deps([], True, includeAnnotationProcessors=True)\n        for dep in deps:\n            if dep == p:\n                continue\n\n            if dep.isLibrary():\n                libraries.add(dep)\n                moduleXml.element('orderEntry', attributes={'type': 'library', 'name': dep.name, 'level': 'project'})\n            elif dep.isProject():\n                moduleXml.element('orderEntry', attributes={'type': 'module', 'module-name': dep.name})\n\n        moduleXml.close('component')\n        moduleXml.close('module')\n        moduleFile = join(p.dir, p.name + '.iml')\n        update_file(moduleFile, moduleXml.xml(indent='  ', newl='\\n'))\n\n        moduleFilePath = \"$PROJECT_DIR$/\" + os.path.relpath(moduleFile, suite.dir)\n        modulesXml.element('module', attributes={'fileurl': 'file://' + moduleFilePath, 'filepath': moduleFilePath})\n\n    modulesXml.close('modules')\n    modulesXml.close('component')\n    modulesXml.close('project')\n    moduleXmlFile = join(ideaProjectDirectory, 'modules.xml')\n    update_file(moduleXmlFile, modulesXml.xml(indent='  ', newl='\\n'))\n\n    # TODO What about cross-suite dependencies?\n\n    librariesDirectory = join(ideaProjectDirectory, 'libraries')\n\n    if not exists(librariesDirectory):\n        os.mkdir(librariesDirectory)\n\n    # Setup the libraries that were used above\n    # TODO: setup all the libraries from the suite regardless of usage?\n    for library in libraries:\n        libraryXml = XMLDoc()\n\n        libraryXml.open('component', attributes={'name': 'libraryTable'})\n        libraryXml.open('library', attributes={'name': library.name})\n        libraryXml.open('CLASSES')\n        libraryXml.element('root', attributes={'url': 'jar://$PROJECT_DIR$/' + os.path.relpath(library.get_path(True), suite.dir) + '!/'})\n        libraryXml.close('CLASSES')\n        libraryXml.element('JAVADOC')\n        if library.sourcePath:\n            libraryXml.open('SOURCES')\n            libraryXml.element('root', attributes={'url': 'jar://$PROJECT_DIR$/' + os.path.relpath(library.get_source_path(True), suite.dir) + '!/'})\n            libraryXml.close('SOURCES')\n        else:\n            libraryXml.element('SOURCES')\n        libraryXml.close('library')\n        libraryXml.close('component')\n\n        libraryFile = join(librariesDirectory, library.name + '.xml')\n        update_file(libraryFile, libraryXml.xml(indent='  ', newl='\\n'))\n\n\n\n    # Set annotation processor profiles up, and link them to modules in compiler.xml\n    compilerXml = XMLDoc()\n    compilerXml.open('project', attributes={'version': '4'})\n    compilerXml.open('component', attributes={'name': 'CompilerConfiguration'})\n\n    compilerXml.element('option', attributes={'name': \"DEFAULT_COMPILER\", 'value': 'Javac'})\n    compilerXml.element('resourceExtensions')\n    compilerXml.open('wildcardResourcePatterns')\n    compilerXml.element('entry', attributes={'name': '!?*.java'})\n    compilerXml.close('wildcardResourcePatterns')\n\n    if annotationProcessorProfiles:\n        compilerXml.open('annotationProcessing')\n        for processors, modules in annotationProcessorProfiles.items():\n            compilerXml.open('profile', attributes={'default': 'false', 'name': '-'.join(processors), 'enabled': 'true'})\n            compilerXml.element('sourceOutputDir', attributes={'name': 'src_gen'})  # TODO use p.source_gen_dir() ?\n            compilerXml.element('outputRelativeToContentRoot', attributes={'value': 'true'})\n            compilerXml.open('processorPath', attributes={'useClasspath': 'false'})\n            for apName in processors:\n                pDep = dependency(apName)\n                for entry in pDep.all_deps([], True):\n                    if entry.isLibrary():\n                        compilerXml.element('entry', attributes={'name': '$PROJECT_DIR$/' + os.path.relpath(entry.path, suite.dir)})\n                    elif entry.isProject():\n                        assert entry.isProject()\n                        compilerXml.element('entry', attributes={'name': '$PROJECT_DIR$/' + os.path.relpath(entry.output_dir(), suite.dir)})\n            compilerXml.close('processorPath')\n            for module in modules:\n                compilerXml.element('module', attributes={'name': module.name})\n            compilerXml.close('profile')\n        compilerXml.close('annotationProcessing')\n\n    compilerXml.close('component')\n    compilerXml.close('project')\n    compilerFile = join(ideaProjectDirectory, 'compiler.xml')\n    update_file(compilerFile, compilerXml.xml(indent='  ', newl='\\n'))\n\n    # Wite misc.xml for global JDK config\n    miscXml = XMLDoc()\n    miscXml.open('project', attributes={'version': '4'})\n    miscXml.element('component', attributes={'name': 'ProjectRootManager', 'version': '2', 'languageLevel': _complianceToIntellijLanguageLevel(java().javaCompliance), 'project-jdk-name': str(java().javaCompliance), 'project-jdk-type': 'JavaSDK'})\n    miscXml.close('project')\n    miscFile = join(ideaProjectDirectory, 'misc.xml')\n    update_file(miscFile, miscXml.xml(indent='  ', newl='\\n'))\n\n\n    # TODO look into copyright settings\n    # TODO should add vcs.xml support\n\ndef ideclean(args):\n    \"\"\"remove all Eclipse and NetBeans project configurations\"\"\"\n    def rm(path):\n        if exists(path):\n            os.remove(path)\n\n    for s in suites():\n        rm(join(s.mxDir, 'eclipse-config.zip'))\n        rm(join(s.mxDir, 'netbeans-config.zip'))\n        shutil.rmtree(join(s.dir, '.idea'), ignore_errors=True)\n\n    for p in projects():\n        if p.native:\n            continue\n\n        shutil.rmtree(join(p.dir, '.settings'), ignore_errors=True)\n        shutil.rmtree(join(p.dir, '.externalToolBuilders'), ignore_errors=True)\n        shutil.rmtree(join(p.dir, 'nbproject'), ignore_errors=True)\n        rm(join(p.dir, '.classpath'))\n        rm(join(p.dir, '.checkstyle'))\n        rm(join(p.dir, '.project'))\n        rm(join(p.dir, '.factorypath'))\n        rm(join(p.dir, p.name + '.iml'))\n        rm(join(p.dir, 'build.xml'))\n        rm(join(p.dir, 'eclipse-build.xml'))\n        try:\n            rm(join(p.dir, p.name + '.jar'))\n        except:\n            log(\"Error removing {0}\".format(p.name + '.jar'))\n\n\ndef ideinit(args, refreshOnly=False, buildProcessorJars=True):\n    \"\"\"(re)generate Eclipse, NetBeans and Intellij project configurations\"\"\"\n    eclipseinit(args, refreshOnly=refreshOnly, buildProcessorJars=buildProcessorJars)\n    netbeansinit(args, refreshOnly=refreshOnly, buildProcessorJars=buildProcessorJars)\n    intellijinit(args, refreshOnly=refreshOnly)\n    if not refreshOnly:\n        fsckprojects([])\n\ndef fsckprojects(args):\n    \"\"\"find directories corresponding to deleted Java projects and delete them\"\"\"\n    for suite in suites(True):\n        projectDirs = [p.dir for p in suite.projects]\n        for dirpath, dirnames, files in os.walk(suite.dir):\n            if dirpath == suite.dir:\n                # no point in traversing .hg or lib/\n                dirnames[:] = [d for d in dirnames if d not in ['.hg', 'lib']]\n                # if there are nested suites must not scan those now, as they are not in projectDirs\n                if _src_suitemodel.nestedsuites_dirname() in dirnames:\n                    dirnames.remove(_src_suitemodel.nestedsuites_dirname())\n            elif dirpath in projectDirs:\n                # don't traverse subdirs of an existing project in this suite\n                dirnames[:] = []\n            else:\n                projectConfigFiles = frozenset(['.classpath', 'nbproject'])\n                indicators = projectConfigFiles.intersection(files)\n                if len(indicators) != 0:\n                    if not sys.stdout.isatty() or ask_yes_no(dirpath + ' looks like a removed project -- delete it', 'n'):\n                        shutil.rmtree(dirpath)\n                        log('Deleted ' + dirpath)\n\ndef javadoc(args, parser=None, docDir='javadoc', includeDeps=True, stdDoclet=True):\n    \"\"\"generate javadoc for some/all Java projects\"\"\"\n\n    parser = ArgumentParser(prog='mx javadoc') if parser is None else parser\n    parser.add_argument('-d', '--base', action='store', help='base directory for output')\n    parser.add_argument('--unified', action='store_true', help='put javadoc in a single directory instead of one per project')\n    parser.add_argument('--force', action='store_true', help='(re)generate javadoc even if package-list file exists')\n    parser.add_argument('--projects', action='store', help='comma separated projects to process (omit to process all projects)')\n    parser.add_argument('--Wapi', action='store_true', dest='warnAPI', help='show warnings about using internal APIs')\n    parser.add_argument('--argfile', action='store', help='name of file containing extra javadoc options')\n    parser.add_argument('--arg', action='append', dest='extra_args', help='extra Javadoc arguments (e.g. --arg @-use)', metavar='@<arg>', default=[])\n    parser.add_argument('-m', '--memory', action='store', help='-Xmx value to pass to underlying JVM')\n    parser.add_argument('--packages', action='store', help='comma separated packages to process (omit to process all packages)')\n    parser.add_argument('--exclude-packages', action='store', help='comma separated packages to exclude')\n\n    args = parser.parse_args(args)\n\n    # build list of projects to be processed\n    if args.projects is not None:\n        candidates = [project(name) for name in args.projects.split(',')]\n    else:\n        candidates = projects_opt_limit_to_suites()\n\n    # optionally restrict packages within a project\n    packages = []\n    if args.packages is not None:\n        packages = [name for name in args.packages.split(',')]\n\n    exclude_packages = []\n    if args.exclude_packages is not None:\n        exclude_packages = [name for name in args.exclude_packages.split(',')]\n\n    def outDir(p):\n        if args.base is None:\n            return join(p.dir, docDir)\n        return join(args.base, p.name, docDir)\n\n    def check_package_list(p):\n        return not exists(join(outDir(p), 'package-list'))\n\n    def assess_candidate(p, projects):\n        if p in projects:\n            return False\n        if args.force or args.unified or check_package_list(p):\n            projects.append(p)\n            return True\n        return False\n\n    projects = []\n    for p in candidates:\n        if not p.native:\n            if includeDeps:\n                deps = p.all_deps([], includeLibs=False, includeSelf=False)\n                for d in deps:\n                    assess_candidate(d, projects)\n            if not assess_candidate(p, projects):\n                logv('[package-list file exists - skipping {0}]'.format(p.name))\n\n\n    def find_packages(sourceDirs, pkgs=None):\n        if pkgs is None:\n            pkgs = set()\n        for sourceDir in sourceDirs:\n            for root, _, files in os.walk(sourceDir):\n                if len([name for name in files if name.endswith('.java')]) != 0:\n                    pkg = root[len(sourceDir) + 1:].replace(os.sep, '.')\n                    if len(packages) == 0 or pkg in packages:\n                        if len(exclude_packages) == 0 or not pkg in exclude_packages:\n                            pkgs.add(pkg)\n        return pkgs\n\n    extraArgs = [a.lstrip('@') for a in args.extra_args]\n    if args.argfile is not None:\n        extraArgs += ['@' + args.argfile]\n    memory = '2g'\n    if args.memory is not None:\n        memory = args.memory\n    memory = '-J-Xmx' + memory\n\n    if not args.unified:\n        for p in projects:\n            # The project must be built to ensure javadoc can find class files for all referenced classes\n            build(['--no-native', '--projects', p.name])\n\n            pkgs = find_packages(p.source_dirs(), set())\n            deps = p.all_deps([], includeLibs=False, includeSelf=False)\n            links = ['-link', 'http://docs.oracle.com/javase/' + str(p.javaCompliance.value) + '/docs/api/']\n            out = outDir(p)\n            for d in deps:\n                depOut = outDir(d)\n                links.append('-link')\n                links.append(os.path.relpath(depOut, out))\n            cp = classpath(p.name, includeSelf=True)\n            sp = os.pathsep.join(p.source_dirs())\n            overviewFile = join(p.dir, 'overview.html')\n            delOverviewFile = False\n            if not exists(overviewFile):\n                with open(overviewFile, 'w') as fp:\n                    print >> fp, '<html><body>Documentation for the <code>' + p.name + '</code> project.</body></html>'\n                delOverviewFile = True\n            nowarnAPI = []\n            if not args.warnAPI:\n                nowarnAPI.append('-XDignore.symbol.file')\n\n            # windowTitle onloy applies to the standard doclet processor\n            windowTitle = []\n            if stdDoclet:\n                windowTitle = ['-windowtitle', p.name + ' javadoc']\n            try:\n                log('Generating {2} for {0} in {1}'.format(p.name, out, docDir))\n                projectJava = java(p.javaCompliance)\n\n                # Once https://bugs.openjdk.java.net/browse/JDK-8041628 is fixed,\n                # this should be reverted to:\n                # javadocExe = java().javadoc\n                javadocExe = projectJava.javadoc\n\n                run([javadocExe, memory,\n                     '-XDignore.symbol.file',\n                     '-classpath', cp,\n                     '-quiet',\n                     '-d', out,\n                     '-overview', overviewFile,\n                     '-sourcepath', sp,\n                     '-source', str(projectJava.javaCompliance),\n                     '-bootclasspath', projectJava.bootclasspath(),\n                     '-extdirs', projectJava.extdirs()] +\n                     ([] if projectJava.javaCompliance < JavaCompliance('1.8') else ['-Xdoclint:none']) +\n                     links +\n                     extraArgs +\n                     nowarnAPI +\n                     windowTitle +\n                     list(pkgs))\n                log('Generated {2} for {0} in {1}'.format(p.name, out, docDir))\n            finally:\n                if delOverviewFile:\n                    os.remove(overviewFile)\n\n    else:\n        # The projects must be built to ensure javadoc can find class files for all referenced classes\n        build(['--no-native'])\n\n        pkgs = set()\n        sp = []\n        names = []\n        for p in projects:\n            find_packages(p.source_dirs(), pkgs)\n            sp += p.source_dirs()\n            names.append(p.name)\n\n        links = ['-link', 'http://docs.oracle.com/javase/' + str(java().javaCompliance.value) + '/docs/api/']\n        out = join(_primary_suite.dir, docDir)\n        if args.base is not None:\n            out = join(args.base, docDir)\n        cp = classpath()\n        sp = os.pathsep.join(sp)\n        nowarnAPI = []\n        if not args.warnAPI:\n            nowarnAPI.append('-XDignore.symbol.file')\n        log('Generating {2} for {0} in {1}'.format(', '.join(names), out, docDir))\n        run([java().javadoc, memory,\n             '-classpath', cp,\n             '-quiet',\n             '-d', out,\n             '-sourcepath', sp] +\n             ([] if java().javaCompliance < JavaCompliance('1.8') else ['-Xdoclint:none']) +\n             links +\n             extraArgs +\n             nowarnAPI +\n             list(pkgs))\n        log('Generated {2} for {0} in {1}'.format(', '.join(names), out, docDir))\n\ndef site(args):\n    \"\"\"creates a website containing javadoc and the project dependency graph\"\"\"\n\n    parser = ArgumentParser(prog='site')\n    parser.add_argument('-d', '--base', action='store', help='directory for generated site', required=True, metavar='<dir>')\n    parser.add_argument('--tmp', action='store', help='directory to use for intermediate results', metavar='<dir>')\n    parser.add_argument('--name', action='store', help='name of overall documentation', required=True, metavar='<name>')\n    parser.add_argument('--overview', action='store', help='path to the overview content for overall documentation', required=True, metavar='<path>')\n    parser.add_argument('--projects', action='store', help='comma separated projects to process (omit to process all projects)')\n    parser.add_argument('--jd', action='append', help='extra Javadoc arguments (e.g. --jd @-use)', metavar='@<arg>', default=[])\n    parser.add_argument('--exclude-packages', action='store', help='comma separated packages to exclude', metavar='<pkgs>')\n    parser.add_argument('--dot-output-base', action='store', help='base file name (relative to <dir>/all) for project dependency graph .svg and .jpg files generated by dot (omit to disable dot generation)', metavar='<path>')\n    parser.add_argument('--title', action='store', help='value used for -windowtitle and -doctitle javadoc args for overall documentation (default: \"<name>\")', metavar='<title>')\n    args = parser.parse_args(args)\n\n    args.base = os.path.abspath(args.base)\n    tmpbase = args.tmp if args.tmp else  tempfile.mkdtemp(prefix=basename(args.base) + '.', dir=dirname(args.base))\n    unified = join(tmpbase, 'all')\n\n    exclude_packages_arg = []\n    if args.exclude_packages is not None:\n        exclude_packages_arg = ['--exclude-packages', args.exclude_packages]\n\n    projects = sorted_deps()\n    projects_arg = []\n    if args.projects is not None:\n        projects_arg = ['--projects', args.projects]\n        projects = [project(name) for name in args.projects.split(',')]\n\n    extra_javadoc_args = []\n    for a in args.jd:\n        extra_javadoc_args.append('--arg')\n        extra_javadoc_args.append('@' + a)\n\n    try:\n        # Create javadoc for each project\n        javadoc(['--base', tmpbase] + exclude_packages_arg + projects_arg + extra_javadoc_args)\n\n        # Create unified javadoc for all projects\n        with open(args.overview) as fp:\n            content = fp.read()\n            idx = content.rfind('</body>')\n            if idx != -1:\n                args.overview = join(tmpbase, 'overview_with_projects.html')\n                with open(args.overview, 'w') as fp2:\n                    print >> fp2, content[0:idx]\n                    print >> fp2, \"\"\"<div class=\"contentContainer\">\n<table class=\"overviewSummary\" border=\"0\" cellpadding=\"3\" cellspacing=\"0\" summary=\"Projects table\">\n<caption><span>Projects</span><span class=\"tabEnd\">&nbsp;</span></caption>\n<tr><th class=\"colFirst\" scope=\"col\">Project</th><th class=\"colLast\" scope=\"col\">&nbsp;</th></tr>\n<tbody>\"\"\"\n                    color = 'row'\n                    for p in projects:\n                        print >> fp2, '<tr class=\"{1}Color\"><td class=\"colFirst\"><a href=\"../{0}/javadoc/index.html\",target = \"_top\">{0}</a></td><td class=\"colLast\">&nbsp;</td></tr>'.format(p.name, color)\n                        color = 'row' if color == 'alt' else 'alt'\n\n                    print >> fp2, '</tbody></table></div>'\n                    print >> fp2, content[idx:]\n\n        title = args.title if args.title is not None else args.name\n        javadoc(['--base', tmpbase,\n                 '--unified',\n                 '--arg', '@-windowtitle', '--arg', '@' + title,\n                 '--arg', '@-doctitle', '--arg', '@' + title,\n                 '--arg', '@-overview', '--arg', '@' + args.overview] + exclude_packages_arg + projects_arg + extra_javadoc_args)\n\n        if exists(unified):\n            shutil.rmtree(unified)\n        os.rename(join(tmpbase, 'javadoc'), unified)\n\n        # Generate dependency graph with Graphviz\n        if args.dot_output_base is not None:\n            dotErr = None\n            try:\n                if not 'version' in subprocess.check_output(['dot', '-V'], stderr=subprocess.STDOUT):\n                    dotErr = 'dot -V does not print a string containing \"version\"'\n            except subprocess.CalledProcessError as e:\n                dotErr = 'error calling \"dot -V\": {}'.format(e)\n            except OSError as e:\n                dotErr = 'error calling \"dot -V\": {}'.format(e)\n\n            if dotErr != None:\n                abort('cannot generate dependency graph: ' + dotErr)\n\n            dot = join(tmpbase, 'all', str(args.dot_output_base) + '.dot')\n            svg = join(tmpbase, 'all', str(args.dot_output_base) + '.svg')\n            jpg = join(tmpbase, 'all', str(args.dot_output_base) + '.jpg')\n            html = join(tmpbase, 'all', str(args.dot_output_base) + '.html')\n            with open(dot, 'w') as fp:\n                dim = len(projects)\n                print >> fp, 'digraph projects {'\n                print >> fp, 'rankdir=BT;'\n                print >> fp, 'size = \"' + str(dim) + ',' + str(dim) + '\";'\n                print >> fp, 'node [shape=rect, fontcolor=\"blue\"];'\n                # print >> fp, 'edge [color=\"green\"];'\n                for p in projects:\n                    print >> fp, '\"' + p.name + '\" [URL = \"../' + p.name + '/javadoc/index.html\", target = \"_top\"]'\n                    for dep in p.canonical_deps():\n                        if dep in [proj.name for proj in projects]:\n                            print >> fp, '\"' + p.name + '\" -> \"' + dep + '\"'\n                depths = dict()\n                for p in projects:\n                    d = p.max_depth()\n                    depths.setdefault(d, list()).append(p.name)\n                print >> fp, '}'\n\n            run(['dot', '-Tsvg', '-o' + svg, '-Tjpg', '-o' + jpg, dot])\n\n            # Post-process generated SVG to remove title elements which most browsers\n            # render as redundant (and annoying) tooltips.\n            with open(svg, 'r') as fp:\n                content = fp.read()\n            content = re.sub('<title>.*</title>', '', content)\n            content = re.sub('xlink:title=\"[^\"]*\"', '', content)\n            with open(svg, 'w') as fp:\n                fp.write(content)\n\n            # Create HTML that embeds the svg file in an <object> frame\n            with open(html, 'w') as fp:\n                print >> fp, '<html><body><object data=\"{}.svg\" type=\"image/svg+xml\"></object></body></html>'.format(args.dot_output_base)\n\n\n        if args.tmp:\n            shutil.copytree(tmpbase, args.base)\n        else:\n            shutil.move(tmpbase, args.base)\n\n        print 'Created website - root is ' + join(args.base, 'all', 'index.html')\n\n    finally:\n        if not args.tmp and exists(tmpbase):\n            shutil.rmtree(tmpbase)\n\ndef _kwArg(kwargs):\n    if len(kwargs) > 0:\n        return kwargs.pop(0)\n    return None\n\ndef sclone(args):\n    \"\"\"clone a suite repository, and its imported suites\"\"\"\n    parser = ArgumentParser(prog='mx sclone')\n    parser.add_argument('--source', help='url/path of repo containing suite', metavar='<url>')\n    parser.add_argument('--dest', help='destination directory (default basename of source)', metavar='<path>')\n    parser.add_argument(\"--no-imports\", action='store_true', help='do not clone imported suites')\n    parser.add_argument('nonKWArgs', nargs=REMAINDER, metavar='source [dest]...')\n    args = parser.parse_args(args)\n    # check for non keyword args\n    if args.source is None:\n        args.source = _kwArg(args.nonKWArgs)\n    if args.dest is None:\n        args.dest = _kwArg(args.nonKWArgs)\n    if len(args.nonKWArgs) > 0:\n        abort('unrecognized args: ' + ' '.join(args.nonKWArgs))\n\n    if args.source is None:\n        # must be primary suite and dest is required\n        if _primary_suite is None:\n            abort('--source missing and no primary suite found')\n        if args.dest is None:\n            abort('--dest required when --source is not given')\n        source = _primary_suite.dir\n    else:\n        source = args.source\n\n    _hg.check()\n\n    if args.dest is not None:\n        dest = args.dest\n    else:\n        dest = basename(source)\n\n    dest = os.path.abspath(dest)\n    # We can now set the primary dir for the src/dst suitemodel\n    _dst_suitemodel.set_primary_dir(dest)\n    _src_suitemodel.set_primary_dir(source)\n\n    _sclone(source, dest, None, args.no_imports)\n\ndef _sclone(source, dest, suite_import, no_imports):\n    cmd = ['hg', 'clone']\n    if suite_import is not None and suite_import.version is not None:\n        cmd.append('-r')\n        cmd.append(suite_import.version)\n    cmd.append(source)\n    cmd.append(dest)\n\n    run(cmd)\n\n    mxDir = _is_suite_dir(dest)\n    if mxDir is None:\n        warn(source + ' is not an mx suite')\n        return None\n\n    # create a Suite (without loading) to enable imports visitor\n    s = Suite(mxDir, False, load=False)\n    if not no_imports:\n        s.visit_imports(_scloneimports_visitor, source=source)\n    return s\n\ndef _scloneimports_visitor(s, suite_import, source, **extra_args):\n    \"\"\"\n    cloneimports visitor for Suite.visit_imports.\n    The destination information is encapsulated by 's'\n    \"\"\"\n    _scloneimports(s, suite_import, source)\n\ndef _scloneimports_suitehelper(sdir):\n    mxDir = _is_suite_dir(sdir)\n    if mxDir is None:\n        abort(sdir + ' is not an mx suite')\n    else:\n        # create a Suite (without loading) to enable imports visitor\n        return Suite(mxDir, False, load=False)\n\ndef _scloneimports(s, suite_import, source):\n    # clone first, then visit imports once we can locate them\n    importee_source = _src_suitemodel.importee_dir(source, suite_import)\n    importee_dest = _dst_suitemodel.importee_dir(s.dir, suite_import)\n    if exists(importee_dest):\n        # already exists in the suite model, but may be wrong version\n        importee_suite = _scloneimports_suitehelper(importee_dest)\n        if suite_import.version is not None and importee_suite.version() != suite_import.version:\n            abort(\"imported version of \" + suite_import.name + \" in \" + s.name + \" does not match the version in already existing suite: \" + importee_suite.dir)\n        importee_suite.visit_imports(_scloneimports_visitor, source=importee_source)\n    else:\n        _sclone(importee_source, importee_dest, suite_import, False)\n        # _clone handles the recursive visit of the new imports\n\ndef scloneimports(args):\n    \"\"\"clone the imports of an existing suite\"\"\"\n    parser = ArgumentParser(prog='mx scloneimports')\n    parser.add_argument('--source', help='url/path of repo containing suite', metavar='<url>')\n    parser.add_argument('nonKWArgs', nargs=REMAINDER, metavar='source [dest]...')\n    args = parser.parse_args(args)\n    # check for non keyword args\n    if args.source is None:\n        args.source = _kwArg(args.nonKWArgs)\n\n    if not os.path.isdir(args.source):\n        abort(args.source + ' is not a directory')\n\n    _hg.check()\n    s = _scloneimports_suitehelper(args.source)\n\n    default_path = _hg.default_push(args.source)\n\n    if default_path is None:\n        abort('no default path in ' + join(args.source, '.hg', 'hgrc'))\n\n    # We can now set the primary dir for the dst suitemodel\n    # N.B. source is effectively the destination and the default_path is the (original) source\n    _dst_suitemodel.set_primary_dir(args.source)\n\n    s.visit_imports(_scloneimports_visitor, source=default_path)\n\ndef _spush_import_visitor(s, suite_import, dest, checks, clonemissing, **extra_args):\n    \"\"\"push visitor for Suite.visit_imports\"\"\"\n    if dest is not None:\n        dest = _dst_suitemodel.importee_dir(dest, suite_import)\n    _spush(suite(suite_import.name), suite_import, dest, checks, clonemissing)\n\ndef _spush_check_import_visitor(s, suite_import, **extra_args):\n    \"\"\"push check visitor for Suite.visit_imports\"\"\"\n    currentTip = suite(suite_import.name).version()\n    if currentTip != suite_import.version:\n        abort('imported version of ' + suite_import.name + ' in suite ' + s.name + ' does not match tip')\n\ndef _spush(s, suite_import, dest, checks, clonemissing):\n    if checks['on']:\n        if not _hg.can_push(s, checks['strict']):\n            abort('working directory ' + s.dir + ' contains uncommitted changes, push aborted')\n\n    # check imports first\n    if checks['on']:\n        s.visit_imports(_spush_check_import_visitor)\n\n    # ok, push imports\n    s.visit_imports(_spush_import_visitor, dest=dest, checks=checks, clonemissing=clonemissing)\n\n    dest_exists = True\n\n    if clonemissing:\n        if not os.path.exists(dest):\n            dest_exists = False\n\n    def add_version(cmd, suite_import):\n        if suite_import is not None and suite_import.version is not None:\n            cmd.append('-r')\n            cmd.append(suite_import.version)\n\n    if dest_exists:\n        cmd = ['hg', '-R', s.dir, 'push']\n        add_version(cmd, suite_import)\n        if dest is not None:\n            cmd.append(dest)\n        rc = run(cmd, nonZeroIsFatal=False)\n        if rc != 0:\n            # rc of 1 not an error,  means no changes\n            if rc != 1:\n                abort(\"push failed, exit code \" + str(rc))\n    else:\n        cmd = ['hg', 'clone']\n        add_version(cmd, suite_import)\n        cmd.append(s.dir)\n        cmd.append(dest)\n        run(cmd)\n\ndef spush(args):\n    \"\"\"push primary suite and all its imports\"\"\"\n    parser = ArgumentParser(prog='mx spush')\n    parser.add_argument('--dest', help='url/path of repo to push to (default as per hg push)', metavar='<path>')\n    parser.add_argument('--no-checks', action='store_true', help='checks on status, versions are disabled')\n    parser.add_argument('--no-strict', action='store_true', help='allows not tracked files')\n    parser.add_argument('--clonemissing', action='store_true', help='clone missing imported repos at destination (forces --no-checks)')\n    parser.add_argument('nonKWArgs', nargs=REMAINDER, metavar='source [dest]...')\n    args = parser.parse_args(args)\n    if args.dest is None:\n        args.dest = _kwArg(args.nonKWArgs)\n    if len(args.nonKWArgs) > 0:\n        abort('unrecognized args: ' + ' '.join(args.nonKWArgs))\n\n    _hg.check()\n    s = _check_primary_suite()\n\n    if args.clonemissing:\n        if args.dest is None:\n            abort('--dest required with --clonemissing')\n        args.nochecks = True\n\n    if args.dest is not None:\n        _dst_suitemodel.set_primary_dir(args.dest)\n\n    checks = dict()\n    checks['on'] = not args.no_checks\n    checks['strict'] = not args.no_strict\n    _spush(s, None, args.dest, checks, args.clonemissing)\n\ndef _supdate_import_visitor(s, suite_import, **extra_args):\n    _supdate(suite(suite_import.name), suite_import)\n\ndef _supdate(s, suite_import):\n    s.visit_imports(_supdate_import_visitor)\n\n    run(['hg', '-R', s.dir, 'update'])\n\ndef supdate(args):\n    \"\"\"update primary suite and all its imports\"\"\"\n\n    parser = ArgumentParser(prog='mx supdate')\n    args = parser.parse_args(args)\n    _hg.check()\n    s = _check_primary_suite()\n\n    _supdate(s, None)\n\ndef _scheck_imports_visitor(s, suite_import, update_versions, updated_imports):\n    \"\"\"scheckimports visitor for Suite.visit_imports\"\"\"\n    _scheck_imports(s, suite(suite_import.name), suite_import, update_versions, updated_imports)\n\ndef _scheck_imports(importing_suite, imported_suite, suite_import, update_versions, updated_imports):\n    # check imports recursively\n    imported_suite.visit_imports(_scheck_imports_visitor, update_versions=update_versions)\n\n    currentTip = imported_suite.version()\n    if currentTip != suite_import.version:\n        print 'imported version of ' + imported_suite.name + ' in ' + importing_suite.name + ' does not match tip' + (': updating' if update_versions else '')\n\n    if update_versions:\n        suite_import.version = currentTip\n        line = str(suite_import)\n        updated_imports.write(line + '\\n')\n\ndef scheckimports(args):\n    \"\"\"check that suite import versions are up to date\"\"\"\n    parser = ArgumentParser(prog='mx scheckimports')\n    parser.add_argument('--update-versions', action='store_true', help='update imported version ids')\n    args = parser.parse_args(args)\n    _hg.check()\n    _check_primary_suite().visit_imports(_scheck_imports_visitor, update_versions=args.update_versions)\n\ndef _sforce_imports_visitor(s, suite_import, import_map, strict_versions, **extra_args):\n    \"\"\"sforceimports visitor for Suite.visit_imports\"\"\"\n    _sforce_imports(s, suite(suite_import.name), suite_import, import_map, strict_versions)\n\ndef _sforce_imports(importing_suite, imported_suite, suite_import, import_map, strict_versions):\n    if imported_suite.name in import_map:\n        # we have seen this already\n        if strict_versions:\n            if suite_import.version and import_map[imported_suite.name] != suite_import.version:\n                abort('inconsistent import versions for suite ' + imported_suite.name)\n        return\n    else:\n        import_map[imported_suite.name] = suite_import.version\n\n    if suite_import.version:\n        currentTip = imported_suite.version()\n        if currentTip != suite_import.version:\n            run(['hg', '-R', imported_suite.dir, 'pull', '-r', suite_import.version])\n            run(['hg', '-R', imported_suite.dir, 'update', '-C', '-r', suite_import.version])\n            run(['hg', '-R', imported_suite.dir, 'purge'])\n            # now (may) need to force imports of this suite if the above changed its import revs\n            imported_suite.visit_imports(_sforce_imports_visitor, import_map=import_map, strict_versions=strict_versions)\n    else:\n        # simple case, pull the tip\n        run(['hg', '-R', imported_suite.dir, 'pull', '-u'])\n\ndef sforceimports(args):\n    '''force working directory revision of imported suites to match primary suite imports'''\n    parser = ArgumentParser(prog='mx sforceimports')\n    parser.add_argument('--strict-versions', action='store_true', help='strict version checking')\n    args = parser.parse_args(args)\n    _hg.check()\n    _check_primary_suite().visit_imports(_sforce_imports_visitor, import_map=dict(), strict_versions=args.strict_versions)\n\ndef _spull_import_visitor(s, suite_import, update_versions, updated_imports):\n    \"\"\"pull visitor for Suite.visit_imports\"\"\"\n    _spull(suite(suite_import.name), suite_import, update_versions, updated_imports)\n\ndef _spull(s, suite_import, update_versions, updated_imports):\n    # s is primary suite if suite_import is None otherwise it is an imported suite\n    # proceed top down to get any updated version ids first\n\n    # by default we pull to the revision id in the import\n    cmd = ['hg', '-R', s.dir, 'pull', '-u']\n    if not update_versions and suite_import and suite_import.version:\n        cmd += ['-r', suite_import.version]\n    run(cmd, nonZeroIsFatal=False)\n    if update_versions and updated_imports is not None:\n        suite_import.version = s.version()\n        updated_imports.write(str(suite_import) + '\\n')\n\n    s.visit_imports(_spull_import_visitor, update_versions=update_versions)\n\ndef spull(args):\n    \"\"\"pull primary suite and all its imports\"\"\"\n    parser = ArgumentParser(prog='mx spull')\n    parser.add_argument('--update-versions', action='store_true', help='update version ids of imported suites')\n    args = parser.parse_args(args)\n\n    _hg.check()\n    _spull(_check_primary_suite(), None, args.update_versions, None)\n\ndef _sincoming_import_visitor(s, suite_import, **extra_args):\n    _sincoming(suite(suite_import.name), suite_import)\n\ndef _sincoming(s, suite_import):\n    s.visit_imports(_sincoming_import_visitor)\n\n    run(['hg', '-R', s.dir, 'incoming'], nonZeroIsFatal=False)\n\ndef sincoming(args):\n    '''check incoming for primary suite and all imports'''\n    parser = ArgumentParser(prog='mx sincoming')\n    args = parser.parse_args(args)\n    _hg.check()\n    s = _check_primary_suite()\n\n    _sincoming(s, None)\n\ndef _stip_import_visitor(s, suite_import, **extra_args):\n    _stip(suite(suite_import.name), suite_import)\n\ndef _stip(s, suite_import):\n    s.visit_imports(_stip_import_visitor)\n\n    print 'tip of %s' % s.name\n    run(['hg', '-R', s.dir, 'tip'], nonZeroIsFatal=False)\n\ndef stip(args):\n    '''check tip for primary suite and all imports'''\n    parser = ArgumentParser(prog='mx stip')\n    args = parser.parse_args(args)\n    _hg.check()\n    s = _check_primary_suite()\n\n    _stip(s, None)\n\ndef findclass(args, logToConsole=True, matcher=lambda string, classname: string in classname):\n    \"\"\"find all classes matching a given substring\"\"\"\n    matches = []\n    for entry, filename in classpath_walk(includeBootClasspath=True):\n        if filename.endswith('.class'):\n            if isinstance(entry, zipfile.ZipFile):\n                classname = filename.replace('/', '.')\n            else:\n                classname = filename.replace(os.sep, '.')\n            classname = classname[:-len('.class')]\n            for a in args:\n                if matcher(a, classname):\n                    matches.append(classname)\n                    if logToConsole:\n                        log(classname)\n    return matches\n\ndef select_items(items, descriptions=None, allowMultiple=True):\n    \"\"\"\n    Presents a command line interface for selecting one or more (if allowMultiple is true) items.\n\n    \"\"\"\n    if len(items) <= 1:\n        return items\n    else:\n        if allowMultiple:\n            log('[0] <all>')\n        for i in range(0, len(items)):\n            if descriptions is None:\n                log('[{0}] {1}'.format(i + 1, items[i]))\n            else:\n                assert len(items) == len(descriptions)\n                wrapper = textwrap.TextWrapper(subsequent_indent='    ')\n                log('\\n'.join(wrapper.wrap('[{0}] {1} - {2}'.format(i + 1, items[i], descriptions[i]))))\n        while True:\n            if allowMultiple:\n                s = raw_input('Enter number(s) of selection (separate multiple choices with spaces): ').split()\n            else:\n                s = [raw_input('Enter number of selection: ')]\n            try:\n                s = [int(x) for x in s]\n            except:\n                log('Selection contains non-numeric characters: \"' + ' '.join(s) + '\"')\n                continue\n\n            if allowMultiple and 0 in s:\n                return items\n\n            indexes = []\n            for n in s:\n                if n not in range(1, len(items) + 1):\n                    log('Invalid selection: ' + str(n))\n                    continue\n                else:\n                    indexes.append(n - 1)\n            if allowMultiple:\n                return [items[i] for i in indexes]\n            if len(indexes) == 1:\n                return items[indexes[0]]\n            return None\n\ndef exportlibs(args):\n    \"\"\"export libraries to an archive file\"\"\"\n\n    parser = ArgumentParser(prog='exportlibs')\n    parser.add_argument('-b', '--base', action='store', help='base name of archive (default: libs)', default='libs', metavar='<path>')\n    parser.add_argument('-a', '--include-all', action='store_true', help=\"include all defined libaries\")\n    parser.add_argument('--arc', action='store', choices=['tgz', 'tbz2', 'tar', 'zip'], default='tgz', help='the type of the archive to create')\n    parser.add_argument('--no-sha1', action='store_false', dest='sha1', help='do not create SHA1 signature of archive')\n    parser.add_argument('--no-md5', action='store_false', dest='md5', help='do not create MD5 signature of archive')\n    parser.add_argument('--include-system-libs', action='store_true', help='include system libraries (i.e., those not downloaded from URLs)')\n    parser.add_argument('extras', nargs=REMAINDER, help='extra files and directories to add to archive', metavar='files...')\n    args = parser.parse_args(args)\n\n    def createArchive(addMethod):\n        entries = {}\n        def add(path, arcname):\n            apath = os.path.abspath(path)\n            if not entries.has_key(arcname):\n                entries[arcname] = apath\n                logv('[adding ' + path + ']')\n                addMethod(path, arcname=arcname)\n            elif entries[arcname] != apath:\n                logv('[warning: ' + apath + ' collides with ' + entries[arcname] + ' as ' + arcname + ']')\n            else:\n                logv('[already added ' + path + ']')\n\n        libsToExport = set()\n        if args.include_all:\n            for lib in _libs.itervalues():\n                libsToExport.add(lib)\n        else:\n            def isValidLibrary(dep):\n                if dep in _libs.iterkeys():\n                    lib = _libs[dep]\n                    if len(lib.urls) != 0 or args.include_system_libs:\n                        return lib\n                return None\n\n            # iterate over all project dependencies and find used libraries\n            for p in _projects.itervalues():\n                for dep in p.deps:\n                    r = isValidLibrary(dep)\n                    if r:\n                        libsToExport.add(r)\n\n            # a library can have other libraries as dependency\n            size = 0\n            while size != len(libsToExport):\n                size = len(libsToExport)\n                for lib in libsToExport.copy():\n                    for dep in lib.deps:\n                        r = isValidLibrary(dep)\n                        if r:\n                            libsToExport.add(r)\n\n        for lib in libsToExport:\n            add(lib.get_path(resolve=True), lib.path)\n            if lib.sha1:\n                add(lib.get_path(resolve=True) + \".sha1\", lib.path + \".sha1\")\n            if lib.sourcePath:\n                add(lib.get_source_path(resolve=True), lib.sourcePath)\n                if lib.sourceSha1:\n                    add(lib.get_source_path(resolve=True) + \".sha1\", lib.sourcePath + \".sha1\")\n\n        if args.extras:\n            for e in args.extras:\n                if os.path.isdir(e):\n                    for root, _, filenames in os.walk(e):\n                        for name in filenames:\n                            f = join(root, name)\n                            add(f, f)\n                else:\n                    add(e, e)\n\n    if args.arc == 'zip':\n        path = args.base + '.zip'\n        with zipfile.ZipFile(path, 'w') as zf:\n            createArchive(zf.write)\n    else:\n        path = args.base + '.tar'\n        mode = 'w'\n        if args.arc != 'tar':\n            sfx = args.arc[1:]\n            mode = mode + ':' + sfx\n            path = path + '.' + sfx\n        with tarfile.open(path, mode) as tar:\n            createArchive(tar.add)\n    log('created ' + path)\n\n    def digest(enabled, path, factory, suffix):\n        if enabled:\n            d = factory()\n            with open(path, 'rb') as f:\n                while True:\n                    buf = f.read(4096)\n                    if not buf:\n                        break\n                    d.update(buf)\n            with open(path + '.' + suffix, 'w') as fp:\n                print >> fp, d.hexdigest()\n            log('created ' + path + '.' + suffix)\n\n    digest(args.sha1, path, hashlib.sha1, 'sha1')\n    digest(args.md5, path, hashlib.md5, 'md5')\n\ndef javap(args):\n    \"\"\"disassemble classes matching given pattern with javap\"\"\"\n\n    javapExe = java().javap\n    if not exists(javapExe):\n        abort('The javap executable does not exists: ' + javapExe)\n    else:\n        candidates = findclass(args, logToConsole=False)\n        if len(candidates) == 0:\n            log('no matches')\n        selection = select_items(candidates)\n        run([javapExe, '-private', '-verbose', '-classpath', classpath()] + selection)\n\ndef show_projects(args):\n    \"\"\"show all loaded projects\"\"\"\n    for s in suites():\n        projectsFile = join(s.mxDir, 'projects')\n        if exists(projectsFile):\n            log(projectsFile)\n            for p in s.projects:\n                log('\\t' + p.name)\n\ndef _compile_mx_class(javaClassName, classpath=None):\n    myDir = dirname(__file__)\n    binDir = join(myDir, 'bin')\n    javaSource = join(myDir, javaClassName + '.java')\n    javaClass = join(binDir, javaClassName + '.class')\n    if not exists(javaClass) or getmtime(javaClass) < getmtime(javaSource):\n        if not exists(binDir):\n            os.mkdir(binDir)\n        # Pick the lowest Java compliance for compiling mx classes\n        lowestJavaConfig = _java_homes[len(_java_homes) - 1]\n        cmd = [java(lowestJavaConfig.javaCompliance).javac, '-d', binDir]\n        if classpath:\n            cmd += ['-cp', binDir + os.pathsep + classpath]\n        cmd += [javaSource]\n        try:\n            subprocess.check_call(cmd)\n        except subprocess.CalledProcessError:\n            abort('failed to compile:' + javaSource)\n\n    return (myDir, binDir)\n\ndef checkcopyrights(args):\n    '''run copyright check on the sources'''\n    class CP(ArgumentParser):\n        def format_help(self):\n            return ArgumentParser.format_help(self) + self._get_program_help()\n\n        def _get_program_help(self):\n            help_output = subprocess.check_output([java().java, '-cp', binDir, 'CheckCopyright', '--help'])\n            return '\\nother argumemnts preceded with --\\n' +  help_output\n\n    myDir, binDir = _compile_mx_class('CheckCopyright')\n\n    parser = CP(prog='mx checkcopyrights')\n\n    parser.add_argument('--primary', action='store_true', help='limit checks to primary suite')\n    parser.add_argument('remainder', nargs=REMAINDER, metavar='...')\n    args = parser.parse_args(args)\n    remove_doubledash(args.remainder)\n\n\n    # ensure compiled form of code is up to date\n\n    result = 0\n    # copyright checking is suite specific as each suite may have different overrides\n    for s in suites(True):\n        if args.primary and not s.primary:\n            continue\n        custom_copyrights = join(s.mxDir, 'copyrights')\n        custom_args = []\n        if exists(custom_copyrights):\n            custom_args = ['--custom-copyright-dir', custom_copyrights]\n        rc = run([java().java, '-cp', binDir, 'CheckCopyright', '--copyright-dir', myDir] + custom_args + args.remainder, cwd=s.dir, nonZeroIsFatal=False)\n        result = result if rc == 0 else rc\n    return result\n\ndef _find_classes_with_annotations(p, pkgRoot, annotations, includeInnerClasses=False):\n    \"\"\"\n    Scan the sources of project 'p' for Java source files containing a line starting with 'annotation'\n    (ignoring preceding whitespace) and return the fully qualified class name for each Java\n    source file matched in a list.\n    \"\"\"\n\n    matches = lambda line: len([a for a in annotations if line == a or line.startswith(a + '(')]) != 0\n    return p.find_classes_with_matching_source_line(pkgRoot, matches, includeInnerClasses)\n\ndef _basic_junit_harness(args, vmArgs, junitArgs):\n    return run_java(junitArgs)\n\ndef junit(args, harness=_basic_junit_harness, parser=None):\n    '''run Junit tests'''\n    suppliedParser = parser is not None\n    parser = parser if suppliedParser else ArgumentParser(prog='mx junit')\n    parser.add_argument('--tests', action='store', help='pattern to match test classes')\n    parser.add_argument('--J', dest='vm_args', help='target VM arguments (e.g. --J @-dsa)', metavar='@<args>')\n    if suppliedParser:\n        parser.add_argument('remainder', nargs=REMAINDER, metavar='...')\n    args = parser.parse_args(args)\n\n    vmArgs = ['-ea', '-esa']\n\n    if args.vm_args:\n        vmArgs = vmArgs + shlex.split(args.vm_args.lstrip('@'))\n\n    testfile = os.environ.get('MX_TESTFILE', None)\n    if testfile is None:\n        (_, testfile) = tempfile.mkstemp(\".testclasses\", \"mx\")\n        os.close(_)\n\n    candidates = []\n    for p in projects_opt_limit_to_suites():\n        if p.native or java().javaCompliance < p.javaCompliance:\n            continue\n        candidates += _find_classes_with_annotations(p, None, ['@Test']).keys()\n\n    tests = [] if args.tests is None else [name for name in args.tests.split(',')]\n    classes = []\n    if len(tests) == 0:\n        classes = candidates\n    else:\n        for t in tests:\n            found = False\n            for c in candidates:\n                if t in c:\n                    found = True\n                    classes.append(c)\n            if not found:\n                log('warning: no tests matched by substring \"' + t)\n\n    projectscp = classpath([pcp.name for pcp in projects_opt_limit_to_suites() if not pcp.native and pcp.javaCompliance <= java().javaCompliance])\n\n    if len(classes) != 0:\n        # Compiling wrt projectscp avoids a dependency on junit.jar in mxtool itself\n        # However, perhaps because it's Friday 13th javac is not actually compiling\n        # this file, yet not returning error. It is perhaps related to annotation processors\n        # so the workaround is to extract the junit path as that is all we need.\n        junitpath = [s for s in projectscp.split(\":\") if \"junit\" in s][0]\n\n        _, binDir = _compile_mx_class('MX2JUnitWrapper', junitpath)\n\n        if len(classes) == 1:\n            testClassArgs = ['--testclass', classes[0]]\n        else:\n            with open(testfile, 'w') as f:\n                for c in classes:\n                    f.write(c + '\\n')\n            testClassArgs = ['--testsfile', testfile]\n        junitArgs = ['-cp', binDir + os.pathsep + projectscp, 'MX2JUnitWrapper'] + testClassArgs\n        rc = harness(args, vmArgs, junitArgs)\n        return rc\n    else:\n        return 0\n\ndef remove_doubledash(args):\n    if '--' in args:\n        args.remove('--')\n\ndef ask_yes_no(question, default=None):\n    \"\"\"\"\"\"\n    assert not default or default == 'y' or default == 'n'\n    if not sys.stdout.isatty():\n        if default:\n            return default\n        else:\n            abort(\"Can not answer '\" + question + \"?' if stdout is not a tty\")\n    questionMark = '? [yn]: '\n    if default:\n        questionMark = questionMark.replace(default, default.upper())\n    answer = raw_input(question + questionMark) or default\n    while not answer:\n        answer = raw_input(question + questionMark)\n    return answer.lower().startswith('y')\n\ndef add_argument(*args, **kwargs):\n    \"\"\"\n    Define how a single command-line argument.\n    \"\"\"\n    assert _argParser is not None\n    _argParser.add_argument(*args, **kwargs)\n\ndef update_commands(suite, new_commands):\n    for key, value in new_commands.iteritems():\n        if _commands.has_key(key):\n            warn(\"redefining command '\" + key + \"' in suite \" + suite.name)\n        _commands[key] = value\n\ndef command_function(name, fatalIfMissing=True):\n    '''\n    Return the function for the (possibly overridden) command named name.\n    If no such command, abort if FatalIsMissing=True, else return None\n    '''\n    if _commands.has_key(name):\n        return _commands[name][0]\n    else:\n        if fatalIfMissing:\n            abort('command ' + name + ' does not exist')\n        else:\n            return None\n\ndef warn(msg):\n    if _warn:\n        print 'WARNING: ' + msg\n\n# Table of commands in alphabetical order.\n# Keys are command names, value are lists: [<function>, <usage msg>, <format args to doc string of function>...]\n# If any of the format args are instances of Callable, then they are called with an 'env' are before being\n# used in the call to str.format().\n# Suite extensions should not update this table directly, but use update_commands\n_commands = {\n    'about': [about, ''],\n    'bench': [bench, ''],\n    'build': [build, '[options]'],\n    'checkstyle': [checkstyle, ''],\n    'canonicalizeprojects': [canonicalizeprojects, ''],\n    'clean': [clean, ''],\n    'checkcopyrights': [checkcopyrights, '[options]'],\n    'createsuite': [createsuite, '[options]'],\n    'eclipseinit': [eclipseinit, ''],\n    'eclipseformat': [eclipseformat, ''],\n    'exportlibs': [exportlibs, ''],\n    'findclass': [findclass, ''],\n    'fsckprojects': [fsckprojects, ''],\n    'gate': [gate, '[options]'],\n    'help': [help_, '[command]'],\n    'ideclean': [ideclean, ''],\n    'ideinit': [ideinit, ''],\n    'intellijinit': [intellijinit, ''],\n    'archive': [_archive, '[options]'],\n    'projectgraph': [projectgraph, ''],\n    'sclone': [sclone, '[options]'],\n    'scheckimports': [scheckimports, '[options]'],\n    'scloneimports': [scloneimports, '[options]'],\n    'sforceimports': [sforceimports, ''],\n    'sincoming': [sincoming, ''],\n    'spull': [spull, '[options]'],\n    'spush': [spush, '[options]'],\n    'stip': [stip, ''],\n    'supdate': [supdate, ''],\n    'pylint': [pylint, ''],\n    'javap': [javap, '<class name patterns>'],\n    'javadoc': [javadoc, '[options]'],\n    'junit': [junit, '[options]'],\n    'site': [site, '[options]'],\n    'netbeansinit': [netbeansinit, ''],\n    'projects': [show_projects, ''],\n}\n\n_argParser = ArgParser()\n\ndef _suitename(mxDir):\n    base = os.path.basename(mxDir)\n    parts = base.split('.')\n    # temporary workaround until mx.graal exists\n    if len(parts) == 1:\n        return 'graal'\n    else:\n        return parts[1]\n\ndef _is_suite_dir(d, mxDirName=None):\n    \"\"\"\n    Checks if d contains a suite.\n    If mxDirName is None, matches any suite name, otherwise checks for exactly that suite.\n    \"\"\"\n    if os.path.isdir(d):\n        for f in os.listdir(d):\n            if (mxDirName == None and (f == 'mx' or fnmatch.fnmatch(f, 'mx.*'))) or f == mxDirName:\n                mxDir = join(d, f)\n                if exists(mxDir) and isdir(mxDir) and exists(join(mxDir, 'projects')):\n                    return mxDir\n\ndef _check_primary_suite():\n    if _primary_suite is None:\n        abort('no primary suite found')\n    else:\n        return _primary_suite\n\nNeeds_primary_suite_exemptions = ['sclone', 'scloneimports', 'createsuite']\n\ndef _needs_primary_suite(command):\n    return not command in Needs_primary_suite_exemptions\n\ndef _needs_primary_suite_cl():\n    args = sys.argv[1:]\n    if len(args) == 0:\n        return False\n    for s in args:\n        if s in Needs_primary_suite_exemptions:\n            return False\n    return True\n\ndef _findPrimarySuiteMxDirFrom(d):\n    \"\"\" search for a suite directory upwards from 'd' \"\"\"\n    while d:\n        mxDir = _is_suite_dir(d)\n        if mxDir is not None:\n            return mxDir\n        parent = dirname(d)\n        if d == parent:\n            return None\n        d = parent\n\n    return None\n\ndef _findPrimarySuiteMxDir():\n    # check for explicit setting\n    if _primary_suite_path is not None:\n        mxDir = _is_suite_dir(_primary_suite_path)\n        if mxDir is not None:\n            return mxDir\n        else:\n            abort(_primary_suite_path + ' does not contain an mx suite')\n\n    # try current working directory first\n    mxDir = _findPrimarySuiteMxDirFrom(os.getcwd())\n    if mxDir is not None:\n        return mxDir\n    # backwards compatibility: search from path of this file\n    return _findPrimarySuiteMxDirFrom(dirname(__file__))\n\ndef _remove_bad_deps():\n    '''Remove projects and libraries that (recursively) depend on an optional library\n    whose artifact does not exist or on a JRE library that is not present in the\n    JDK for a project. Also remove projects whose Java compliance requirement\n    cannot be satisfied by the configured JDKs.\n    Removed projects and libraries are also removed from\n    distributions in they are listed as dependencies.'''\n    for d in sorted_deps(includeLibs=True):\n        if d.isLibrary():\n            if d.optional:\n                try:\n                    d.optional = False\n                    path = d.get_path(resolve=True)\n                except SystemExit:\n                    path = None\n                finally:\n                    d.optional = True\n                if not path:\n                    logv('[omitting optional library {} as {} does not exist]'.format(d, d.path))\n                    del _libs[d.name]\n                    d.suite.libs.remove(d)\n        elif d.isProject():\n            if java(d.javaCompliance) is None:\n                logv('[omitting project {} as Java compliance {} cannot be satisfied by configured JDKs]'.format(d, d.javaCompliance))\n                del _projects[d.name]\n                d.suite.projects.remove(d)\n            else:\n                for name in list(d.deps):\n                    jreLib = _jreLibs.get(name)\n                    if jreLib:\n                        if not jreLib.is_present_in_jdk(java(d.javaCompliance)):\n                            if jreLib.optional:\n                                logv('[omitting project {} as dependency {} is missing]'.format(d, name))\n                                del _projects[d.name]\n                                d.suite.projects.remove(d)\n                            else:\n                                abort('JRE library {} required by {} not found'.format(jreLib, d))\n                    elif not dependency(name, fatalIfMissing=False):\n                        logv('[omitting project {} as dependency {} is missing]'.format(d, name))\n                        del _projects[d.name]\n                        d.suite.projects.remove(d)\n\n    for dist in _dists.values():\n        for name in list(dist.deps):\n            if not dependency(name, fatalIfMissing=False):\n                logv('[omitting {} from distribution {}]'.format(name, dist))\n                dist.deps.remove(name)\n\ndef main():\n    SuiteModel.parse_options()\n\n    global _hg\n    _hg = HgConfig()\n\n    primary_suite_error = 'no primary suite found'\n    primarySuiteMxDir = _findPrimarySuiteMxDir()\n    if primarySuiteMxDir:\n        _src_suitemodel.set_primary_dir(dirname(primarySuiteMxDir))\n        global _primary_suite\n        _primary_suite = Suite(primarySuiteMxDir, True)\n    else:\n        # in general this is an error, except for the Needs_primary_suite_exemptions commands,\n        # and an extensions command will likely not parse in this case, as any extra arguments\n        # will not have been added to _argParser.\n        # If the command line does not contain a string matching one of the exemptions, we can safely abort,\n        # but not otherwise, as we can't be sure the string isn't in a value for some other option.\n        if _needs_primary_suite_cl():\n            abort(primary_suite_error)\n\n    opts, commandAndArgs = _argParser._parse_cmd_line()\n\n    if primarySuiteMxDir is None:\n        if len(commandAndArgs) > 0 and _needs_primary_suite(commandAndArgs[0]):\n            abort(primary_suite_error)\n        else:\n            warn(primary_suite_error)\n\n    global _opts, _java_homes\n    _opts = opts\n    defaultJdk = JavaConfig(opts.java_home, opts.java_dbg_port)\n    _java_homes = [defaultJdk]\n    if opts.extra_java_homes:\n        for java_home in opts.extra_java_homes.split(os.pathsep):\n            extraJdk = JavaConfig(java_home, opts.java_dbg_port)\n            if extraJdk > defaultJdk:\n                abort('Secondary JDK ' + extraJdk.jdk + ' has higher compliance level than default JDK ' + defaultJdk.jdk)\n            _java_homes.append(extraJdk)\n\n    if primarySuiteMxDir:\n        _primary_suite._depth_first_post_init()\n\n    _remove_bad_deps()\n\n    command = commandAndArgs[0]\n    command_args = commandAndArgs[1:]\n\n    if not _commands.has_key(command):\n        hits = [c for c in _commands.iterkeys() if c.startswith(command)]\n        if len(hits) == 1:\n            command = hits[0]\n        elif len(hits) == 0:\n            abort('mx: unknown command \\'{0}\\'\\n{1}use \"mx help\" for more options'.format(command, _format_commands()))\n        else:\n            abort('mx: command \\'{0}\\' is ambiguous\\n    {1}'.format(command, ' '.join(hits)))\n\n    c, _ = _commands[command][:2]\n    def term_handler(signum, frame):\n        abort(1)\n    signal.signal(signal.SIGTERM, term_handler)\n\n    def quit_handler(signum, frame):\n        _send_sigquit()\n    if get_os() != 'windows':\n        signal.signal(signal.SIGQUIT, quit_handler)\n\n    try:\n        if opts.timeout != 0:\n            def alarm_handler(signum, frame):\n                abort('Command timed out after ' + str(opts.timeout) + ' seconds: ' + ' '.join(commandAndArgs))\n            signal.signal(signal.SIGALRM, alarm_handler)\n            signal.alarm(opts.timeout)\n        retcode = c(command_args)\n        if retcode is not None and retcode != 0:\n            abort(retcode)\n    except KeyboardInterrupt:\n        # no need to show the stack trace when the user presses CTRL-C\n        abort(1)\n\nversion = VersionSpec(\"2.3.6\")\n\ncurrentUmask = None\n\nif __name__ == '__main__':\n    # rename this module as 'mx' so it is not imported twice by the commands.py modules\n    sys.modules['mx'] = sys.modules.pop('__main__')\n\n    # Capture the current umask since there's no way to query it without mutating it.\n    currentUmask = os.umask(0)\n    os.umask(currentUmask)\n\n    main()\n","markers":{"markers":{"1":{"id":1,"range":[[0,0],[0,0]],"tailed":false,"reversed":false,"valid":true,"invalidate":"never","persistent":true,"properties":{"type":"selection","editorId":22,"goalBufferRange":null},"deserializer":"Marker"}},"deserializer":"MarkerManager"},"history":{"undoStack":[{"patches":[{"oldRange":[[109,0],[109,0]],"newRange":[[109,0],[110,0]],"oldText":"","newText":"                        logv('warning: ' + self.path + ': avoid overwrite of ' + arcname + '\\n  new: ' + source + '\\n  old: ' + existingSource)\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[110,0],[111,0]],"newRange":[[110,0],[110,0]],"oldText":"                        log('warning: ' + self.path + ': avoid overwrite of ' + arcname + '\\n  new: ' + source + '\\n  old: ' + existingSource)\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[1226,0],[1226,0]],"newRange":[[1226,0],[1227,0]],"oldText":"","newText":"\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[1228,0],[1228,0]],"newRange":[[1228,0],[1232,0]],"oldText":"","newText":"                if extra_args[\"dynamicImport\"]:\n                    return None\n                else:\n                    abort('import ' + suite_import.name + ' not found')\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[1232,0],[1233,0]],"newRange":[[1232,0],[1232,0]],"oldText":"                abort('import ' + suite_import.name + ' not found')\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[1238,0],[1238,0]],"newRange":[[1238,0],[1239,0]],"oldText":"","newText":"        \"\"\"Dynamic import of a suite. Returns None if the suite cannot be found\"\"\"\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[1239,0],[1240,0]],"newRange":[[1239,0],[1239,0]],"oldText":"        \"\"\"dynamic import of a suite\"\"\"\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[1240,0],[1240,0]],"newRange":[[1240,0],[1242,0]],"oldText":"","newText":"        imported_suite = Suite._find_and_loadsuite(self, suite_import, dynamicImport=True)\n        if imported_suite and not imported_suite.post_init:\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[1242,0],[1244,0]],"newRange":[[1242,0],[1242,0]],"oldText":"        imported_suite = Suite._find_and_loadsuite(self, suite_import)\n        if not imported_suite.post_init:\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[1729,0],[1729,0]],"newRange":[[1729,0],[1730,0]],"oldText":"","newText":"    Get the class path for a list of given dependencies and distributions, resolving each entry in the\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[1730,0],[1731,0]],"newRange":[[1730,0],[1730,0]],"oldText":"    Get the class path for a list of given dependencies, resolving each entry in the\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[1733,0],[1733,0]],"newRange":[[1733,0],[1735,0]],"oldText":"","newText":"        deps = sorted_deps(includeLibs=True)\n        dists = list(_dists.values())\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[1735,0],[1736,0]],"newRange":[[1735,0],[1735,0]],"oldText":"        result = _as_classpath(sorted_deps(includeLibs=True), resolve)\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[1737,0],[1737,0]],"newRange":[[1737,0],[1738,0]],"oldText":"","newText":"        dists = []\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[1741,0],[1741,0]],"newRange":[[1741,0],[1768,0]],"oldText":"","newText":"            dep = dependency(n, fatalIfMissing=False)\n            if dep:\n                dep.all_deps(deps, True, includeSelf)\n            else:\n                dist = distribution(n)\n                if not dist:\n                    abort('project, library or distribution named ' + n + ' not found')\n                dists.append(dist)\n\n    if len(dists):\n        distsDeps = set()\n        for d in dists:\n            distsDeps.update(d.sorted_deps())\n\n        # remove deps covered by a dist that will be on the class path\n        deps = [d for d in deps if d not in distsDeps]\n\n    result = _as_classpath(deps, resolve)\n\n    # prepend distributions\n    if len(dists):\n        distsCp = os.pathsep.join(dist.path for dist in dists)\n        if len(result):\n            result = distsCp + os.pathsep + result\n        else:\n            result = distsCp\n\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[1768,0],[1770,0]],"newRange":[[1768,0],[1768,0]],"oldText":"            dependency(n).all_deps(deps, True, includeSelf)\n        result = _as_classpath(deps, resolve)\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[2271,0],[2271,0]],"newRange":[[2271,0],[2283,0]],"oldText":"","newText":"        def _checkOutput(out):\n            return 'version' in out\n\n        # hotspot can print a warning, e.g. if there's a .hotspot_compiler file in the cwd\n        output = output.split('\\n')\n        version = None\n        for o in output:\n            if _checkOutput(o):\n                assert version is None\n                version = o\n\n        self.version = VersionSpec(version.split()[2].strip('\"'))\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[2283,0],[2286,0]],"newRange":[[2283,0],[2283,0]],"oldText":"        output = output.split()\n        assert output[1] == 'version'\n        self.version = VersionSpec(output[2].strip('\"'))\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[2290,0],[2290,0]],"newRange":[[2290,0],[2291,0]],"oldText":"","newText":"        self._bootclasspath, self._extdirs, self._endorseddirs = [x if x != 'null' else None for x in subprocess.check_output([self.java, '-cp', binDir, 'ClasspathDump'], stderr=subprocess.PIPE).split('|')]\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[2291,0],[2292,0]],"newRange":[[2291,0],[2291,0]],"oldText":"        self._bootclasspath, self._extdirs, self._endorseddirs = [x if x != 'null' else None for x in subprocess.check_output([self.java, '-cp', binDir, 'ClasspathDump']).split('|')]\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[2579,0],[2579,0]],"newRange":[[2579,0],[2580,0]],"oldText":"","newText":"                    _eclipseinit_project(self.proj)\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[2580,0],[2581,0]],"newRange":[[2580,0],[2580,0]],"oldText":"                    eclipseinit([], buildProcessorJars=False)\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3729,0],[3729,0]],"newRange":[[3729,0],[3731,0]],"oldText":"","newText":"def _eclipseinit_project(p, files=None, libFiles=None):\n    assert java(p.javaCompliance)\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3731,0],[3736,0]],"newRange":[[3731,0],[3731,0]],"oldText":"def _eclipseinit_suite(args, suite, buildProcessorJars=True, refreshOnly=False):\n    configZip = TimeStampFile(join(suite.mxDir, 'eclipse-config.zip'))\n    configLibsZip = join(suite.mxDir, 'eclipse-config-libs.zip')\n    if refreshOnly and not configZip.exists():\n        return\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3732,0],[3732,0]],"newRange":[[3732,0],[3734,0]],"oldText":"","newText":"    if not exists(p.dir):\n        os.makedirs(p.dir)\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3734,0],[3737,0]],"newRange":[[3734,0],[3734,0]],"oldText":"    if _check_ide_timestamp(suite, configZip, 'eclipse'):\n        logv('[Eclipse configurations are up to date - skipping]')\n        return\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3735,0],[3735,0]],"newRange":[[3735,0],[3737,0]],"oldText":"","newText":"    out = XMLDoc()\n    out.open('classpath')\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3737,0],[3741,0]],"newRange":[[3737,0],[3737,0]],"oldText":"    files = []\n    libFiles = []\n    if buildProcessorJars:\n        files += _processorjars_suite(suite)\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3738,0],[3738,0]],"newRange":[[3738,0],[3743,0]],"oldText":"","newText":"    for src in p.srcDirs:\n        srcDir = join(p.dir, src)\n        if not exists(srcDir):\n            os.mkdir(srcDir)\n        out.element('classpathentry', {'kind' : 'src', 'path' : src})\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3743,0],[3748,0]],"newRange":[[3743,0],[3743,0]],"oldText":"    projToDist = dict()\n    for dist in _dists.values():\n        distDeps = dist.sorted_deps()\n        for p in distDeps:\n            projToDist[p.name] = (dist, [dep.name for dep in distDeps])\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3744,0],[3744,0]],"newRange":[[3744,0],[3750,0]],"oldText":"","newText":"    if len(p.annotation_processors()) > 0:\n        genDir = p.source_gen_dir()\n        if not exists(genDir):\n            os.mkdir(genDir)\n        out.element('classpathentry', {'kind' : 'src', 'path' : 'src_gen'})\n        if files:\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3750,0],[3773,0]],"newRange":[[3750,0],[3750,0]],"oldText":"    for p in suite.projects:\n        if p.native:\n            continue\n\n        assert java(p.javaCompliance)\n\n        if not exists(p.dir):\n            os.makedirs(p.dir)\n\n        out = XMLDoc()\n        out.open('classpath')\n\n        for src in p.srcDirs:\n            srcDir = join(p.dir, src)\n            if not exists(srcDir):\n                os.mkdir(srcDir)\n            out.element('classpathentry', {'kind' : 'src', 'path' : src})\n\n        if len(p.annotation_processors()) > 0:\n            genDir = p.source_gen_dir()\n            if not exists(genDir):\n                os.mkdir(genDir)\n            out.element('classpathentry', {'kind' : 'src', 'path' : 'src_gen'})\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3752,0],[3752,0]],"newRange":[[3752,0],[3754,0]],"oldText":"","newText":"    # Every Java program depends on a JRE\n    out.element('classpathentry', {'kind' : 'con', 'path' : 'org.eclipse.jdt.launching.JRE_CONTAINER/org.eclipse.jdt.internal.debug.ui.launcher.StandardVMType/JavaSE-' + str(p.javaCompliance)})\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3754,0],[3756,0]],"newRange":[[3754,0],[3754,0]],"oldText":"        # Every Java program depends on a JRE\n        out.element('classpathentry', {'kind' : 'con', 'path' : 'org.eclipse.jdt.launching.JRE_CONTAINER/org.eclipse.jdt.internal.debug.ui.launcher.StandardVMType/JavaSE-' + str(p.javaCompliance)})\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3755,0],[3755,0]],"newRange":[[3755,0],[3757,0]],"oldText":"","newText":"    if exists(join(p.dir, 'plugin.xml')):  # eclipse plugin project\n        out.element('classpathentry', {'kind' : 'con', 'path' : 'org.eclipse.pde.core.requiredPlugins'})\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3757,0],[3759,0]],"newRange":[[3757,0],[3757,0]],"oldText":"        if exists(join(p.dir, 'plugin.xml')):  # eclipse plugin project\n            out.element('classpathentry', {'kind' : 'con', 'path' : 'org.eclipse.pde.core.requiredPlugins'})\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3758,0],[3758,0]],"newRange":[[3758,0],[3761,0]],"oldText":"","newText":"    containerDeps = set()\n    libraryDeps = set()\n    projectDeps = set()\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3761,0],[3764,0]],"newRange":[[3761,0],[3761,0]],"oldText":"        containerDeps = set()\n        libraryDeps = set()\n        projectDeps = set()\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3762,0],[3762,0]],"newRange":[[3762,0],[3774,0]],"oldText":"","newText":"    for dep in p.all_deps([], True):\n        if dep == p:\n            continue\n        if dep.isLibrary():\n            if hasattr(dep, 'eclipse.container'):\n                container = getattr(dep, 'eclipse.container')\n                containerDeps.add(container)\n                libraryDeps -= set(dep.all_deps([], True))\n            else:\n                libraryDeps.add(dep)\n        elif dep.isProject():\n            projectDeps.add(dep)\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3774,0],[3786,0]],"newRange":[[3774,0],[3774,0]],"oldText":"        for dep in p.all_deps([], True):\n            if dep == p:\n                continue\n            if dep.isLibrary():\n                if hasattr(dep, 'eclipse.container'):\n                    container = getattr(dep, 'eclipse.container')\n                    containerDeps.add(container)\n                    libraryDeps -= set(dep.all_deps([], True))\n                else:\n                    libraryDeps.add(dep)\n            elif dep.isProject():\n                projectDeps.add(dep)\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3775,0],[3775,0]],"newRange":[[3775,0],[3777,0]],"oldText":"","newText":"    for dep in containerDeps:\n        out.element('classpathentry', {'exported' : 'true', 'kind' : 'con', 'path' : dep})\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3777,0],[3779,0]],"newRange":[[3777,0],[3777,0]],"oldText":"        for dep in containerDeps:\n            out.element('classpathentry', {'exported' : 'true', 'kind' : 'con', 'path' : dep})\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3778,0],[3778,0]],"newRange":[[3778,0],[3781,0]],"oldText":"","newText":"    for dep in libraryDeps:\n        path = dep.path\n        dep.get_path(resolve=True)\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3781,0],[3784,0]],"newRange":[[3781,0],[3781,0]],"oldText":"        for dep in libraryDeps:\n            path = dep.path\n            dep.get_path(resolve=True)\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3782,0],[3782,0]],"newRange":[[3782,0],[3785,0]],"oldText":"","newText":"        # Relative paths for \"lib\" class path entries have various semantics depending on the Eclipse\n        # version being used (e.g. see https://bugs.eclipse.org/bugs/show_bug.cgi?id=274737) so it's\n        # safest to simply use absolute paths.\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3785,0],[3788,0]],"newRange":[[3785,0],[3785,0]],"oldText":"            # Relative paths for \"lib\" class path entries have various semantics depending on the Eclipse\n            # version being used (e.g. see https://bugs.eclipse.org/bugs/show_bug.cgi?id=274737) so it's\n            # safest to simply use absolute paths.\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3786,0],[3786,0]],"newRange":[[3786,0],[3789,0]],"oldText":"","newText":"        # It's important to use dep.suite as the location for when one suite references\n        # a library in another suite.\n        path = _make_absolute(path, dep.suite.dir)\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3789,0],[3792,0]],"newRange":[[3789,0],[3789,0]],"oldText":"            # It's important to use dep.suite as the location for when one suite references\n            # a library in another suite.\n            path = _make_absolute(path, dep.suite.dir)\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3790,0],[3790,0]],"newRange":[[3790,0],[3791,0]],"oldText":"","newText":"        attributes = {'exported' : 'true', 'kind' : 'lib', 'path' : path}\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3791,0],[3792,0]],"newRange":[[3791,0],[3791,0]],"oldText":"            attributes = {'exported' : 'true', 'kind' : 'lib', 'path' : path}\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3792,0],[3792,0]],"newRange":[[3792,0],[3797,0]],"oldText":"","newText":"        sourcePath = dep.get_source_path(resolve=True)\n        if sourcePath is not None:\n            attributes['sourcepath'] = sourcePath\n        out.element('classpathentry', attributes)\n        if libFiles:\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3797,0],[3801,0]],"newRange":[[3797,0],[3797,0]],"oldText":"            sourcePath = dep.get_source_path(resolve=True)\n            if sourcePath is not None:\n                attributes['sourcepath'] = sourcePath\n            out.element('classpathentry', attributes)\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3799,0],[3799,0]],"newRange":[[3799,0],[3801,0]],"oldText":"","newText":"    for dep in projectDeps:\n        out.element('classpathentry', {'combineaccessrules' : 'false', 'exported' : 'true', 'kind' : 'src', 'path' : '/' + dep.name})\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3801,0],[3803,0]],"newRange":[[3801,0],[3801,0]],"oldText":"        for dep in projectDeps:\n            out.element('classpathentry', {'combineaccessrules' : 'false', 'exported' : 'true', 'kind' : 'src', 'path' : '/' + dep.name})\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3802,0],[3802,0]],"newRange":[[3802,0],[3807,0]],"oldText":"","newText":"    out.element('classpathentry', {'kind' : 'output', 'path' : getattr(p, 'eclipse.output', 'bin')})\n    out.close('classpath')\n    classpathFile = join(p.dir, '.classpath')\n    update_file(classpathFile, out.xml(indent='\\t', newl='\\n'))\n    if files:\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3807,0],[3812,0]],"newRange":[[3807,0],[3807,0]],"oldText":"\n        out.element('classpathentry', {'kind' : 'output', 'path' : getattr(p, 'eclipse.output', 'bin')})\n        out.close('classpath')\n        classpathFile = join(p.dir, '.classpath')\n        update_file(classpathFile, out.xml(indent='\\t', newl='\\n'))\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3809,0],[3809,0]],"newRange":[[3809,0],[3812,0]],"oldText":"","newText":"    csConfig = join(project(p.checkstyleProj).dir, '.checkstyle_checks.xml')\n    if exists(csConfig):\n        out = XMLDoc()\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3812,0],[3815,0]],"newRange":[[3812,0],[3812,0]],"oldText":"        csConfig = join(project(p.checkstyleProj).dir, '.checkstyle_checks.xml')\n        if exists(csConfig):\n            out = XMLDoc()\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3813,0],[3813,0]],"newRange":[[3813,0],[3836,0]],"oldText":"","newText":"        dotCheckstyle = join(p.dir, \".checkstyle\")\n        checkstyleConfigPath = '/' + p.checkstyleProj + '/.checkstyle_checks.xml'\n        out.open('fileset-config', {'file-format-version' : '1.2.0', 'simple-config' : 'true'})\n        out.open('local-check-config', {'name' : 'Checks', 'location' : checkstyleConfigPath, 'type' : 'project', 'description' : ''})\n        out.element('additional-data', {'name' : 'protect-config-file', 'value' : 'false'})\n        out.close('local-check-config')\n        out.open('fileset', {'name' : 'all', 'enabled' : 'true', 'check-config-name' : 'Checks', 'local' : 'true'})\n        out.element('file-match-pattern', {'match-pattern' : '.', 'include-pattern' : 'true'})\n        out.close('fileset')\n        out.open('filter', {'name' : 'all', 'enabled' : 'true', 'check-config-name' : 'Checks', 'local' : 'true'})\n        out.element('filter-data', {'value' : 'java'})\n        out.close('filter')\n\n        exclude = join(p.dir, '.checkstyle.exclude')\n        if exists(exclude):\n            out.open('filter', {'name' : 'FilesFromPackage', 'enabled' : 'true'})\n            with open(exclude) as f:\n                for line in f:\n                    if not line.startswith('#'):\n                        line = line.strip()\n                        exclDir = join(p.dir, line)\n                        assert isdir(exclDir), 'excluded source directory listed in ' + exclude + ' does not exist or is not a directory: ' + exclDir\n                    out.element('filter-data', {'value' : line})\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3836,0],[3847,0]],"newRange":[[3836,0],[3836,0]],"oldText":"            dotCheckstyle = join(p.dir, \".checkstyle\")\n            checkstyleConfigPath = '/' + p.checkstyleProj + '/.checkstyle_checks.xml'\n            out.open('fileset-config', {'file-format-version' : '1.2.0', 'simple-config' : 'true'})\n            out.open('local-check-config', {'name' : 'Checks', 'location' : checkstyleConfigPath, 'type' : 'project', 'description' : ''})\n            out.element('additional-data', {'name' : 'protect-config-file', 'value' : 'false'})\n            out.close('local-check-config')\n            out.open('fileset', {'name' : 'all', 'enabled' : 'true', 'check-config-name' : 'Checks', 'local' : 'true'})\n            out.element('file-match-pattern', {'match-pattern' : '.', 'include-pattern' : 'true'})\n            out.close('fileset')\n            out.open('filter', {'name' : 'all', 'enabled' : 'true', 'check-config-name' : 'Checks', 'local' : 'true'})\n            out.element('filter-data', {'value' : 'java'})\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3838,0],[3838,0]],"newRange":[[3838,0],[3841,0]],"oldText":"","newText":"        out.close('fileset-config')\n        update_file(dotCheckstyle, out.xml(indent='  ', newl='\\n'))\n        if files:\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3841,0],[3855,0]],"newRange":[[3841,0],[3841,0]],"oldText":"            exclude = join(p.dir, '.checkstyle.exclude')\n            if exists(exclude):\n                out.open('filter', {'name' : 'FilesFromPackage', 'enabled' : 'true'})\n                with open(exclude) as f:\n                    for line in f:\n                        if not line.startswith('#'):\n                            line = line.strip()\n                            exclDir = join(p.dir, line)\n                            assert isdir(exclDir), 'excluded source directory listed in ' + exclude + ' does not exist or is not a directory: ' + exclDir\n                        out.element('filter-data', {'value' : line})\n                out.close('filter')\n\n            out.close('fileset-config')\n            update_file(dotCheckstyle, out.xml(indent='  ', newl='\\n'))\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3842,0],[3842,0]],"newRange":[[3842,0],[3847,0]],"oldText":"","newText":"    else:\n        # clean up existing .checkstyle file\n        dotCheckstyle = join(p.dir, \".checkstyle\")\n        if exists(dotCheckstyle):\n            os.unlink(dotCheckstyle)\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3847,0],[3852,0]],"newRange":[[3847,0],[3847,0]],"oldText":"        else:\n            # clean up existing .checkstyle file\n            dotCheckstyle = join(p.dir, \".checkstyle\")\n            if exists(dotCheckstyle):\n                os.unlink(dotCheckstyle)\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3848,0],[3848,0]],"newRange":[[3848,0],[3859,0]],"oldText":"","newText":"    out = XMLDoc()\n    out.open('projectDescription')\n    out.element('name', data=p.name)\n    out.element('comment', data='')\n    out.element('projects', data='')\n    out.open('buildSpec')\n    out.open('buildCommand')\n    out.element('name', data='org.eclipse.jdt.core.javabuilder')\n    out.element('arguments', data='')\n    out.close('buildCommand')\n    if exists(csConfig):\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3859,0],[3865,0]],"newRange":[[3859,0],[3859,0]],"oldText":"        out = XMLDoc()\n        out.open('projectDescription')\n        out.element('name', data=p.name)\n        out.element('comment', data='')\n        out.element('projects', data='')\n        out.open('buildSpec')\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3860,0],[3860,0]],"newRange":[[3860,0],[3861,0]],"oldText":"","newText":"        out.element('name', data='net.sf.eclipsecs.core.CheckstyleBuilder')\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3861,0],[3862,0]],"newRange":[[3861,0],[3861,0]],"oldText":"        out.element('name', data='org.eclipse.jdt.core.javabuilder')\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3863,0],[3863,0]],"newRange":[[3863,0],[3865,0]],"oldText":"","newText":"    if exists(join(p.dir, 'plugin.xml')):  # eclipse plugin project\n        for buildCommand in ['org.eclipse.pde.ManifestBuilder', 'org.eclipse.pde.SchemaBuilder']:\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3865,0],[3866,0]],"newRange":[[3865,0],[3865,0]],"oldText":"        if exists(csConfig):\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3866,0],[3866,0]],"newRange":[[3866,0],[3867,0]],"oldText":"","newText":"            out.element('name', data=buildCommand)\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3867,0],[3868,0]],"newRange":[[3867,0],[3867,0]],"oldText":"            out.element('name', data='net.sf.eclipsecs.core.CheckstyleBuilder')\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3869,0],[3875,0]],"newRange":[[3869,0],[3869,0]],"oldText":"        if exists(join(p.dir, 'plugin.xml')):  # eclipse plugin project\n            for buildCommand in ['org.eclipse.pde.ManifestBuilder', 'org.eclipse.pde.SchemaBuilder']:\n                out.open('buildCommand')\n                out.element('name', data=buildCommand)\n                out.element('arguments', data='')\n                out.close('buildCommand')\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3870,0],[3870,0]],"newRange":[[3870,0],[3874,0]],"oldText":"","newText":"    # The path should always be p.name/dir. independent of where the workspace actually is.\n    # So we use the parent folder of the project, whatever that is, to generate such a relative path.\n    logicalWorkspaceRoot = os.path.dirname(p.dir)\n    binFolder = os.path.relpath(p.output_dir(), logicalWorkspaceRoot)\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3874,0],[3878,0]],"newRange":[[3874,0],[3874,0]],"oldText":"        # The path should always be p.name/dir. independent of where the workspace actually is.\n        # So we use the parent folder of the project, whatever that is, to generate such a relative path.\n        logicalWorkspaceRoot = os.path.dirname(p.dir)\n        binFolder = os.path.relpath(p.output_dir(), logicalWorkspaceRoot)\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3875,0],[3875,0]],"newRange":[[3875,0],[3878,0]],"oldText":"","newText":"    if _isAnnotationProcessorDependency(p):\n        refreshFile = os.path.relpath(join(p.dir, p.name + '.jar'), logicalWorkspaceRoot)\n        _genEclipseBuilder(out, p, 'Jar', 'archive ' + p.name, refresh=True, refreshFile=refreshFile, relevantResources=[binFolder], async=True, xmlIndent='', xmlStandalone='no')\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3878,0],[3881,0]],"newRange":[[3878,0],[3878,0]],"oldText":"        if _isAnnotationProcessorDependency(p):\n            refreshFile = os.path.relpath(join(p.dir, p.name + '.jar'), logicalWorkspaceRoot)\n            _genEclipseBuilder(out, p, 'Jar', 'archive ' + p.name, refresh=True, refreshFile=refreshFile, relevantResources=[binFolder], async=True, xmlIndent='', xmlStandalone='no')\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3879,0],[3879,0]],"newRange":[[3879,0],[3891,0]],"oldText":"","newText":"    out.close('buildSpec')\n    out.open('natures')\n    out.element('nature', data='org.eclipse.jdt.core.javanature')\n    if exists(csConfig):\n        out.element('nature', data='net.sf.eclipsecs.core.CheckstyleNature')\n    if exists(join(p.dir, 'plugin.xml')):  # eclipse plugin project\n        out.element('nature', data='org.eclipse.pde.PluginNature')\n    out.close('natures')\n    out.close('projectDescription')\n    projectFile = join(p.dir, '.project')\n    update_file(projectFile, out.xml(indent='\\t', newl='\\n'))\n    if files:\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3891,0],[3902,0]],"newRange":[[3891,0],[3891,0]],"oldText":"        out.close('buildSpec')\n        out.open('natures')\n        out.element('nature', data='org.eclipse.jdt.core.javanature')\n        if exists(csConfig):\n            out.element('nature', data='net.sf.eclipsecs.core.CheckstyleNature')\n        if exists(join(p.dir, 'plugin.xml')):  # eclipse plugin project\n            out.element('nature', data='org.eclipse.pde.PluginNature')\n        out.close('natures')\n        out.close('projectDescription')\n        projectFile = join(p.dir, '.project')\n        update_file(projectFile, out.xml(indent='\\t', newl='\\n'))\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3893,0],[3893,0]],"newRange":[[3893,0],[3896,0]],"oldText":"","newText":"    settingsDir = join(p.dir, \".settings\")\n    if not exists(settingsDir):\n        os.mkdir(settingsDir)\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3896,0],[3899,0]],"newRange":[[3896,0],[3896,0]],"oldText":"        settingsDir = join(p.dir, \".settings\")\n        if not exists(settingsDir):\n            os.mkdir(settingsDir)\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3897,0],[3897,0]],"newRange":[[3897,0],[3904,0]],"oldText":"","newText":"    # collect the defaults from mxtool\n    defaultEclipseSettingsDir = join(dirname(__file__), 'eclipse-settings')\n    esdict = {}\n    if exists(defaultEclipseSettingsDir):\n        for name in os.listdir(defaultEclipseSettingsDir):\n            if isfile(join(defaultEclipseSettingsDir, name)):\n                esdict[name] = os.path.abspath(join(defaultEclipseSettingsDir, name))\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3904,0],[3911,0]],"newRange":[[3904,0],[3904,0]],"oldText":"        # collect the defaults from mxtool\n        defaultEclipseSettingsDir = join(dirname(__file__), 'eclipse-settings')\n        esdict = {}\n        if exists(defaultEclipseSettingsDir):\n            for name in os.listdir(defaultEclipseSettingsDir):\n                if isfile(join(defaultEclipseSettingsDir, name)):\n                    esdict[name] = os.path.abspath(join(defaultEclipseSettingsDir, name))\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3905,0],[3905,0]],"newRange":[[3905,0],[3911,0]],"oldText":"","newText":"    # check for suite overrides\n    eclipseSettingsDir = join(p.suite.mxDir, 'eclipse-settings')\n    if exists(eclipseSettingsDir):\n        for name in os.listdir(eclipseSettingsDir):\n            if isfile(join(eclipseSettingsDir, name)):\n                esdict[name] = os.path.abspath(join(eclipseSettingsDir, name))\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3911,0],[3917,0]],"newRange":[[3911,0],[3911,0]],"oldText":"        # check for suite overrides\n        eclipseSettingsDir = join(p.suite.mxDir, 'eclipse-settings')\n        if exists(eclipseSettingsDir):\n            for name in os.listdir(eclipseSettingsDir):\n                if isfile(join(eclipseSettingsDir, name)):\n                    esdict[name] = os.path.abspath(join(eclipseSettingsDir, name))\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3912,0],[3912,0]],"newRange":[[3912,0],[3918,0]],"oldText":"","newText":"    # check for project overrides\n    projectSettingsDir = join(p.dir, 'eclipse-settings')\n    if exists(projectSettingsDir):\n        for name in os.listdir(projectSettingsDir):\n            if isfile(join(projectSettingsDir, name)):\n                esdict[name] = os.path.abspath(join(projectSettingsDir, name))\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3918,0],[3924,0]],"newRange":[[3918,0],[3918,0]],"oldText":"        # check for project overrides\n        projectSettingsDir = join(p.dir, 'eclipse-settings')\n        if exists(projectSettingsDir):\n            for name in os.listdir(projectSettingsDir):\n                if isfile(join(projectSettingsDir, name)):\n                    esdict[name] = os.path.abspath(join(projectSettingsDir, name))\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3919,0],[3919,0]],"newRange":[[3919,0],[3924,0]],"oldText":"","newText":"    # copy a possibly modified file to the project's .settings directory\n    for name, path in esdict.iteritems():\n        # ignore this file altogether if this project has no annotation processors\n        if name == \"org.eclipse.jdt.apt.core.prefs\" and not len(p.annotation_processors()) > 0:\n            continue\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3924,0],[3929,0]],"newRange":[[3924,0],[3924,0]],"oldText":"        # copy a possibly modified file to the project's .settings directory\n        for name, path in esdict.iteritems():\n            # ignore this file altogether if this project has no annotation processors\n            if name == \"org.eclipse.jdt.apt.core.prefs\" and not len(p.annotation_processors()) > 0:\n                continue\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3925,0],[3925,0]],"newRange":[[3925,0],[3932,0]],"oldText":"","newText":"        with open(path) as f:\n            content = f.read()\n        content = content.replace('${javaCompliance}', str(p.javaCompliance))\n        if len(p.annotation_processors()) > 0:\n            content = content.replace('org.eclipse.jdt.core.compiler.processAnnotations=disabled', 'org.eclipse.jdt.core.compiler.processAnnotations=enabled')\n        update_file(join(settingsDir, name), content)\n        if files:\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3932,0],[3938,0]],"newRange":[[3932,0],[3932,0]],"oldText":"            with open(path) as f:\n                content = f.read()\n            content = content.replace('${javaCompliance}', str(p.javaCompliance))\n            if len(p.annotation_processors()) > 0:\n                content = content.replace('org.eclipse.jdt.core.compiler.processAnnotations=disabled', 'org.eclipse.jdt.core.compiler.processAnnotations=enabled')\n            update_file(join(settingsDir, name), content)\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3934,0],[3934,0]],"newRange":[[3934,0],[3947,0]],"oldText":"","newText":"    if len(p.annotation_processors()) > 0:\n        out = XMLDoc()\n        out.open('factorypath')\n        out.element('factorypathentry', {'kind' : 'PLUGIN', 'id' : 'org.eclipse.jst.ws.annotations.core', 'enabled' : 'true', 'runInBatchMode' : 'false'})\n        for ap in p.annotation_processors():\n            for dep in dependency(ap).all_deps([], True):\n                if dep.isLibrary():\n                    # Relative paths for \"lib\" class path entries have various semantics depending on the Eclipse\n                    # version being used (e.g. see https://bugs.eclipse.org/bugs/show_bug.cgi?id=274737) so it's\n                    # safest to simply use absolute paths.\n                    path = _make_absolute(dep.get_path(resolve=True), p.suite.dir)\n                    out.element('factorypathentry', {'kind' : 'EXTJAR', 'id' : path, 'enabled' : 'true', 'runInBatchMode' : 'false'})\n                    if files:\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3947,0],[3959,0]],"newRange":[[3947,0],[3947,0]],"oldText":"        if len(p.annotation_processors()) > 0:\n            out = XMLDoc()\n            out.open('factorypath')\n            out.element('factorypathentry', {'kind' : 'PLUGIN', 'id' : 'org.eclipse.jst.ws.annotations.core', 'enabled' : 'true', 'runInBatchMode' : 'false'})\n            for ap in p.annotation_processors():\n                for dep in dependency(ap).all_deps([], True):\n                    if dep.isLibrary():\n                        # Relative paths for \"lib\" class path entries have various semantics depending on the Eclipse\n                        # version being used (e.g. see https://bugs.eclipse.org/bugs/show_bug.cgi?id=274737) so it's\n                        # safest to simply use absolute paths.\n                        path = _make_absolute(dep.get_path(resolve=True), p.suite.dir)\n                        out.element('factorypathentry', {'kind' : 'EXTJAR', 'id' : path, 'enabled' : 'true', 'runInBatchMode' : 'false'})\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3948,0],[3948,0]],"newRange":[[3948,0],[3953,0]],"oldText":"","newText":"                elif dep.isProject():\n                    out.element('factorypathentry', {'kind' : 'WKSPJAR', 'id' : '/' + dep.name + '/' + dep.name + '.jar', 'enabled' : 'true', 'runInBatchMode' : 'false'})\n        out.close('factorypath')\n        update_file(join(p.dir, '.factorypath'), out.xml(indent='\\t', newl='\\n'))\n        if files:\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3953,0],[3957,0]],"newRange":[[3953,0],[3953,0]],"oldText":"                    elif dep.isProject():\n                        out.element('factorypathentry', {'kind' : 'WKSPJAR', 'id' : '/' + dep.name + '/' + dep.name + '.jar', 'enabled' : 'true', 'runInBatchMode' : 'false'})\n            out.close('factorypath')\n            update_file(join(p.dir, '.factorypath'), out.xml(indent='\\t', newl='\\n'))\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3955,0],[3955,0]],"newRange":[[3955,0],[3983,0]],"oldText":"","newText":"def _eclipseinit_suite(args, suite, buildProcessorJars=True, refreshOnly=False):\n    configZip = TimeStampFile(join(suite.mxDir, 'eclipse-config.zip'))\n    configLibsZip = join(suite.mxDir, 'eclipse-config-libs.zip')\n    if refreshOnly and not configZip.exists():\n        return\n\n    if _check_ide_timestamp(suite, configZip, 'eclipse'):\n        logv('[Eclipse configurations are up to date - skipping]')\n        return\n\n\n\n    files = []\n    libFiles = []\n    if buildProcessorJars:\n        files += _processorjars_suite(suite)\n\n    projToDist = dict()\n    for dist in _dists.values():\n        distDeps = dist.sorted_deps()\n        for p in distDeps:\n            projToDist[p.name] = (dist, [dep.name for dep in distDeps])\n\n    for p in suite.projects:\n        if p.native:\n            continue\n        _eclipseinit_project(p)\n\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[3992,0],[3993,0]],"newRange":[[3992,0],[3992,0]],"oldText":"        name = dist.name\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[4769,0],[4769,0]],"newRange":[[4769,0],[4771,0]],"oldText":"","newText":"                # no point in traversing .hg or lib/\n                dirnames[:] = [d for d in dirnames if d not in ['.hg', 'lib']]\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[4771,0],[4774,0]],"newRange":[[4771,0],[4771,0]],"oldText":"                # no point in traversing .hg\n                if '.hg' in dirnames:\n                    dirnames.remove('.hg')\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[6049,0],[6049,0]],"newRange":[[6049,0],[6050,0]],"oldText":"","newText":"version = VersionSpec(\"2.3.6\")\n","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"},{"oldRange":[[6050,0],[6051,0]],"newRange":[[6050,0],[6050,0]],"oldText":"version = VersionSpec(\"2.3.4\")\n","newText":"","normalizeLineEndings":{"normalizeLineEndings":false},"markerPatches":{},"deserializer":"BufferPatch"}],"deserializer":"Transaction"}],"redoStack":[],"deserializer":"History"},"filePath":"/Users/zwei/Workspace/mxtool2/mx.py","modifiedWhenLastPersisted":false,"digestWhenLastPersisted":"55c5dd108a3390a086a605c18f76c096b9a2108c","deserializer":"TextBuffer"}],"deserializer":"Project"},"workspace":{"paneContainer":{"root":{"items":[{"id":22,"softTabs":true,"displayBuffer":{"id":23,"softWrap":true,"editorWidthInChars":null,"scrollTop":3952,"scrollLeft":0,"tokenizedBuffer":{"bufferPath":"/Users/zwei/Workspace/mxtool2/mx.py","tabLength":2,"deserializer":"TokenizedBuffer"},"deserializer":"DisplayBuffer"},"deserializer":"Editor"}],"activeItemUri":"/Users/zwei/Workspace/mxtool2/mx.py","focused":false,"active":true,"deserializer":"Pane"},"deserializer":"PaneContainer"},"fullScreen":false,"deserializer":"Workspace"},"packageStates":{"fuzzy-finder":{"/Users/zwei/Workspace/mxtool2/mx.py":1405987445487},"keybinding-resolver":{"attached":false},"metrics":{"sessionLength":167620751},"tree-view":{"directoryExpansionStates":{},"selectedPath":"/Users/zwei/Workspace/mxtool2/mx.py","hasFocus":false,"attached":true,"scrollLeft":0,"scrollTop":0,"width":200}}}